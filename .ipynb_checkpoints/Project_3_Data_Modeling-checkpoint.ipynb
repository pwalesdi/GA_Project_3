{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Webscrapping: Modeling Notebook\n",
    "\n",
    "_Authors: Patrick Wales-Dinan_\n",
    "\n",
    "---\n",
    "\n",
    "This lab was incredibly challenging. We had to extensively clean a date set that was missing a lot of values and had TONS of categorical data. Then we had to decide what features to use to model that data. After that we had to build and fit the models making decisions like whether to use polynomial features, dummy variables etc, log scaling features or log scaling the depended variable.\n",
    "\n",
    "After that we had to re run our model over and over again, looking at the different values of $\\beta$ and seeing if they were contributing to the predictive power of the model. We had to decide if we should throw those values out or if we should leave them. We also had to make judgement calls to see if our model appeared to be over fitting or suffering from bias. \n",
    "\n",
    "## Contents:\n",
    "- [Data Import](#Data-Import)\n",
    "- [Baseline Accuracy](#Calculate-the-Baseline-Accuracy)\n",
    "- [Train Test Split](#Train-Test-Split-Our-Data)\n",
    "- [Log Scaling](#Log-Scaling-Independent-Variables)\n",
    "- [Cleaning the Data and Modifying the Data](#Cleaning-&-Creating-the-Data-Set)\n",
    "- [Modeling the Data](#Modeling-the-Data)\n",
    "- [Model Analysis](#Analyzing-the-model)\n",
    "\n",
    "Please visit the Graphs & Relationships notebook for additional visuals: Notebook - [Here](/Users/pwalesdi/Desktop/GA/GA_Project_2/Project_2_Graphs_&_Relationships.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = pd.read_csv('./reddit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.511458\n",
       "1    0.488542\n",
       "Name: ca, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our baseline accuracy :: So 0.51 is the baseline accuracy for 0.\n",
    "df_reddit['ca'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title']\n",
    "y = df_reddit['ca']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=79)\n",
    "pipe = Pipeline([\n",
    "            ('vec', CountVectorizer()),\n",
    "            ('model', LogisticRegression())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  60 tasks      | elapsed:    5.7s\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'vec' : [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__max_features': [1500, 2000, 2500, 2700],\n",
    "    'vec__min_df': [2, 3, 4],\n",
    "#     'vec__max_df': [0.5, .60, .70],\n",
    "    'vec__ngram_range': [(1,2), (1,1)],\n",
    "    'model' : [LogisticRegression(), LogisticRegression(penalty='l1', solver='liblinear'), LogisticRegression(penalty='l2', solver='liblinear'), MultinomialNB()]\n",
    "#     'vec__stop_words': ['english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5, verbose=1, n_jobs=2)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f' Best Parameters: {gs.best_params_}')\n",
    "print('')\n",
    "print(f' Cross Validation Accuracy Score: {gs.best_score_}')\n",
    "print(f' Training Data Accuracy Score: {gs.score(X_train, y_train)}')\n",
    "print(f' Testing Data Accuracy Score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['param_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('grad', GradientBoostingClassifier()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vote', vote)\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'vote__tree__max_depth' : [None, 1, 2],\n",
    "    'vote__ada__n_estimators' : [40, 50, 60],\n",
    "    'vote__grad__n_estimators' : [90, 100],\n",
    "    'vote__logreg__penalty' : ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_) # cross val accuracy score\n",
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
