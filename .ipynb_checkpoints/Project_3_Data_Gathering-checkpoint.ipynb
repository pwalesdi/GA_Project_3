{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Webscrapping: Data Gathering Notebook\n",
    "\n",
    "_Authors: Patrick Wales-Dinan_\n",
    "\n",
    "---\n",
    "\n",
    "This lab was incredibly challenging. We had to extensively clean a date set that was missing a lot of values and had TONS of categorical data. Then we had to decide what features to use to model that data. After that we had to build and fit the models making decisions like whether to use polynomial features, dummy variables etc, log scaling features or log scaling the depended variable.\n",
    "\n",
    "After that we had to re run our model over and over again, looking at the different values of $\\beta$ and seeing if they were contributing to the predictive power of the model. We had to decide if we should throw those values out or if we should leave them. We also had to make judgement calls to see if our model appeared to be over fitting or suffering from bias. \n",
    "\n",
    "## Contents:\n",
    "- [Creating our URLs](#Instantiate-our-URL)\n",
    "- [Data Import](#Data-Import)\n",
    "- [Feature Creation](#Feature-Creation)\n",
    "- [Choosing the Features](#Feature-Choice)\n",
    "- [Log Scaling](#Log-Scaling-Independent-Variables)\n",
    "- [Cleaning the Data and Modifying the Data](#Cleaning-&-Creating-the-Data-Set)\n",
    "- [Modeling the Data](#Modeling-the-Data)\n",
    "- [Model Analysis](#Analyzing-the-model)\n",
    "\n",
    "Please visit the Graphs & Relationships notebook for additional visuals: Notebook - [Here](/Users/pwalesdi/Desktop/GA/GA_Project_2/Project_2_Graphs_&_Relationships.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_url = 'https://www.reddit.com/r/TexasPolitics.json'\n",
    "ca_url = 'https://www.reddit.com/r/California_Politics.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(url):\n",
    "    # Setting up my unique user agent so that I can pull posts from reddit\n",
    "    user_agent = {'User-agent' : 'pat bot 0.1'}\n",
    "    \n",
    "    # Empty posts list\n",
    "    posts = []\n",
    "    \n",
    "    # Setting after to NONE to start as this needs to be there in order to begin each pull\n",
    "    after = None\n",
    "    \n",
    "    for i in range(0,60):\n",
    "        print(i)\n",
    "        url = url\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after' : after}\n",
    "        res = requests.get(url, params=params, headers=user_agent)\n",
    "        if res.status_code == 200:\n",
    "            json = res.json()\n",
    "            posts.extend(json['data']['children'])\n",
    "            after = json['data']['after']\n",
    "        else: \n",
    "            print(tx_res.status_code)\n",
    "            break\n",
    "        time.sleep(2)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "tx_posts = get_posts(tx_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "ca_posts = get_posts(ca_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ca_posts)\n",
    "len(set([p['data']['name'] for p in ca_posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tx_posts)\n",
    "len(set([p['data']['name'] for p in tx_posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>theProgressiveGOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_3x0d2uzk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>calmatters.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1s4p</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1s4p</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/cb1s4p/state_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/GcoPg0hQ78iGU...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://calmatters.org/articles/redistricting-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CALmatters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_kwolsnv</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562668e+09</td>\n",
       "      <td>1.562639e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>calmatters.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>caupt1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_caupt1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/caupt1/new_cal...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>45</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/2FFGh5hCRRYIk...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>https://calmatters.org/articles/ca-passes-dead...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>BlankVerse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_97a3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562634e+09</td>\n",
       "      <td>1.562606e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>thetrace.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>canr0y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_canr0y</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/canr0y/the_nra...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>53</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/5y-0hwerp_6jF...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.thetrace.org/rounds/california-rea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>travadera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_10ukzyn2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562636e+09</td>\n",
       "      <td>1.562607e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>latimes.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cao5lo</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cao5lo</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/cao5lo/ca15_er...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/EXvjRI9EHJh4w...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.latimes.com/politics/la-na-pol-202...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>BlankVerse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_97a3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562642e+09</td>\n",
       "      <td>1.562613e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>capegr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_capegr</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/capegr/eric_sw...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/pKle1lzNaOWeA...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.cnn.com/2019/07/08/politics/eric-s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments approved_at_utc approved_by  archived             author author_cakeday author_flair_background_color author_flair_css_class author_flair_richtext author_flair_template_id author_flair_text author_flair_text_color author_flair_type author_fullname author_patreon_flair banned_at_utc banned_by  can_gild  can_mod_post category  clicked content_categories  contest_mode       created   created_utc crosspost_parent crosspost_parent_list discussion_type distinguished          domain  downs edited  gilded gildings  hidden  hide_score      id  is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  is_robot_indexable  is_self  is_video likes link_flair_background_color link_flair_css_class link_flair_richtext link_flair_text link_flair_text_color link_flair_type  locked media media_embed  media_only mod_note mod_reason_by mod_reason_title mod_reports       name  no_follow  num_comments  num_crossposts num_reports  over_18  \\\n",
       "0            []                False            None        None     False  theProgressiveGOP            NaN                          None                   None                    []                     None              None                    None              text     t2_3x0d2uzk                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None  calmatters.org      0  False       0       {}   False        True  cb1s4p             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_cb1s4p      False             0               0        None    False   \n",
       "1            []                False            None        None     False         CALmatters            NaN                          None                   None                    []                     None              None                    None              text      t2_kwolsnv                False          None      None     False         False     None    False               None         False  1.562668e+09  1.562639e+09              NaN                   NaN            None          None  calmatters.org      0  False       0       {}   False       False  caupt1             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_caupt1      False             3               1        None    False   \n",
       "2            []                False            None        None     False         BlankVerse            NaN                          None                   None                    []                     None              None                    None              text         t2_97a3                False          None      None     False         False     None    False               None         False  1.562634e+09  1.562606e+09              NaN                   NaN            None          None    thetrace.org      0  False       0       {}   False       False  canr0y             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_canr0y      False            21               0        None    False   \n",
       "3            []                False            None        None     False          travadera            NaN                          None                   None                    []                     None              None                    None              text     t2_10ukzyn2                False          None      None     False         False     None    False               None         False  1.562636e+09  1.562607e+09              NaN                   NaN            None          None     latimes.com      0  False       0       {}   False       False  cao5lo             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_cao5lo      False             5               1        None    False   \n",
       "4            []                False            None        None     False         BlankVerse            NaN                          None                   None                    []                     None              None                    None              text         t2_97a3                False          None      None     False         False     None    False               None         False  1.562642e+09  1.562613e+09              NaN                   NaN            None          None         cnn.com      0  False       0       {}   False       False  capegr             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_capegr      False             4               0        None    False   \n",
       "\n",
       "  parent_whitelist_status                                          permalink  pinned post_hint                                            preview  pwls  quarantine removal_reason report_reasons  saved  score secure_media secure_media_embed selftext selftext_html  send_replies  spoiler  stickied            subreddit subreddit_id subreddit_name_prefixed  subreddit_subscribers subreddit_type suggested_sort                                          thumbnail  thumbnail_height  thumbnail_width                                              title  total_awards_received  ups                                                url user_reports view_count  visited whitelist_status   wls  \n",
       "0                    None  /r/California_Politics/comments/cb1s4p/state_m...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     13         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/GcoPg0hQ78iGU...              93.0            140.0  State May Push Cities and Counties to Draw \"fa...                      0   13  https://calmatters.org/articles/redistricting-...           []       None    False             None  None  \n",
       "1                    None  /r/California_Politics/comments/caupt1/new_cal...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     45         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/2FFGh5hCRRYIk...              93.0            140.0  New California rules for deadly police force g...                      0   45  https://calmatters.org/articles/ca-passes-dead...           []       None    False             None  None  \n",
       "2                    None  /r/California_Politics/comments/canr0y/the_nra...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     53         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/5y-0hwerp_6jF...              93.0            140.0  The NRA Opposes A California Gun Regulation It...                      0   53  https://www.thetrace.org/rounds/california-rea...           []       None    False             None  None  \n",
       "3                    None  /r/California_Politics/comments/cao5lo/ca15_er...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     20         None                 {}                   None         False    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/EXvjRI9EHJh4w...              78.0            140.0  [CA-15] Eric Swalwell is expected to withdraw ...                      0   20  https://www.latimes.com/politics/la-na-pol-202...           []       None    False             None  None  \n",
       "4                    None  /r/California_Politics/comments/capegr/eric_sw...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     12         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/pKle1lzNaOWeA...              78.0            140.0  Eric Swalwell expected to end presidential bid...                      0   12  https://www.cnn.com/2019/07/08/politics/eric-s...           []       None    False             None  None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_post_new = []\n",
    "ca_post_names = set()\n",
    "for post_dict in ca_posts:\n",
    "    keep_data = post_dict['data']\n",
    "    if keep_data['name'] not in ca_post_names:\n",
    "        ca_post_new.append(keep_data)\n",
    "        ca_post_names.add(keep_data['name'])\n",
    "df_ca = pd.DataFrame(ca_post_new)\n",
    "df_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>arcanition</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>17553cd2-9c63-11e7-b44c-0e30f0006cb4</td>\n",
       "      <td>3rd District (Northern Dallas Suburbs)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_5d5mc</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.559800e+09</td>\n",
       "      <td>1.559771e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>moderator</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.55984e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bx8cik</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_bx8cik</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/bx8cik/welcome_new_r...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Hey all,\\n\\nAfter much time reading applicatio...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Welcome New /r/TexasPolitics Moderators - Q&amp;amp;A</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Texas_Monthly</td>\n",
       "      <td></td>\n",
       "      <td>verified</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Verified - Texas Monthly</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_3x7xx9qc</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.561003e+09</td>\n",
       "      <td>1.560974e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.56107e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>c2lven</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>ama</td>\n",
       "      <td>[]</td>\n",
       "      <td>b8855642-9c62-11e7-ae9f-0e71ceb054c0</td>\n",
       "      <td>AMA</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_c2lven</td>\n",
       "      <td>False</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/c2lven/im_chris_hook...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Hey, r/TexasPolitics! Im Chris Hooks, a write...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>qa</td>\n",
       "      <td></td>\n",
       "      <td>Im Chris Hooks, a Texas Monthly writer who wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>beanzamillion21</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>2584f856-9c63-11e7-93b7-0e2bf15991f0</td>\n",
       "      <td>12th Congressional District (Western Fort Worth)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_50w01</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dallasnews.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1r8d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1r8d</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cb1r8d/ross_perot_se...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Ross Perot, self-made billionaire, patriot and...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>https://www.dallasnews.com/business/business/2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>irony_glazed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_442pim8f</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562678e+09</td>\n",
       "      <td>1.562649e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.56268e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cawekm</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>discussion</td>\n",
       "      <td>[]</td>\n",
       "      <td>ac3a0f90-9c62-11e7-9e00-0e65ddf91c6e</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cawekm</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cawekm/lets_talk_abo...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Disclaimer: I am not a lawyer, this is not leg...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Let's Talk About Why Texas's Hemp Law is Stupid</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>beanzamillion21</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>2584f856-9c63-11e7-93b7-0e2bf15991f0</td>\n",
       "      <td>12th Congressional District (Western Fort Worth)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_50w01</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>housingwire.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1sin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1sin</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cb1sin/this_texas_to...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>This Texas town is the most affordable housing...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.housingwire.com/articles/49504-thi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments approved_at_utc approved_by  archived           author author_flair_background_color author_flair_css_class author_flair_richtext              author_flair_template_id                                 author_flair_text author_flair_text_color author_flair_type author_fullname author_patreon_flair banned_at_utc banned_by  can_gild  can_mod_post category  clicked content_categories  contest_mode       created   created_utc crosspost_parent crosspost_parent_list discussion_type distinguished              domain  downs       edited  gilded gildings  hidden  hide_score      id  is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  is_robot_indexable  is_self  is_video likes link_flair_background_color link_flair_css_class link_flair_richtext                link_flair_template_id link_flair_text link_flair_text_color link_flair_type  locked media media_embed media_metadata  media_only mod_note mod_reason_by mod_reason_title mod_reports  \\\n",
       "0            []                False            None        None     False       arcanition                          None                      3                    []  17553cd2-9c63-11e7-b44c-0e30f0006cb4            3rd District (Northern Dallas Suburbs)                    dark              text        t2_5d5mc                False          None      None     False         False     None    False               None         False  1.559800e+09  1.559771e+09              NaN                   NaN            None     moderator  self.TexasPolitics      0  1.55984e+09       0       {}   False       False  bx8cik             False    False                False                   False                True     True     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "1            []                 True            None        None     False    Texas_Monthly                                             verified                    []                                  None                          Verified - Texas Monthly                    dark              text     t2_3x7xx9qc                False          None      None     False         False     None    False               None         False  1.561003e+09  1.560974e+09              NaN                   NaN            None          None  self.TexasPolitics      0  1.56107e+09       0       {}   False       False  c2lven             False    False                False                   False                True     True     False  None                                              ama                  []  b8855642-9c62-11e7-ae9f-0e71ceb054c0             AMA                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "2            []                False            None        None     False  beanzamillion21                          None                     12                    []  2584f856-9c63-11e7-93b7-0e2bf15991f0  12th Congressional District (Western Fort Worth)                    dark              text        t2_50w01                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None      dallasnews.com      0        False       0       {}   False        True  cb1r8d             False    False                False                   False                True    False     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "3            []                False            None        None     False     irony_glazed                          None                   None                    []                                  None                                              None                    None              text     t2_442pim8f                False          None      None     False         False     None    False               None         False  1.562678e+09  1.562649e+09              NaN                   NaN            None          None  self.TexasPolitics      0  1.56268e+09       0       {}   False       False  cawekm             False    False                False                   False                True     True     False  None                                       discussion                  []  ac3a0f90-9c62-11e7-9e00-0e65ddf91c6e      Discussion                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "4            []                False            None        None     False  beanzamillion21                          None                     12                    []  2584f856-9c63-11e7-93b7-0e2bf15991f0  12th Congressional District (Western Fort Worth)                    dark              text        t2_50w01                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None     housingwire.com      0        False       0       {}   False        True  cb1sin             False    False                False                   False                True    False     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "\n",
       "        name  no_follow  num_comments  num_crossposts num_reports  over_18 parent_whitelist_status                                          permalink  pinned  pwls  quarantine removal_reason report_reasons  saved  score secure_media secure_media_embed                                           selftext                                      selftext_html  send_replies  spoiler  stickied      subreddit subreddit_id subreddit_name_prefixed  subreddit_subscribers subreddit_type suggested_sort thumbnail                                              title  total_awards_received  ups                                                url user_reports view_count  visited whitelist_status   wls  \n",
       "0  t3_bx8cik      False            22               0        None    False                    None  /r/TexasPolitics/comments/bx8cik/welcome_new_r...   False  None       False           None           None  False     13         None                 {}  Hey all,\\n\\nAfter much time reading applicatio...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False      True  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            Welcome New /r/TexasPolitics Moderators - Q&amp;A                      0   13  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "1  t3_c2lven      False           243               0        None    False                    None  /r/TexasPolitics/comments/c2lven/im_chris_hook...   False  None       False           None           None  False     85         None                 {}  Hey, r/TexasPolitics! Im Chris Hooks, a write...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False      True  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public             qa            Im Chris Hooks, a Texas Monthly writer who wo...                      0   85  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "2  t3_cb1r8d      False             8               0        None    False                    None  /r/TexasPolitics/comments/cb1r8d/ross_perot_se...   False  None       False           None           None  False     25         None                 {}                                                                                                  None          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            Ross Perot, self-made billionaire, patriot and...                      0   25  https://www.dallasnews.com/business/business/2...           []       None    False             None  None  \n",
       "3  t3_cawekm      False            14               1        None    False                    None  /r/TexasPolitics/comments/cawekm/lets_talk_abo...   False  None       False           None           None  False     37         None                 {}  Disclaimer: I am not a lawyer, this is not leg...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None              Let's Talk About Why Texas's Hemp Law is Stupid                      0   37  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "4  t3_cb1sin      False             3               0        None    False                    None  /r/TexasPolitics/comments/cb1sin/this_texas_to...   False  None       False           None           None  False      2         None                 {}                                                                                                  None          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            This Texas town is the most affordable housing...                      0    2  https://www.housingwire.com/articles/49504-thi...           []       None    False             None  None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_post_new = []\n",
    "tx_post_names = set()\n",
    "for post_dict in tx_posts:\n",
    "    keep_data = post_dict['data']\n",
    "    if keep_data['name'] not in tx_post_names:\n",
    "        tx_post_new.append(keep_data)\n",
    "        tx_post_names.add(keep_data['name'])\n",
    "df_tx = pd.DataFrame(tx_post_new)\n",
    "df_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx = df_tx[['subreddit', 'title', 'num_comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca[['subreddit', 'title', 'num_comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = df_ca.append(df_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                              title  num_comments\n",
       "0  California_Politics  State May Push Cities and Counties to Draw \"fa...             0\n",
       "1  California_Politics  New California rules for deadly police force g...             3\n",
       "2  California_Politics  The NRA Opposes A California Gun Regulation It...            21\n",
       "3  California_Politics  [CA-15] Eric Swalwell is expected to withdraw ...             5\n",
       "4  California_Politics  Eric Swalwell expected to end presidential bid...             4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit['ca'] = df_reddit['subreddit'].map({'California_Politics':1,\n",
    "                                                 'TexasPolitics':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.drop(labels='subreddit', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tom Steyer Is Telling Allies Hes Running for ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>California's Governor is Asking Trump for Emer...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>State Promises to Rebuild: Ridgecrest Will Not...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How California made a 'dramatic' impact on kin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California's Politically Powerful Unions Aim T...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New state budget a windfall for unions</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>End the federal prohibition of marijuana: vote...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overregulation is one reason for the high cost...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How Big Soda used its clout to stop 5 of 5 C...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kamala Harris Flip Flops on the Elimination of...</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Democrats to allow non-citizens to ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Trump Thinks Putins Attack on Western-Style ...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Police accountability: Six months after new Ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>One year ago, L.A. approved an ambitious housi...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Newsom calls out Californias racist first gov...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Californias proposed opioid tax would hurt pa...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Megan Dahle announces run for husbands state ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[CA-50] Duncan Hunter 'Marital Spat,' Italian ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Indictments Charge Widespread Voting Fraud Sch...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter's Affairs Were Simp...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Berkeley moves to the forefront in California ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter Will Resign, Says D...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gavin Newsoms biggest accomplishment as gover...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[CA-AD-73] Orange County Supervisor Lisa Bartl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>California Gov. Gavin Newsom has signed his fi...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Why cant California pass more housing legisla...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Why California is so liberal and progressive i...</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[CA-50] Former Hill staffer alleges Rep. Dunca...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Are California utilities doing enough to firep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Many people are moving from California to Texa...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lawmakerscutting big checks to combat the hou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>California trails in regulating short-term len...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Commentary: Gavin Newsom faces his first defin...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A coworker told me that, due to a proposition ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Meet Rev. Andy Bales  The Man Who Lost A Leg ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Housing barbs spark backlash against Cupertino...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter used campaign funds...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Who Can Be On The California Redistricting Com...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Why a California state senator wants to ban ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Any spike in repeat crimes after California sp...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>State Unemployment Drops; Job Growth, Shrinkin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[San Diego Mayor] Former Gov. Jerry Brown, ex-...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>California program to track state worker haras...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>California Gov. Gavin Newsoms not-so-spectacu...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NCAA says California schools could be banned f...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>How can California solve its homeless crisis? ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Treat workers as employees? Uber, Lyft and oth...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>California gas tax goes up July 1, but leaders...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Californias budget offers new help for millio...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[CA-AD-73] Orange County Supervisor Lisa Bartl...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Berkeley helps to push back against excessive ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>The Future of the California Republican Party?</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>California vaccine bill clears Assembly panel ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>California political parties couldnt use ind...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Gov. Newsom, Bay Area leaders respond to propo...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[CA-50] Darrell Issa, the former Oversight Com...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Its been a mess for decades. Can Gov. Newsom ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>California State University stashed $1.5 billi...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>California is one step closer to passing the m...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>A million independent voters risk being irrele...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>George Takei: 'At Least During the Internment ...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>a chart of what Californians will pay annually...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Poster for the upcoming CNP convention (June 2...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>In Los Angeles, only people of color are sente...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>California Activists Participate in Muslim Da...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>California Legislature must act to protect env...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Immigrant entrepreneurs continue to shape Cali...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>How Redding, California, became an unlikely ep...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Crime fighting robot deployed in California</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Commentary: Cities pledge to find solutions to...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Wet California winter is a boon for skiers and...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Rent Control Measure Returns, Legislative Anal...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Bay Area Foothills College student homelessnes...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>California needs a big pot of money for wildfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Redding Town Hall receives criticism after \"Ch...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>The Homeless Are Dying in Record Numbers on th...</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Open Forum: Exaggerating California crime to p...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>California Legislation to Limit Predatory Lend...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Duncan Hunter's wife agrees to cooperate with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Warren rises to second in California poll</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Where Gov. Gavin Newsom wins and loses in newl...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>California just passed a $215 billion budget. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>California Democratic 2020 presidential primar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>New report: Google campus will lead to $235M m...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>California may automatically expunge 1 million...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Abuse in the Disabled Care Indjstry in California</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>California National Party Convention on June 22nd</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Towing a car can be financially ruinous. Shoul...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[CA-50] Margaret Hunter to plead out in case a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Leslie Marshall: California is right to give i...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LGBTQ nightclub in Fresno targeted by threats,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>L.A. council members propose taxing landlords ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A new Citizens Redistricting Commission is bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Homeless crisis: Los Angeles County seeks help...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[CA-39, CA-45, CA-48, CA-49] Orange County Rep...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>California considers ban on facial recognition...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Heres how California can become a tuition-fre...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Should community colleges in California build ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Does this vaccine bill go too far? Concerned f...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Report links evictions, homelessness in Los An...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Keeping an eye on sheriffs: California Democra...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>White House rejects carmakers plea for a deal...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Gov. Newsom stepped into a vaccine debate we s...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In two California Senate special elections, Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Californias top bullet train consultant is su...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>New California Could Become 51st State</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Swinging at Every Pitch: Californias Govern...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>In The Dreamt Land, Mark Arax unpacks the Gold...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>California Democratic voters disagree with Nan...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Startup rents bunkbeds in San Francisco for $1...</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>How California became far more energy-efficien...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Californians favor dramatic changes to build m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Gov. Newsom criticized the new vaccine bill. A...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Landlords win, renters take a hit. Just one te...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>How Californias big plans to address housing ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Californias long-overlooked Central Valley ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Brace for a Spike in Homelessness in LA County</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>California Democratic Party elects L.A. labor ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Northern California state Senate special elect...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Say goodbye to your local precinct. Voting in ...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>The New Front in the SB 50 Battle Is Toni Atki...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The looming California challenge for Kamala Ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Californias state universities are a path to ...</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>The soul-crushing cost of college in Californi...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Death Watch: Keeping track of the bills Califo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Cruel and Unusual: A Guide to Californias Bro...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>California plan to prevent big rent hikes adva...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>A California bill to ban flavored tobacco prod...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>California gasoline prices increase following ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Rethinking Disaster Recovery After A Californi...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Los Angeles County Bans Use of Roundup Weed Ki...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Think Californias too big and influential? Wa...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>California probably will pick next president</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In need of teacher housing, more California sc...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Two tax hikes for schools could end up on Cali...</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>[CA-50] Rep. Hunter on War-Crimes Suspect Gall...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>California says it's now in compliance with US...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>In California, Agreement On New Rules For When...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Why California's Efforts To Limit Soda Keep Fi...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Congress reaches deal on disaster aid: Califor...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Editorial: California needs to resume investme...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>California Senate passes legislation to create...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>California GOP picks favorites for re-flipping...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>California regulators arent taking action aga...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Trump threatens to cut millions from fire depa...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Poll: Two-thirds of California voters back SB ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>SF District Attorney Gascn questions SF polic...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>How Legalization Changed Humboldt County Marij...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Brawl erupts at convention for local-governmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Retired Oil Rigs off the California Coast Coul...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Its time to politically destroy Kevin McCart...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Pipeline Shutdown Prevented 27 Million Tons of...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Trump tears into California for high-speed rai...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Guns, gas and soda  most California tax propo...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Commentary: These are the key conflicts in Cal...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Alabama and Georgia passed abortion bans. Cali...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Report: Gas price hike could be due to manipul...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Assemblyman Marc Bermans college car camping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>High-profile California housing bill dies with...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Trump Wants to Open Public Lands to Oil Drilli...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Kamala Harris still not catching on with Democ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Californias New Prohibitionists</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Trump pardons media tycoon, former GOP leader ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Oakland Unifieds journey: When the state step...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>California Primary Becomes a Tantalizing Prize...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Simon Liu Isnt A Sex Offender. But Hes Still...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>California is bringing law and order to big da...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>California files 50th lawsuit against Trump ad...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>California might triple number of marijuana sh...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>California on track to lose at least one congr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Bill to stiffen Californias vaccine law must ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>California could bring radical change to singl...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>White House Hopefuls Swarm Rivals Home Turf o...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>My patient was homeless. I knew she was going ...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Only one California Republican defied Trump on...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>CA Democratic Conventionphotography questions</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>California governor unveils record $213 billio...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Trump Finalizes Plan to Open 725,500 Acres of ...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Potential Impact of Californias Split Roll ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Revenge of the Coastal Elites: How California,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>California defies Trump to ban pesticide linke...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Biden meets with big-dollar California donors,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Gavin Newsoms California budget rises to $213...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Make California map chart with your data.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Mayor Pete blindsides Kamala Harris in California</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Gavin Newsom wants to end California taxes on ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Hot off the grille: Is California ready to leg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>In Trump vs. California, the state is winning ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>California Activists Take First Steps To Decri...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>State officials keep hiring their relatives. W...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>More reasons to be concerned with election int...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>The Trump Administration Stopped Working On Ca...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Swastika leaflets calling press the enemy dr...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>In a Galaxy Not So Far Away: California Offici...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>When the next recession hits, will California ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Californias 2018 midterm election: A dive int...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Reminder: Only post matters that are specific ...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>How powerful lawmakers are killing California ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>California bill fighting discrimination of 'na...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>The kingmakers in Californias 2020 elections ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>California Economy Soars Above U.K., France an...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Los Angeles sets dramatic new goals for electr...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>A doctored photo and a lawsuit: California GOP...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Freshman Rep. Katie Hill, who was part of the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Trump abortion policy targeting Planned Parent...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>New Election Ordered After Ca. Dems Caught Che...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Californias Hidden Corporate Tax Cuts  If ci...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Trump fracking plan targets over 1 million acr...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>San Francisco approves homeless shelter despit...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Joe Biden lacks big-name California allies as ...</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>California rent control moves forward with Gav...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Morgan Stanley to pay $150 million to settle C...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>California May Ban Schools From Suspending Stu...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Andrew Yangs Campaign Grows with Large LA Rally</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Anti-vaccine families crowd California Capitol...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Rep. Katie Hill has Nancy Pelosis favor, but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Newsom seeks an explanation for Californias h...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>L.A. students want to lower voting age in scho...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Sideshows and the Extractive Economy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Combating homelessness</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Temecula will weigh city resolution to describ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>California Cities Have Shredded Decades of Pol...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Republicans Lining Up for 2020 House Fights in...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Beto ORourke opens his California campaign Sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>How Trump factors into California's charter sc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Bill To End Hair Discrimination In The Workpla...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Use of Force bills AB 392 and SB 230 now linke...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Julin Castro Is First 2020 Democratic Preside...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>CA-15: State Sen. Bob Wieckowski is forming an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>California housing bill targeting wealthy citi...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>California Democrats are awash in cash as the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>How Gov. Gavin Newsom is progressing on his ke...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Green Solutions for Governor Newsom's Climate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Kamala Harris regrets California truancy law t...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>The Public Banking Revolution Is Upon Us: Cali...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>[CA-50] Rep. Hunter pretends to cross the Mexi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>[L.A. County Supervisor (District 4)] Former a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Buttigieg plans aggressive fundraising push in...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Former California water lobbyist, Trump's Inte...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Fastest litigant in the west: Californias on ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>U.S. appeals court upholds most of California...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>California gives out too many tax breaks. And ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Swalwell calls for Barr to resign</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Years of bad rules led to Californias unfixab...</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Kamala Harris has collected millions from big ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>In Wake Of Measles Cases, Health Advocates Wan...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>Proposed law would make hemp products legal in...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Red-Light Camera Ban Filed in House</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>MJ Hegar  Texas veteran behind viral \"Doors\" ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>How Will Texas Lawmakers Pay For School Financ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Go big (with our bandwidth) or go home, Verizo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Thousands of Texans were shocked by surprise m...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Quorum Report: Business effort begins at the C...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>McAllen orders Catholic Charities to vacate im...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Texas Senate Advances Property Tax Reform To F...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>El Paso Fire Department denies Trump's crowd c...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>HD125 with 100% reporting: Republican Fred Ran...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Images From the Dueling Trump and Beto O'Rourk...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Four Democrats and one Republican vie for Just...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Judges Ruling Could Have Big Implications For...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>The AG's office told lawmakers it isnt invest...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Beto ORourke to hold counter-speech same time...</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Texas Attorney General wont investigate voter...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>Nearly 100 Texans have submitted ideas for wha...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Texas Public Opinion on Donald Trump, Immigrat...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Texas Dan Patrick claims taxes are too high. ...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>Richardson councilman, Scott Dunn, issues apol...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>Bill: HB 1408, introduced by Texas Rep. Jared ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>Texas Has Been Just a Prop for Trump From the ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>Texas bill would ban throttling in disaster ar...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>Texas AG Ken Paxton Says He Hasnt Launched Cr...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>Texas Secretary of State David Whitley defends...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>Fact Checking the Voter Fraud Debacle. With st...</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>Analysis: A Green Appointees [Sec. of State D...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Navy veteran challenges fellow Navy veteran Re...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>Some Texas Democrats want Beto O'Rourke to shu...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>Trump's underwater (-1) in Texas for the first...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>Despite running in a midterm year, Beto ORour...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>Texas Republican Accused of Sending Student Nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>A border fence did not lower crime rates in El...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>Native Americans protest building border wall ...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Beto ORourke Was Once Adrift in New York City...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Texas Gov. Greg Abbott give his State of the S...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Rick Perry is the Designated Survivor for to...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>Woman who filmed shooting of Botham Jean fired...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>Rattled by CBD Debate, Shop Owners React: I D...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Texas lawmaker wants cars with MSRP over $60k ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>More Civil Rights Groups Sue Over Secretary Of...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>Texas Republicans Are Lying About Voter Fraud ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>Abbott Names School Finance, Property Taxes, M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Texas Public Opinion and Governor Abbott's Eme...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Elizabeth Warren APOLOGIZES for Native America...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Ted Cruz just compared rape victims to a man w...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Hispanics are propping up Texas economy, workf...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Property tax relief, but only for some: small ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>Audio: Border Patrol Plans to Light Up Butterf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Texas AG asks federal judge to end DACA program</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Texans Can Appeal Surprise Medical Bills, But ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Cornyn braces for brutal Texas reelection battle</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Despite beer and lobby ties, Speaker Dennis Bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>Can Oprah help restore the Beto ORourke glow?</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Texas Attorney General Ken Paxton is seeking m...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>President Trumps Texas-size whoppers</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Far-Right Texas Legislator Files Bill to Compl...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>A No-Knock Raid in Houston Led to Deaths and P...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>US prepares to start building portion of Texas...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Rural Texas needs more veterinarians, and Texa...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Texas optometrists \"just roll our eyes\" over t...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Texas judge says Sutherland Springs families c...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Should someone propose a 70% tax on the Patri...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Dems are headed to Texas to probe suspected vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>What goes on in the secret world of private pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Americans like me don't belong on Texas' botch...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Naturalized citizens suing over Texas voter ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Naturalized citizen is angry to find her name ...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Civil rights group sues Texas over order to in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>100 Richest people in Texas</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>The Texas voter purge (Vox Overview of Recent ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Healing Power or a Dose of Trouble? CBD Oil Ta...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>State: All 366 on local list of potential nonc...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Question for the sub conservatives</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>In Harris County, Thousands Of Registered Vote...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Armed Man Disrupts Houston Library During Drag...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Texas lawmakers file identical companion bills...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Indigenous Activists Set Up Protest Camp at So...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Texas Gov. Greg Abbott Downplays Concerns Abou...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>The Woman Behind the Kamala Harris Presidentia...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Texas Officials Begin Walking Back Allegations...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Many see \"Robin Hood\" as a villain. But lawmak...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>New legislation would allow Texas liquor store...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Former Proud Boys trial canceled after witn...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Analysis: Texas election officials serve up a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Heres What You Need to Know About the Texas V...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Texas Republicans fear Trump could lose the st...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>The Poor in Texas Have Been Vastly Undercounte...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Top Texas Democrat rules out funding Trump's b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Democrat Art Fierro wins #HD79 special electio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Texas quietly tells counties that some of the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Texas Republican introduces bill to make discr...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2 Democrats headed to runoff in race to replac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>[Special Election Today HD 79 &amp;amp; HD 145] Vo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>President Donald Trump: \"58,000 non-citizens v...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Texas officials sued by Latino group over sugg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>After Ted Cruz's close race against Beto O'Rou...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>'We were not welcomed': Gay couple rejected by...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Harris County GOP draws fire for post blaming ...</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>Seliger, West Texas deserve better from Patrick.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Winners and losers: When Texas House's powerfu...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Texas says it has found 95,000 non citizens on...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>As it ponders where to put a Confederate plaqu...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>3 air traffic controllers in Texas resign over...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>TexSoS: Use of Non-U.S. Citizen Data obtained ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>[Meta] What happened to Moderator /u/Lemon_Lym...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Texas Sec. of State: 58K non-US citizens voted...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Texas Border Sheriffs: There is No Crisis and ...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Opinion | I joined the GOP because it stood fo...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Most members of the Texas Legislature are whit...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>AG Announcement: 56,000 non-citizens have vote...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Bennet Rips Cruz | User Clip | C-SPAN.org</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Exclusive: White House preparing draft nationa...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>Texas conservatives claim LGBTQ equality bill ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Where Texans in Congress come down on the gove...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Texas House committee appointments bode well f...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Every TX GOP House Member (except for William ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Texas Comptroller releases new report on Schoo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>Johnson Space Center workers being asked to cl...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Texas man organizes 'search party' event to lo...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>Texas House Speaker Dennis Bonnen names commit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Sheila Jackson Lee Leaves 2 Posts After Aide S...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>Sheila Jackson Lee steps down from key posts a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>Cornyn and Cruz vote against motion to keep Ru...</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>Texas House Speaker appoints three Dallas lawm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>Dan Crenshaw: Only six Democrats voted to pay ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Texas Rep. Eric Johnson running for Dallas mayor</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>Some Parents Concerned Increased School Fundin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>To combat opioid addiction crisis, Texas AG Ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>Lt. Gov. Dan Patrick pulls Sen. Kel Seligers ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Bullshit is how one Texan describes Trump an...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2019's Most Educated States in America: Texas #39</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Texas coal power plants leaching toxic polluta...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>Trump Touts Border Wall In San Antonio, Which ...</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>Texas Judge Tells Jury God Says Defendant Is N...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>Beto ORourkes road trip drives home his message</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>Rep. Will Hurd (R-Texas), calls Trump's border...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>The Big Dogs Of Texas Local Government: 18,000...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Court rules against Planned Parenthood in Texa...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Texas Unions are ready for this session.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>7 Feb marijuana lobbying day in Austin</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>GOP Rep. Will Hurd calls wall \"least effective...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Texas Rep. Castro wants Trump impeached after ...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>GOP tries to re-create its special-election ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>Court rules Texas can bar Planned Parenthood f...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>Both Cornyn and Cruz voted to protect Russia s...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>During the shutdown, government lawyers in Sou...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>How will the Texas Legislature address school ...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Report: Power Plants are Leaking Cancer-Causin...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Harris County judges unveil drastic new plan f...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>During the shutdown, government lawyers in Sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>We Just Listened to Him Talk: Sister Norma P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Texas has most people without health insurance...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Tyler Congressman Louie Gohmert says Steve Kin...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Will 2019 Be The Year Texas Breweries Can Sell...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Can Texas Build a Working Medical Cannabis Pro...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Governor, top Texans in Congress ask Trump not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Texas House proposes massive increase for publ...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Beto ORourkes Washington Post Interview Spel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Congresswoman Alexandria Ocasio-Cortez Joins S...</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Texas Senate wants billions to fund $5,000 pay...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Texas Rep. Dan Crenshaw under fire for shutdow...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Just 87 people voted today in the HD145 specia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>The Rise and Fall of the Tornillo Tent City fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>Why Americas Largest Migrant Youth Detention ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Beto ORourkes immigration plan: No wall but ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Ted Cruz defends Trump on Russia: \"I don't see...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>We're up to five candidates filed for #HD125 s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Medicaid, opioids and abortion: Health care is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>[slimy former US Rep] Farenthold resigns as Ca...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Chip Roy of CD21. Workers will forever remembe...</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Julin Castro, Former Housing Secretary, Annou...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>El Paso Times column refutes AG Paxton's claim...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Tarrant County GOP votes to retain Muslim vice...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Texas officials vote to remove Confederate pla...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>Texas lawmakers indicate they may use rainy da...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>New Anti-LGBT Initiative Pops up Despite What ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>Democrat Julian Castro expected to launch 2020...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>Texas ranked No.1 state for women entrepreneurs</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>Donald Trump says Lt. Gov. Dan Patrick offered...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Trump Plans To Use Eminent Domain Against Priv...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>Texas Republicans Rally Behind Muslim Official...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Tarrant County GOP votes to keep vice chair de...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>Interesting response from Senator Cornyn's office</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>Cruz defends eminent domain for border wall</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>Tarrant County GOPs vice-chairman survives re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>One small group of private citizens is posted ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>What some border residents feel might be the b...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>Rep Bonnen replaces coffee cups in the break r...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Climate change science presented to uncertain ...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Victims of DV having right to a Court Appointe...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Newly elected House Speaker Dennis Bonnen says...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Beto O'Rourke plans nationwide road trip to me...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>GOP operatives dig for dirt against rising sta...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Scientists To Abbott: Climate Change Is Happe...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>San Antonio lands Texas first opportunity zo...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>The Texas Legislature Gaveled In Today Without...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Border crisis echo in Texas is faint as lawm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>If the Texas Legislature wont help solve Dall...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Texas libertarian Ron Paul: We don't need Trum...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Texas economy is robust, giving lawmakers $9...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>Austin Council Member Delia Garza Elected Mayo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>In Latest Development About [Harris County] Ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>Live coverage: Texas lawmakers meet for the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Live coverage: Texas lawmakers meet for the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>I dont think the two are mutually exclusive,...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Five things to watch in the Texas Legislature ...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>A Texas State Senator has filed a bill that co...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>I'm Cassi Pollock, a reporter for The Texas Tr...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>AMA Announcement: Cassi Pollock of the Texas T...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Hidalgo to refuse donations from Harris County...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>Federal judge closes book on Houstons drag qu...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>Returning Texas Republicans In Congress Prepar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>Quality Pre-K Can Improve Childrens Health, B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Sen. Cruz, Rep. Rooney Introduce Constitutiona...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>What new marijuana laws might pass in Texas th...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>School Finance Reform &amp;amp; Property Tax Refo...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>How An Oil Boom in West Texas Is Reshaping the...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>The Texas Education Challenge</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Joe Straus: I'm leaving the Texas Legislature ...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Dennis Bonnen has spent half his life in the T...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>One Texas county just swore in 17 black female...</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Texas Democrat Introduces Bill to Remove Ban o...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>AG Paxton Releases Statement on U.S. Distr...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>ACLU sues Texas over law that says contractors...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>Rice Professor Mark Jones says Texas would be ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Texan of the Year 2018: Laura W. Bush | Dallas...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Voting question: Voting in primaries, moving b...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>Betos viral video explains the overlooked rea...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>Discipline rates higher for Texas special educ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Ken Paxton's allies are trying to kill case ag...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>AP Exclusive: Tornillo facility staying open i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>Trump threatens to shut U.S. border with Mexic...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>In fiery filing, Ken Paxton prosecutors ask Te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>Is It OK To Criticize Politicians For Things T...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Texas Gov. Greg Abbott vacations in Japan, tak...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Taxpayers need to know how money is spent, say...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>San Antonio Congressman Joaquin Castro Calls f...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Texas Takes The Next Step To Make College More...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Ted Cruzs anti-Obamacare crusade continues wi...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>ICE Quietly Drops 200 Asylum Seekers at El Pas...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Texas Tent City Housing 2,500 Migrant Children...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Texas One-Stop Shopping for Judge in Health C...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Beto O'Rourke Demands Closure Of Migrant Camp...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>The 2020 Democratic frontrunner is a Republican</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Inside Bernie-world's war on Beto O'Rourke</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Closed Until Further Notice, a letter from Bet...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Beto ORourke Reflects On the Border as His Te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  num_comments  ca\n",
       "0    State May Push Cities and Counties to Draw \"fa...             0   1\n",
       "1    New California rules for deadly police force g...             3   1\n",
       "2    The NRA Opposes A California Gun Regulation It...            21   1\n",
       "3    [CA-15] Eric Swalwell is expected to withdraw ...             5   1\n",
       "4    Eric Swalwell expected to end presidential bid...             4   1\n",
       "5    Tom Steyer Is Telling Allies Hes Running for ...             9   1\n",
       "6    California's Governor is Asking Trump for Emer...            29   1\n",
       "7    State Promises to Rebuild: Ridgecrest Will Not...             4   1\n",
       "8    How California made a 'dramatic' impact on kin...             0   1\n",
       "9    California's Politically Powerful Unions Aim T...            12   1\n",
       "10              New state budget a windfall for unions            15   1\n",
       "11   End the federal prohibition of marijuana: vote...             2   1\n",
       "12   Overregulation is one reason for the high cost...            59   1\n",
       "13   How Big Soda used its clout to stop 5 of 5 C...            19   1\n",
       "14   Kamala Harris Flip Flops on the Elimination of...            64   1\n",
       "15   California Democrats to allow non-citizens to ...             6   1\n",
       "16   Trump Thinks Putins Attack on Western-Style ...            34   1\n",
       "17   Police accountability: Six months after new Ca...             2   1\n",
       "18   One year ago, L.A. approved an ambitious housi...            24   1\n",
       "19   Newsom calls out Californias racist first gov...            16   1\n",
       "20   Californias proposed opioid tax would hurt pa...             2   1\n",
       "21   Megan Dahle announces run for husbands state ...             2   1\n",
       "22   [CA-50] Duncan Hunter 'Marital Spat,' Italian ...             0   1\n",
       "23   Indictments Charge Widespread Voting Fraud Sch...             4   1\n",
       "24   [CA-50] Rep. Duncan Hunter's Affairs Were Simp...             1   1\n",
       "25   Berkeley moves to the forefront in California ...             1   1\n",
       "26   [CA-50] Rep. Duncan Hunter Will Resign, Says D...             2   1\n",
       "27   Gavin Newsoms biggest accomplishment as gover...            15   1\n",
       "28   [CA-AD-73] Orange County Supervisor Lisa Bartl...             0   1\n",
       "29   California Gov. Gavin Newsom has signed his fi...            30   1\n",
       "30   Why cant California pass more housing legisla...            26   1\n",
       "31   Why California is so liberal and progressive i...            84   1\n",
       "32   [CA-50] Former Hill staffer alleges Rep. Dunca...             7   1\n",
       "33   Are California utilities doing enough to firep...             1   1\n",
       "34   Many people are moving from California to Texa...            13   1\n",
       "35   Lawmakerscutting big checks to combat the hou...             1   1\n",
       "36   California trails in regulating short-term len...             0   1\n",
       "37   Commentary: Gavin Newsom faces his first defin...             8   1\n",
       "38   A coworker told me that, due to a proposition ...             6   1\n",
       "39   Meet Rev. Andy Bales  The Man Who Lost A Leg ...             2   1\n",
       "40   Housing barbs spark backlash against Cupertino...             2   1\n",
       "41   [CA-50] Rep. Duncan Hunter used campaign funds...            25   1\n",
       "42   Who Can Be On The California Redistricting Com...             0   1\n",
       "43   Why a California state senator wants to ban ci...             2   1\n",
       "44   Any spike in repeat crimes after California sp...             3   1\n",
       "45   State Unemployment Drops; Job Growth, Shrinkin...             0   1\n",
       "46   [San Diego Mayor] Former Gov. Jerry Brown, ex-...            13   1\n",
       "47   California program to track state worker haras...             0   1\n",
       "48   California Gov. Gavin Newsoms not-so-spectacu...            16   1\n",
       "49   NCAA says California schools could be banned f...             9   1\n",
       "50   How can California solve its homeless crisis? ...             3   1\n",
       "51   Treat workers as employees? Uber, Lyft and oth...             8   1\n",
       "52   California gas tax goes up July 1, but leaders...            22   1\n",
       "53   Californias budget offers new help for millio...             5   1\n",
       "54   [CA-AD-73] Orange County Supervisor Lisa Bartl...             2   1\n",
       "55   Berkeley helps to push back against excessive ...             0   1\n",
       "56      The Future of the California Republican Party?            12   1\n",
       "57   California vaccine bill clears Assembly panel ...            18   1\n",
       "58   California political parties couldnt use ind...            10   1\n",
       "59   Gov. Newsom, Bay Area leaders respond to propo...             7   1\n",
       "60   [CA-50] Darrell Issa, the former Oversight Com...             5   1\n",
       "61   Its been a mess for decades. Can Gov. Newsom ...             8   1\n",
       "62   California State University stashed $1.5 billi...            11   1\n",
       "63   California is one step closer to passing the m...            12   1\n",
       "64   A million independent voters risk being irrele...             4   1\n",
       "65   George Takei: 'At Least During the Internment ...            12   1\n",
       "66   a chart of what Californians will pay annually...             0   1\n",
       "67   Poster for the upcoming CNP convention (June 2...            16   1\n",
       "68   In Los Angeles, only people of color are sente...             1   1\n",
       "69   California Activists Participate in Muslim Da...             4   1\n",
       "70   California Legislature must act to protect env...             0   1\n",
       "71   Immigrant entrepreneurs continue to shape Cali...             0   1\n",
       "72   How Redding, California, became an unlikely ep...            12   1\n",
       "73         Crime fighting robot deployed in California             5   1\n",
       "74   Commentary: Cities pledge to find solutions to...            10   1\n",
       "75   Wet California winter is a boon for skiers and...             0   1\n",
       "76   Rent Control Measure Returns, Legislative Anal...            70   1\n",
       "77   Bay Area Foothills College student homelessnes...             4   1\n",
       "78   California needs a big pot of money for wildfi...             1   1\n",
       "79   Redding Town Hall receives criticism after \"Ch...            17   1\n",
       "80   The Homeless Are Dying in Record Numbers on th...            68   1\n",
       "81   Open Forum: Exaggerating California crime to p...            13   1\n",
       "82   California Legislation to Limit Predatory Lend...             9   1\n",
       "83   Duncan Hunter's wife agrees to cooperate with ...             1   1\n",
       "84           Warren rises to second in California poll            62   1\n",
       "85   Where Gov. Gavin Newsom wins and loses in newl...            27   1\n",
       "86   California just passed a $215 billion budget. ...             2   1\n",
       "87   California Democratic 2020 presidential primar...             0   1\n",
       "88   New report: Google campus will lead to $235M m...             1   1\n",
       "89   California may automatically expunge 1 million...             0   1\n",
       "90   Abuse in the Disabled Care Indjstry in California             3   1\n",
       "91   California National Party Convention on June 22nd             4   1\n",
       "92   Towing a car can be financially ruinous. Shoul...            22   1\n",
       "93   [CA-50] Margaret Hunter to plead out in case a...             1   1\n",
       "94   Leslie Marshall: California is right to give i...             5   1\n",
       "95   LGBTQ nightclub in Fresno targeted by threats,...             2   1\n",
       "96   L.A. council members propose taxing landlords ...             1   1\n",
       "97   A new Citizens Redistricting Commission is bei...             1   1\n",
       "98   Homeless crisis: Los Angeles County seeks help...             4   1\n",
       "99   [CA-39, CA-45, CA-48, CA-49] Orange County Rep...            20   1\n",
       "100  California considers ban on facial recognition...             1   1\n",
       "101  Heres how California can become a tuition-fre...             5   1\n",
       "102  Should community colleges in California build ...            41   1\n",
       "103  Does this vaccine bill go too far? Concerned f...             7   1\n",
       "104  Report links evictions, homelessness in Los An...             1   1\n",
       "105  Keeping an eye on sheriffs: California Democra...             4   1\n",
       "106  White House rejects carmakers plea for a deal...            25   1\n",
       "107  Gov. Newsom stepped into a vaccine debate we s...             8   1\n",
       "108  In two California Senate special elections, Go...             0   1\n",
       "109  Californias top bullet train consultant is su...            30   1\n",
       "110           New California Could Become 51st State            24   1\n",
       "111  Swinging at Every Pitch: Californias Govern...            15   1\n",
       "112  In The Dreamt Land, Mark Arax unpacks the Gold...             1   1\n",
       "113  California Democratic voters disagree with Nan...             3   1\n",
       "114  Startup rents bunkbeds in San Francisco for $1...            33   1\n",
       "115  How California became far more energy-efficien...             0   1\n",
       "116  Californians favor dramatic changes to build m...             0   1\n",
       "117  Gov. Newsom criticized the new vaccine bill. A...            11   1\n",
       "118  Landlords win, renters take a hit. Just one te...            65   1\n",
       "119  How Californias big plans to address housing ...             3   1\n",
       "120  Californias long-overlooked Central Valley ho...             0   1\n",
       "121     Brace for a Spike in Homelessness in LA County             5   1\n",
       "122  California Democratic Party elects L.A. labor ...             4   1\n",
       "123  Northern California state Senate special elect...             1   1\n",
       "124  Say goodbye to your local precinct. Voting in ...            29   1\n",
       "125  The New Front in the SB 50 Battle Is Toni Atki...             1   1\n",
       "126  The looming California challenge for Kamala Ha...             6   1\n",
       "127  Californias state universities are a path to ...            49   1\n",
       "128  The soul-crushing cost of college in Californi...             5   1\n",
       "129  Death Watch: Keeping track of the bills Califo...             1   1\n",
       "130  Cruel and Unusual: A Guide to Californias Bro...             2   1\n",
       "131  California plan to prevent big rent hikes adva...             2   1\n",
       "132  A California bill to ban flavored tobacco prod...            23   1\n",
       "133  California gasoline prices increase following ...             2   1\n",
       "134  Rethinking Disaster Recovery After A Californi...             5   1\n",
       "135  Los Angeles County Bans Use of Roundup Weed Ki...            36   1\n",
       "136  Think Californias too big and influential? Wa...            26   1\n",
       "137       California probably will pick next president            29   1\n",
       "138  In need of teacher housing, more California sc...            12   1\n",
       "139  Two tax hikes for schools could end up on Cali...            47   1\n",
       "140  [CA-50] Rep. Hunter on War-Crimes Suspect Gall...             9   1\n",
       "141  California says it's now in compliance with US...             5   1\n",
       "142  In California, Agreement On New Rules For When...            13   1\n",
       "143  Why California's Efforts To Limit Soda Keep Fi...            27   1\n",
       "144  Congress reaches deal on disaster aid: Califor...             3   1\n",
       "145  Editorial: California needs to resume investme...             3   1\n",
       "146  California Senate passes legislation to create...             5   1\n",
       "147  California GOP picks favorites for re-flipping...            14   1\n",
       "148  California regulators arent taking action aga...             0   1\n",
       "149  Trump threatens to cut millions from fire depa...            13   1\n",
       "150  Poll: Two-thirds of California voters back SB ...             9   1\n",
       "151  SF District Attorney Gascn questions SF polic...             3   1\n",
       "152  How Legalization Changed Humboldt County Marij...             0   1\n",
       "153  Brawl erupts at convention for local-governmen...             0   1\n",
       "154  Retired Oil Rigs off the California Coast Coul...             0   1\n",
       "155  Its time to politically destroy Kevin McCart...            38   1\n",
       "156  Pipeline Shutdown Prevented 27 Million Tons of...             4   1\n",
       "157  Trump tears into California for high-speed rai...             2   1\n",
       "158  Guns, gas and soda  most California tax propo...             8   1\n",
       "159  Commentary: These are the key conflicts in Cal...             0   1\n",
       "160  Alabama and Georgia passed abortion bans. Cali...             9   1\n",
       "161  Report: Gas price hike could be due to manipul...             1   1\n",
       "162  Assemblyman Marc Bermans college car camping ...             0   1\n",
       "163  High-profile California housing bill dies with...            57   1\n",
       "164  Trump Wants to Open Public Lands to Oil Drilli...            12   1\n",
       "165  Kamala Harris still not catching on with Democ...            10   1\n",
       "166                   Californias New Prohibitionists             0   1\n",
       "167  Trump pardons media tycoon, former GOP leader ...             0   1\n",
       "168  Oakland Unifieds journey: When the state step...             0   1\n",
       "169  California Primary Becomes a Tantalizing Prize...            16   1\n",
       "170  Simon Liu Isnt A Sex Offender. But Hes Still...             1   1\n",
       "171  California is bringing law and order to big da...             0   1\n",
       "172  California files 50th lawsuit against Trump ad...             2   1\n",
       "173  California might triple number of marijuana sh...            11   1\n",
       "174  California on track to lose at least one congr...             5   1\n",
       "175  Bill to stiffen Californias vaccine law must ...             4   1\n",
       "176  California could bring radical change to singl...            12   1\n",
       "177  White House Hopefuls Swarm Rivals Home Turf o...             5   1\n",
       "178  My patient was homeless. I knew she was going ...            15   1\n",
       "179  Only one California Republican defied Trump on...            13   1\n",
       "180     CA Democratic Conventionphotography questions             5   1\n",
       "181  California governor unveils record $213 billio...             7   1\n",
       "182  Trump Finalizes Plan to Open 725,500 Acres of ...            27   1\n",
       "183  Potential Impact of Californias Split Roll ...             5   1\n",
       "184  Revenge of the Coastal Elites: How California,...             2   1\n",
       "185  California defies Trump to ban pesticide linke...             2   1\n",
       "186  Biden meets with big-dollar California donors,...             1   1\n",
       "187  Gavin Newsoms California budget rises to $213...             5   1\n",
       "188          Make California map chart with your data.             0   1\n",
       "189  Mayor Pete blindsides Kamala Harris in California             5   1\n",
       "190  Gavin Newsom wants to end California taxes on ...            18   1\n",
       "191  Hot off the grille: Is California ready to leg...             1   1\n",
       "192  In Trump vs. California, the state is winning ...             8   1\n",
       "193  California Activists Take First Steps To Decri...            12   1\n",
       "194  State officials keep hiring their relatives. W...            11   1\n",
       "195  More reasons to be concerned with election int...             5   1\n",
       "196  The Trump Administration Stopped Working On Ca...            34   1\n",
       "197  Swastika leaflets calling press the enemy dr...            14   1\n",
       "198  In a Galaxy Not So Far Away: California Offici...            24   1\n",
       "199  When the next recession hits, will California ...             9   1\n",
       "200  Californias 2018 midterm election: A dive int...             0   1\n",
       "201  Reminder: Only post matters that are specific ...            21   1\n",
       "202  How powerful lawmakers are killing California ...            14   1\n",
       "203  California bill fighting discrimination of 'na...            12   1\n",
       "204  The kingmakers in Californias 2020 elections ...             6   1\n",
       "205  California Economy Soars Above U.K., France an...            36   1\n",
       "206  Los Angeles sets dramatic new goals for electr...             4   1\n",
       "207  A doctored photo and a lawsuit: California GOP...            16   1\n",
       "208  Freshman Rep. Katie Hill, who was part of the ...             6   1\n",
       "209  Trump abortion policy targeting Planned Parent...            13   1\n",
       "210  New Election Ordered After Ca. Dems Caught Che...             5   1\n",
       "211  Californias Hidden Corporate Tax Cuts  If ci...             1   1\n",
       "212  Trump fracking plan targets over 1 million acr...             9   1\n",
       "213  San Francisco approves homeless shelter despit...            50   1\n",
       "214  Joe Biden lacks big-name California allies as ...            63   1\n",
       "215  California rent control moves forward with Gav...             7   1\n",
       "216  Morgan Stanley to pay $150 million to settle C...             1   1\n",
       "217  California May Ban Schools From Suspending Stu...             4   1\n",
       "218   Andrew Yangs Campaign Grows with Large LA Rally             8   1\n",
       "219  Anti-vaccine families crowd California Capitol...             8   1\n",
       "220  Rep. Katie Hill has Nancy Pelosis favor, but ...             0   1\n",
       "221  Newsom seeks an explanation for Californias h...            57   1\n",
       "222  L.A. students want to lower voting age in scho...            31   1\n",
       "223               Sideshows and the Extractive Economy             0   1\n",
       "224                             Combating homelessness             0   1\n",
       "225  Temecula will weigh city resolution to describ...            26   1\n",
       "226  California Cities Have Shredded Decades of Pol...             1   1\n",
       "227  Republicans Lining Up for 2020 House Fights in...             9   1\n",
       "228  Beto ORourke opens his California campaign Sa...             3   1\n",
       "229  How Trump factors into California's charter sc...             0   1\n",
       "230  Bill To End Hair Discrimination In The Workpla...             1   1\n",
       "231  Use of Force bills AB 392 and SB 230 now linke...             1   1\n",
       "232  Julin Castro Is First 2020 Democratic Preside...             4   1\n",
       "233  CA-15: State Sen. Bob Wieckowski is forming an...             1   1\n",
       "234  California housing bill targeting wealthy citi...             6   1\n",
       "235  California Democrats are awash in cash as the ...             0   1\n",
       "236  How Gov. Gavin Newsom is progressing on his ke...            18   1\n",
       "237  Green Solutions for Governor Newsom's Climate ...             0   1\n",
       "238  Kamala Harris regrets California truancy law t...            17   1\n",
       "239  The Public Banking Revolution Is Upon Us: Cali...            18   1\n",
       "240  [CA-50] Rep. Hunter pretends to cross the Mexi...             1   1\n",
       "241  [L.A. County Supervisor (District 4)] Former a...             0   1\n",
       "242  Buttigieg plans aggressive fundraising push in...            38   1\n",
       "243  Former California water lobbyist, Trump's Inte...             0   1\n",
       "244  Fastest litigant in the west: Californias on ...             6   1\n",
       "245  U.S. appeals court upholds most of California...             2   1\n",
       "246  California gives out too many tax breaks. And ...             6   1\n",
       "247                  Swalwell calls for Barr to resign             3   1\n",
       "248  Years of bad rules led to Californias unfixab...            52   1\n",
       "249  Kamala Harris has collected millions from big ...             9   1\n",
       "..                                                 ...           ...  ..\n",
       "732  In Wake Of Measles Cases, Health Advocates Wan...             9   0\n",
       "733  Proposed law would make hemp products legal in...            23   0\n",
       "734                Red-Light Camera Ban Filed in House            13   0\n",
       "735  MJ Hegar  Texas veteran behind viral \"Doors\" ...            13   0\n",
       "736  How Will Texas Lawmakers Pay For School Financ...             3   0\n",
       "737  Go big (with our bandwidth) or go home, Verizo...             0   0\n",
       "738  Thousands of Texans were shocked by surprise m...             7   0\n",
       "739  Quorum Report: Business effort begins at the C...            19   0\n",
       "740  McAllen orders Catholic Charities to vacate im...            33   0\n",
       "741  Texas Senate Advances Property Tax Reform To F...            10   0\n",
       "742  El Paso Fire Department denies Trump's crowd c...             4   0\n",
       "743  HD125 with 100% reporting: Republican Fred Ran...             1   0\n",
       "744  Images From the Dueling Trump and Beto O'Rourk...            39   0\n",
       "745  Four Democrats and one Republican vie for Just...             0   0\n",
       "746  Judges Ruling Could Have Big Implications For...             8   0\n",
       "747  The AG's office told lawmakers it isnt invest...            17   0\n",
       "748  Beto ORourke to hold counter-speech same time...            58   0\n",
       "749  Texas Attorney General wont investigate voter...             5   0\n",
       "750  Nearly 100 Texans have submitted ideas for wha...             5   0\n",
       "751  Texas Public Opinion on Donald Trump, Immigrat...             7   0\n",
       "752  Texas Dan Patrick claims taxes are too high. ...            41   0\n",
       "753  Richardson councilman, Scott Dunn, issues apol...            64   0\n",
       "754  Bill: HB 1408, introduced by Texas Rep. Jared ...             1   0\n",
       "755  Texas Has Been Just a Prop for Trump From the ...            11   0\n",
       "756  Texas bill would ban throttling in disaster ar...             5   0\n",
       "757  Texas AG Ken Paxton Says He Hasnt Launched Cr...             3   0\n",
       "758  Texas Secretary of State David Whitley defends...            14   0\n",
       "759  Fact Checking the Voter Fraud Debacle. With st...            49   0\n",
       "760  Analysis: A Green Appointees [Sec. of State D...             0   0\n",
       "761  Navy veteran challenges fellow Navy veteran Re...             6   0\n",
       "762  Some Texas Democrats want Beto O'Rourke to shu...            47   0\n",
       "763  Trump's underwater (-1) in Texas for the first...            10   0\n",
       "764  Despite running in a midterm year, Beto ORour...             9   0\n",
       "765  Texas Republican Accused of Sending Student Nu...             0   0\n",
       "766  A border fence did not lower crime rates in El...             5   0\n",
       "767  Native Americans protest building border wall ...            30   0\n",
       "768  Beto ORourke Was Once Adrift in New York City...             7   0\n",
       "769  Texas Gov. Greg Abbott give his State of the S...            14   0\n",
       "770  Rick Perry is the Designated Survivor for to...            10   0\n",
       "771  Woman who filmed shooting of Botham Jean fired...             9   0\n",
       "772  Rattled by CBD Debate, Shop Owners React: I D...             0   0\n",
       "773  Texas lawmaker wants cars with MSRP over $60k ...            11   0\n",
       "774  More Civil Rights Groups Sue Over Secretary Of...            10   0\n",
       "775  Texas Republicans Are Lying About Voter Fraud ...             4   0\n",
       "776  Abbott Names School Finance, Property Taxes, M...             2   0\n",
       "777  Texas Public Opinion and Governor Abbott's Eme...             1   0\n",
       "778  Elizabeth Warren APOLOGIZES for Native America...             7   0\n",
       "779  Ted Cruz just compared rape victims to a man w...             9   0\n",
       "780  Hispanics are propping up Texas economy, workf...            10   0\n",
       "781  Property tax relief, but only for some: small ...             2   0\n",
       "782  Audio: Border Patrol Plans to Light Up Butterf...             0   0\n",
       "783    Texas AG asks federal judge to end DACA program             6   0\n",
       "784  Texans Can Appeal Surprise Medical Bills, But ...             0   0\n",
       "785   Cornyn braces for brutal Texas reelection battle             1   0\n",
       "786  Despite beer and lobby ties, Speaker Dennis Bo...             2   0\n",
       "787     Can Oprah help restore the Beto ORourke glow?             2   0\n",
       "788  Texas Attorney General Ken Paxton is seeking m...            26   0\n",
       "789              President Trumps Texas-size whoppers            14   0\n",
       "790  Far-Right Texas Legislator Files Bill to Compl...             2   0\n",
       "791  A No-Knock Raid in Houston Led to Deaths and P...            10   0\n",
       "792  US prepares to start building portion of Texas...             2   0\n",
       "793  Rural Texas needs more veterinarians, and Texa...             4   0\n",
       "794  Texas optometrists \"just roll our eyes\" over t...             6   0\n",
       "795  Texas judge says Sutherland Springs families c...             3   0\n",
       "796  Should someone propose a 70% tax on the Patri...             4   0\n",
       "797  Dems are headed to Texas to probe suspected vo...             7   0\n",
       "798  What goes on in the secret world of private pr...             0   0\n",
       "799  Americans like me don't belong on Texas' botch...            11   0\n",
       "800  Naturalized citizens suing over Texas voter ci...             2   0\n",
       "801  Naturalized citizen is angry to find her name ...            32   0\n",
       "802  Civil rights group sues Texas over order to in...             1   0\n",
       "803                        100 Richest people in Texas            43   0\n",
       "804  The Texas voter purge (Vox Overview of Recent ...             1   0\n",
       "805  Healing Power or a Dose of Trouble? CBD Oil Ta...             4   0\n",
       "806  State: All 366 on local list of potential nonc...            18   0\n",
       "807                 Question for the sub conservatives            86   0\n",
       "808  In Harris County, Thousands Of Registered Vote...             7   0\n",
       "809  Armed Man Disrupts Houston Library During Drag...             3   0\n",
       "810  Texas lawmakers file identical companion bills...            30   0\n",
       "811  Indigenous Activists Set Up Protest Camp at So...             0   0\n",
       "812  Texas Gov. Greg Abbott Downplays Concerns Abou...             4   0\n",
       "813  The Woman Behind the Kamala Harris Presidentia...             4   0\n",
       "814  Texas Officials Begin Walking Back Allegations...            20   0\n",
       "815  Many see \"Robin Hood\" as a villain. But lawmak...             3   0\n",
       "816  New legislation would allow Texas liquor store...            73   0\n",
       "817  Former Proud Boys trial canceled after witn...            25   0\n",
       "818  Analysis: Texas election officials serve up a ...             1   0\n",
       "819  Heres What You Need to Know About the Texas V...             0   0\n",
       "820  Texas Republicans fear Trump could lose the st...            24   0\n",
       "821  The Poor in Texas Have Been Vastly Undercounte...            12   0\n",
       "822  Top Texas Democrat rules out funding Trump's b...             1   0\n",
       "823  Democrat Art Fierro wins #HD79 special electio...             0   0\n",
       "824  Texas quietly tells counties that some of the ...             6   0\n",
       "825  Texas Republican introduces bill to make discr...             4   0\n",
       "826  2 Democrats headed to runoff in race to replac...             0   0\n",
       "827  [Special Election Today HD 79 &amp; HD 145] Vo...             1   0\n",
       "828  President Donald Trump: \"58,000 non-citizens v...             9   0\n",
       "829  Texas officials sued by Latino group over sugg...             0   0\n",
       "830  After Ted Cruz's close race against Beto O'Rou...            19   0\n",
       "831  'We were not welcomed': Gay couple rejected by...            21   0\n",
       "832  Harris County GOP draws fire for post blaming ...            77   0\n",
       "833   Seliger, West Texas deserve better from Patrick.             1   0\n",
       "834  Winners and losers: When Texas House's powerfu...             4   0\n",
       "835  Texas says it has found 95,000 non citizens on...            29   0\n",
       "836  As it ponders where to put a Confederate plaqu...            47   0\n",
       "837  3 air traffic controllers in Texas resign over...            36   0\n",
       "838  TexSoS: Use of Non-U.S. Citizen Data obtained ...             5   0\n",
       "839  [Meta] What happened to Moderator /u/Lemon_Lym...             3   0\n",
       "840  Texas Sec. of State: 58K non-US citizens voted...            13   0\n",
       "841  Texas Border Sheriffs: There is No Crisis and ...            42   0\n",
       "842  Opinion | I joined the GOP because it stood fo...            10   0\n",
       "843  Most members of the Texas Legislature are whit...            18   0\n",
       "844  AG Announcement: 56,000 non-citizens have vote...            36   0\n",
       "845          Bennet Rips Cruz | User Clip | C-SPAN.org            13   0\n",
       "846  Exclusive: White House preparing draft nationa...             2   0\n",
       "847  Texas conservatives claim LGBTQ equality bill ...             2   0\n",
       "848  Where Texans in Congress come down on the gove...             5   0\n",
       "849  Texas House committee appointments bode well f...             4   0\n",
       "850  Every TX GOP House Member (except for William ...            13   0\n",
       "851  Texas Comptroller releases new report on Schoo...             1   0\n",
       "852  Johnson Space Center workers being asked to cl...             3   0\n",
       "853  Texas man organizes 'search party' event to lo...             8   0\n",
       "854  Texas House Speaker Dennis Bonnen names commit...             0   0\n",
       "855  Sheila Jackson Lee Leaves 2 Posts After Aide S...             0   0\n",
       "856  Sheila Jackson Lee steps down from key posts a...             0   0\n",
       "857  Cornyn and Cruz vote against motion to keep Ru...            44   0\n",
       "858  Texas House Speaker appoints three Dallas lawm...             0   0\n",
       "859  Dan Crenshaw: Only six Democrats voted to pay ...            12   0\n",
       "860   Texas Rep. Eric Johnson running for Dallas mayor             3   0\n",
       "861  Some Parents Concerned Increased School Fundin...             5   0\n",
       "862  To combat opioid addiction crisis, Texas AG Ke...             1   0\n",
       "863  Lt. Gov. Dan Patrick pulls Sen. Kel Seligers ...             3   0\n",
       "864  Bullshit is how one Texan describes Trump an...            36   0\n",
       "865  2019's Most Educated States in America: Texas #39             6   0\n",
       "866  Texas coal power plants leaching toxic polluta...            16   0\n",
       "867  Trump Touts Border Wall In San Antonio, Which ...            48   0\n",
       "868  Texas Judge Tells Jury God Says Defendant Is N...            11   0\n",
       "869  Beto ORourkes road trip drives home his message             4   0\n",
       "870  Rep. Will Hurd (R-Texas), calls Trump's border...            14   0\n",
       "871  The Big Dogs Of Texas Local Government: 18,000...             3   0\n",
       "872  Court rules against Planned Parenthood in Texa...             3   0\n",
       "873           Texas Unions are ready for this session.             0   0\n",
       "874             7 Feb marijuana lobbying day in Austin            17   0\n",
       "875  GOP Rep. Will Hurd calls wall \"least effective...             2   0\n",
       "876  Texas Rep. Castro wants Trump impeached after ...            32   0\n",
       "877  GOP tries to re-create its special-election ma...             1   0\n",
       "878  Court rules Texas can bar Planned Parenthood f...            13   0\n",
       "879  Both Cornyn and Cruz voted to protect Russia s...            19   0\n",
       "880  During the shutdown, government lawyers in Sou...             2   0\n",
       "881  How will the Texas Legislature address school ...            81   0\n",
       "882  Report: Power Plants are Leaking Cancer-Causin...             4   0\n",
       "883  Harris County judges unveil drastic new plan f...            15   0\n",
       "884  During the shutdown, government lawyers in Sou...             0   0\n",
       "885  We Just Listened to Him Talk: Sister Norma P...             0   0\n",
       "886  Texas has most people without health insurance...            19   0\n",
       "887  Tyler Congressman Louie Gohmert says Steve Kin...             3   0\n",
       "888  Will 2019 Be The Year Texas Breweries Can Sell...            17   0\n",
       "889  Can Texas Build a Working Medical Cannabis Pro...            22   0\n",
       "890  Governor, top Texans in Congress ask Trump not...             0   0\n",
       "891  Texas House proposes massive increase for publ...            67   0\n",
       "892  Beto ORourkes Washington Post Interview Spel...             3   0\n",
       "893  Congresswoman Alexandria Ocasio-Cortez Joins S...            69   0\n",
       "894  Texas Senate wants billions to fund $5,000 pay...            16   0\n",
       "895  Texas Rep. Dan Crenshaw under fire for shutdow...             5   0\n",
       "896  Just 87 people voted today in the HD145 specia...             0   0\n",
       "897  The Rise and Fall of the Tornillo Tent City fo...             0   0\n",
       "898  Why Americas Largest Migrant Youth Detention ...             0   0\n",
       "899  Beto ORourkes immigration plan: No wall but ...             3   0\n",
       "900  Ted Cruz defends Trump on Russia: \"I don't see...            57   0\n",
       "901  We're up to five candidates filed for #HD125 s...             0   0\n",
       "902  Medicaid, opioids and abortion: Health care is...             0   0\n",
       "903  [slimy former US Rep] Farenthold resigns as Ca...             5   0\n",
       "904  Chip Roy of CD21. Workers will forever remembe...            45   0\n",
       "905  Julin Castro, Former Housing Secretary, Annou...            28   0\n",
       "906  El Paso Times column refutes AG Paxton's claim...             7   0\n",
       "907  Tarrant County GOP votes to retain Muslim vice...             6   0\n",
       "908  Texas officials vote to remove Confederate pla...            14   0\n",
       "909  Texas lawmakers indicate they may use rainy da...             5   0\n",
       "910  New Anti-LGBT Initiative Pops up Despite What ...            11   0\n",
       "911  Democrat Julian Castro expected to launch 2020...             1   0\n",
       "912    Texas ranked No.1 state for women entrepreneurs            31   0\n",
       "913  Donald Trump says Lt. Gov. Dan Patrick offered...             7   0\n",
       "914  Trump Plans To Use Eminent Domain Against Priv...            67   0\n",
       "915  Texas Republicans Rally Behind Muslim Official...             0   0\n",
       "916  Tarrant County GOP votes to keep vice chair de...             3   0\n",
       "917  Interesting response from Senator Cornyn's office            12   0\n",
       "918        Cruz defends eminent domain for border wall            32   0\n",
       "919  Tarrant County GOPs vice-chairman survives re...             0   0\n",
       "920  One small group of private citizens is posted ...             2   0\n",
       "921  What some border residents feel might be the b...            34   0\n",
       "922  Rep Bonnen replaces coffee cups in the break r...            19   0\n",
       "923  Climate change science presented to uncertain ...            18   0\n",
       "924  Victims of DV having right to a Court Appointe...             3   0\n",
       "925  Newly elected House Speaker Dennis Bonnen says...             9   0\n",
       "926  Beto O'Rourke plans nationwide road trip to me...             0   0\n",
       "927  GOP operatives dig for dirt against rising sta...            27   0\n",
       "928  Scientists To Abbott: Climate Change Is Happe...             6   0\n",
       "929  San Antonio lands Texas first opportunity zo...            11   0\n",
       "930  The Texas Legislature Gaveled In Today Without...             0   0\n",
       "931  Border crisis echo in Texas is faint as lawm...             0   0\n",
       "932  If the Texas Legislature wont help solve Dall...             2   0\n",
       "933  Texas libertarian Ron Paul: We don't need Trum...             1   0\n",
       "934  Texas economy is robust, giving lawmakers $9...             5   0\n",
       "935  Austin Council Member Delia Garza Elected Mayo...             0   0\n",
       "936  In Latest Development About [Harris County] Ba...             0   0\n",
       "937  Live coverage: Texas lawmakers meet for the st...             0   0\n",
       "938  Live coverage: Texas lawmakers meet for the st...             0   0\n",
       "939  I dont think the two are mutually exclusive,...            41   0\n",
       "940  Five things to watch in the Texas Legislature ...            29   0\n",
       "941  A Texas State Senator has filed a bill that co...            14   0\n",
       "942  I'm Cassi Pollock, a reporter for The Texas Tr...            57   0\n",
       "943  AMA Announcement: Cassi Pollock of the Texas T...             2   0\n",
       "944  Hidalgo to refuse donations from Harris County...             2   0\n",
       "945  Federal judge closes book on Houstons drag qu...            19   0\n",
       "946  Returning Texas Republicans In Congress Prepar...             0   0\n",
       "947  Quality Pre-K Can Improve Childrens Health, B...             0   0\n",
       "948  Sen. Cruz, Rep. Rooney Introduce Constitutiona...            33   0\n",
       "949  What new marijuana laws might pass in Texas th...            65   0\n",
       "950  School Finance Reform &amp; Property Tax Refo...            14   0\n",
       "951  How An Oil Boom in West Texas Is Reshaping the...            15   0\n",
       "952                      The Texas Education Challenge            10   0\n",
       "953  Joe Straus: I'm leaving the Texas Legislature ...            17   0\n",
       "954  Dennis Bonnen has spent half his life in the T...             4   0\n",
       "955  One Texas county just swore in 17 black female...            53   0\n",
       "956  Texas Democrat Introduces Bill to Remove Ban o...            16   0\n",
       "957  AG Paxton Releases Statement on U.S. Distr...            13   0\n",
       "958  ACLU sues Texas over law that says contractors...            26   0\n",
       "959  Rice Professor Mark Jones says Texas would be ...            12   0\n",
       "960  Texan of the Year 2018: Laura W. Bush | Dallas...             3   0\n",
       "961  Voting question: Voting in primaries, moving b...             3   0\n",
       "962  Betos viral video explains the overlooked rea...            15   0\n",
       "963  Discipline rates higher for Texas special educ...             2   0\n",
       "964  Ken Paxton's allies are trying to kill case ag...             3   0\n",
       "965  AP Exclusive: Tornillo facility staying open i...             1   0\n",
       "966  Trump threatens to shut U.S. border with Mexic...            36   0\n",
       "967  In fiery filing, Ken Paxton prosecutors ask Te...             1   0\n",
       "968  Is It OK To Criticize Politicians For Things T...            33   0\n",
       "969  Texas Gov. Greg Abbott vacations in Japan, tak...            19   0\n",
       "970  Taxpayers need to know how money is spent, say...             1   0\n",
       "971  San Antonio Congressman Joaquin Castro Calls f...             5   0\n",
       "972  Texas Takes The Next Step To Make College More...             9   0\n",
       "973  Ted Cruzs anti-Obamacare crusade continues wi...            12   0\n",
       "974  ICE Quietly Drops 200 Asylum Seekers at El Pas...            21   0\n",
       "975  Texas Tent City Housing 2,500 Migrant Children...             2   0\n",
       "976  Texas One-Stop Shopping for Judge in Health C...             1   0\n",
       "977  Beto O'Rourke Demands Closure Of Migrant Camp...            17   0\n",
       "978    The 2020 Democratic frontrunner is a Republican             3   0\n",
       "979         Inside Bernie-world's war on Beto O'Rourke            51   0\n",
       "980  Closed Until Further Notice, a letter from Bet...            27   0\n",
       "981  Beto ORourke Reflects On the Border as His Te...             1   0\n",
       "\n",
       "[1920 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_reddit).to_csv('reddit.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.511458\n",
       "1    0.488542\n",
       "Name: ca, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our baseline accuracy\n",
    "df_reddit['ca'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title']\n",
    "y = df_reddit['ca']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=79)\n",
    "pipe = Pipeline([\n",
    "            ('vec', CountVectorizer()),\n",
    "            ('model', LogisticRegression())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  61 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=2)]: Done 361 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=2)]: Done 717 out of 720 | elapsed:   35.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done 720 out of 720 | elapsed:   35.9s finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=4,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'vec__max_features': 1500, 'vec__min_df': 4, 'vec__ngram_range': (1, 2)}\n",
      "\n",
      " Cross Validation Accuracy Score: 0.91875\n",
      " Training Data Accuracy Score: 0.98125\n",
      " Testing Data Accuracy Score: 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'vec' : [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__max_features': [1500, 2000, 2500, 2700],\n",
    "    'vec__min_df': [2, 3, 4],\n",
    "#     'vec__max_df': [0.5, .60, .70],\n",
    "    'vec__ngram_range': [(1,2), (1,1)],\n",
    "    'model' : [LogisticRegression(), LogisticRegression(penalty='l1', solver='liblinear'), LogisticRegression(penalty='l2', solver='liblinear'), MultinomialNB]\n",
    "#     'vec__stop_words': ['english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5, verbose=1, n_jobs=2)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f' Best Parameters: {gs.best_params_}')\n",
    "print('')\n",
    "print(f' Cross Validation Accuracy Score: {gs.best_score_}')\n",
    "print(f' Training Data Accuracy Score: {gs.score(X_train, y_train)}')\n",
    "print(f' Testing Data Accuracy Score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.18865619, 0.03053555, 0.07005663, 0.03046961, 0.07289577,\n",
       "        0.02966752, 0.07243733, 0.03045788, 0.0748312 , 0.03077445,\n",
       "        0.06893911, 0.03069229, 0.07196903, 0.03061595, 0.07140865,\n",
       "        0.03032808, 0.07067137, 0.03006439, 0.07305241, 0.03057675,\n",
       "        0.07306881, 0.02957802, 0.07030916, 0.03063912, 0.06786838,\n",
       "        0.02912092, 0.06871009, 0.0287528 , 0.06891675, 0.02798734,\n",
       "        0.06915956, 0.02865872, 0.06998906, 0.03007245, 0.06853371,\n",
       "        0.03061614, 0.07114568, 0.03089533, 0.07210298, 0.0287921 ,\n",
       "        0.07042923, 0.02851915, 0.07878366, 0.02995596, 0.06798496,\n",
       "        0.02878227, 0.07238388, 0.02834811, 0.07018409, 0.02945461,\n",
       "        0.06937637, 0.02930965, 0.06950932, 0.02811322, 0.06867752,\n",
       "        0.02877254, 0.07135801, 0.02861009, 0.07596879, 0.02905493,\n",
       "        0.07254014, 0.02970834, 0.06847043, 0.02909288, 0.07176232,\n",
       "        0.02889848, 0.07128401, 0.03021598, 0.07185802, 0.03041396,\n",
       "        0.07037158, 0.02943029, 0.07186995, 0.03003359, 0.07369375,\n",
       "        0.03050823, 0.07050505, 0.02840695, 0.06912012, 0.03105593,\n",
       "        0.07231956, 0.02873302, 0.07130466, 0.02906137, 0.07051001,\n",
       "        0.0310082 , 0.0707458 , 0.02872462, 0.06819558, 0.03142185,\n",
       "        0.07247744, 0.02942257, 0.06972322, 0.03062849, 0.07683296,\n",
       "        0.0317306 , 0.07106447, 0.03043609, 0.0765295 , 0.02910686,\n",
       "        0.1054956 , 0.05844369, 0.09783301, 0.03137932, 0.06979661,\n",
       "        0.03069386, 0.06990542, 0.03002076, 0.07209954, 0.03002725,\n",
       "        0.07042155, 0.03229604, 0.08237081, 0.03799596, 0.0865849 ,\n",
       "        0.03469462, 0.08746018, 0.03196392, 0.09126606, 0.03249607,\n",
       "        0.07168107, 0.03202748, 0.07983766, 0.02984715, 0.0746006 ,\n",
       "        0.03348379, 0.07571564, 0.03015523, 0.07121153, 0.02848921,\n",
       "        0.06928535, 0.0280127 , 0.08050218, 0.03204002, 0.08110437,\n",
       "        0.03484755, 0.08439455, 0.02962794, 0.07389045, 0.03320661,\n",
       "        0.08224983, 0.03031158, 0.07600923, 0.03048639, 0.07973566,\n",
       "        0.03021855, 0.07196546, 0.02982311, 0.08131795, 0.03143153,\n",
       "        0.07864122, 0.03028798, 0.07530298, 0.03051343, 0.07932343,\n",
       "        0.03024707, 0.07747951, 0.03304501, 0.07541494, 0.02945046,\n",
       "        0.07953191, 0.03154898, 0.08516822, 0.03189411, 0.08476782,\n",
       "        0.03123407, 0.07533698, 0.03189187, 0.09247217, 0.03487329,\n",
       "        0.08748031, 0.03212562, 0.08039322, 0.03318958, 0.08026118,\n",
       "        0.03300734, 0.08256345, 0.03678885, 0.09099054, 0.03885484,\n",
       "        0.08125596, 0.0332818 , 0.08412056, 0.02907629, 0.07115684,\n",
       "        0.02951937, 0.10697241, 0.03097181, 0.08310938, 0.02986045,\n",
       "        0.07177706, 0.03389187]),\n",
       " 'std_fit_time': array([0.14465076, 0.00106576, 0.00124527, 0.00044418, 0.00338671,\n",
       "        0.00170225, 0.00235424, 0.00166945, 0.00290333, 0.00170284,\n",
       "        0.00179593, 0.00207138, 0.0025728 , 0.00178704, 0.00488449,\n",
       "        0.00142418, 0.00095994, 0.00091685, 0.00473583, 0.00165994,\n",
       "        0.00477863, 0.00172816, 0.00262185, 0.00194594, 0.00193279,\n",
       "        0.00161664, 0.00117796, 0.00116827, 0.00297302, 0.00089667,\n",
       "        0.00116186, 0.00108657, 0.00241561, 0.00307369, 0.00257364,\n",
       "        0.00278126, 0.00330305, 0.00146395, 0.00255592, 0.00140332,\n",
       "        0.00239847, 0.00087145, 0.00800027, 0.00068801, 0.00277404,\n",
       "        0.00143143, 0.00379318, 0.00108263, 0.0018716 , 0.00092836,\n",
       "        0.00184051, 0.00136678, 0.00158395, 0.00102682, 0.00229014,\n",
       "        0.00094958, 0.00264575, 0.00123279, 0.01006507, 0.00186608,\n",
       "        0.00274117, 0.00151351, 0.00293259, 0.00234277, 0.00411317,\n",
       "        0.0010963 , 0.00429123, 0.00174534, 0.00344594, 0.00106048,\n",
       "        0.00461871, 0.00270613, 0.00242154, 0.0015974 , 0.00744516,\n",
       "        0.0026324 , 0.0007431 , 0.00122002, 0.00155478, 0.00211995,\n",
       "        0.00331637, 0.00099613, 0.00227503, 0.0014595 , 0.00233543,\n",
       "        0.00289911, 0.00145701, 0.00102752, 0.00302528, 0.00176385,\n",
       "        0.00178872, 0.00132048, 0.00282317, 0.00261222, 0.00996472,\n",
       "        0.00482622, 0.00103607, 0.00136863, 0.00456957, 0.00082086,\n",
       "        0.02629471, 0.01765573, 0.0332631 , 0.00157635, 0.00175975,\n",
       "        0.00260893, 0.00143852, 0.00150252, 0.00267884, 0.00196735,\n",
       "        0.00259371, 0.00400333, 0.00576926, 0.00165608, 0.00327187,\n",
       "        0.00290102, 0.01435531, 0.00311495, 0.0125936 , 0.00281042,\n",
       "        0.00198562, 0.00252765, 0.00440268, 0.00150524, 0.00162087,\n",
       "        0.00352597, 0.00483707, 0.00222125, 0.00747332, 0.00225464,\n",
       "        0.00199479, 0.00103518, 0.00909462, 0.00221728, 0.00523672,\n",
       "        0.00269242, 0.00506996, 0.00294345, 0.00307486, 0.00049605,\n",
       "        0.00693688, 0.00252409, 0.00368271, 0.00121783, 0.00219855,\n",
       "        0.00190587, 0.00168104, 0.00202884, 0.00350057, 0.00217299,\n",
       "        0.00118301, 0.00134497, 0.00311955, 0.00187067, 0.00318758,\n",
       "        0.00065413, 0.00172769, 0.00553899, 0.00757353, 0.00149816,\n",
       "        0.01110868, 0.00296873, 0.00661732, 0.0045998 , 0.00495966,\n",
       "        0.00226625, 0.0021533 , 0.00498884, 0.01929662, 0.00450276,\n",
       "        0.01327505, 0.00316283, 0.00225387, 0.00572389, 0.00741136,\n",
       "        0.00474413, 0.00361118, 0.01130851, 0.00848964, 0.00670213,\n",
       "        0.00585379, 0.00385476, 0.00695912, 0.00090631, 0.00197719,\n",
       "        0.00163671, 0.02915676, 0.00203017, 0.01011885, 0.00140631,\n",
       "        0.00194996, 0.00211879]),\n",
       " 'mean_score_time': array([0.00858006, 0.00499678, 0.00871439, 0.00529337, 0.0082046 ,\n",
       "        0.00526848, 0.00850301, 0.00522833, 0.00875258, 0.00513892,\n",
       "        0.00879107, 0.00495214, 0.00910778, 0.00522947, 0.00887914,\n",
       "        0.00499654, 0.00812821, 0.00585022, 0.00839663, 0.00558481,\n",
       "        0.0082243 , 0.00529575, 0.00817003, 0.00516438, 0.00895286,\n",
       "        0.00619516, 0.0085196 , 0.00532885, 0.00950513, 0.00540919,\n",
       "        0.00858641, 0.00585647, 0.00931907, 0.00549989, 0.00837297,\n",
       "        0.00558958, 0.00868196, 0.00554562, 0.00865297, 0.00541887,\n",
       "        0.00874653, 0.00608125, 0.00888243, 0.00543599, 0.00985422,\n",
       "        0.0060452 , 0.00842214, 0.00533204, 0.00870514, 0.00547409,\n",
       "        0.00921054, 0.00503993, 0.00801406, 0.00492187, 0.0093925 ,\n",
       "        0.00536227, 0.00816832, 0.00510011, 0.01050439, 0.00499964,\n",
       "        0.00843525, 0.00506291, 0.00917411, 0.00510473, 0.01040721,\n",
       "        0.00499854, 0.00923576, 0.00511637, 0.00814538, 0.00518303,\n",
       "        0.00801797, 0.00499249, 0.00883889, 0.00546904, 0.01060967,\n",
       "        0.00535684, 0.00855203, 0.0056798 , 0.00893741, 0.00626955,\n",
       "        0.00874257, 0.00615082, 0.00829053, 0.0063303 , 0.00872006,\n",
       "        0.00591178, 0.00868206, 0.00535536, 0.01004224, 0.00544319,\n",
       "        0.00868592, 0.00604401, 0.00885987, 0.00535297, 0.01235666,\n",
       "        0.00546861, 0.00833793, 0.00550442, 0.0081459 , 0.00498676,\n",
       "        0.01366396, 0.01363411, 0.00937338, 0.00506992, 0.00867023,\n",
       "        0.00533576, 0.00893016, 0.00489836, 0.00875845, 0.00508614,\n",
       "        0.00822062, 0.00504203, 0.01041179, 0.00560622, 0.0111403 ,\n",
       "        0.00629191, 0.00822215, 0.00530496, 0.0091424 , 0.00512047,\n",
       "        0.00932388, 0.00644011, 0.00933771, 0.00543232, 0.00993519,\n",
       "        0.00564122, 0.00875421, 0.00558457, 0.00926995, 0.00529575,\n",
       "        0.00834198, 0.00525537, 0.01121359, 0.00592856, 0.01014705,\n",
       "        0.00678487, 0.01195078, 0.00539885, 0.00917397, 0.00625153,\n",
       "        0.01062074, 0.00566163, 0.00882602, 0.00561881, 0.00947447,\n",
       "        0.00585957, 0.00960221, 0.00588565, 0.00954084, 0.0057672 ,\n",
       "        0.00935535, 0.00600748, 0.00981159, 0.00576577, 0.00983934,\n",
       "        0.0057765 , 0.01037817, 0.00631118, 0.00868087, 0.00565195,\n",
       "        0.01031551, 0.00569983, 0.01129198, 0.00673413, 0.0096621 ,\n",
       "        0.00636883, 0.00966225, 0.00641041, 0.00950289, 0.00890861,\n",
       "        0.00988503, 0.00716338, 0.01212921, 0.00593085, 0.00998483,\n",
       "        0.00708961, 0.01230054, 0.00653968, 0.01288676, 0.00671506,\n",
       "        0.012498  , 0.00696235, 0.01011138, 0.00556288, 0.00987649,\n",
       "        0.00545726, 0.01335182, 0.00757942, 0.01064334, 0.00602422,\n",
       "        0.01017494, 0.00628963]),\n",
       " 'std_score_time': array([5.36897330e-04, 1.66599219e-04, 4.46136740e-04, 6.58997992e-04,\n",
       "        4.56590704e-04, 6.46336851e-04, 4.29432021e-04, 1.40780742e-04,\n",
       "        8.97539780e-04, 1.70720915e-04, 1.00560468e-03, 8.96526648e-05,\n",
       "        1.08832079e-03, 2.61944016e-04, 7.68969391e-04, 1.92870050e-04,\n",
       "        2.54483546e-04, 1.66336984e-03, 2.66491699e-04, 5.95252588e-04,\n",
       "        1.40774686e-04, 3.61442692e-04, 3.97158943e-04, 5.34685417e-04,\n",
       "        7.63284203e-04, 5.89609187e-04, 1.99884847e-04, 2.04293611e-04,\n",
       "        1.41842946e-03, 3.88668163e-04, 1.66778321e-04, 5.40475145e-04,\n",
       "        7.06521541e-04, 1.67272096e-04, 3.08067487e-04, 5.06371418e-04,\n",
       "        2.74806405e-04, 1.71043223e-04, 3.81755168e-04, 1.44002027e-04,\n",
       "        7.19880042e-04, 1.22246838e-03, 3.47401694e-04, 2.30123579e-04,\n",
       "        1.66050328e-03, 1.30165276e-03, 2.55416567e-04, 1.29439587e-04,\n",
       "        7.02569189e-04, 6.86826897e-04, 1.34258113e-03, 1.58811544e-04,\n",
       "        1.82632902e-04, 1.80608984e-04, 1.21857682e-03, 6.31028034e-04,\n",
       "        2.24462177e-04, 2.22991897e-04, 1.60011483e-03, 2.05788797e-04,\n",
       "        4.25697371e-04, 1.41268479e-04, 1.24067317e-03, 2.95728139e-04,\n",
       "        3.59391357e-03, 1.56449581e-04, 1.13640515e-03, 1.03586066e-04,\n",
       "        1.73264042e-04, 2.72315284e-04, 2.12445826e-04, 1.07458827e-04,\n",
       "        7.44561243e-04, 1.72581946e-04, 2.88821922e-03, 1.58745629e-04,\n",
       "        2.89651450e-04, 7.05805776e-04, 6.49685202e-04, 6.46998771e-04,\n",
       "        2.38004468e-04, 9.21054569e-04, 1.87631029e-04, 1.39108523e-03,\n",
       "        2.48302419e-04, 3.79673226e-04, 1.70279971e-04, 1.47123931e-04,\n",
       "        1.66877630e-03, 2.57197521e-04, 2.34077977e-04, 6.22051328e-04,\n",
       "        5.19543593e-04, 1.73924940e-04, 4.70834936e-03, 3.47897136e-04,\n",
       "        1.58525257e-04, 6.86097595e-04, 2.02962285e-04, 1.35762247e-04,\n",
       "        4.67230909e-03, 5.73363672e-03, 1.98509142e-03, 1.14590864e-04,\n",
       "        9.86769707e-04, 6.35397070e-04, 1.86676917e-03, 1.20199989e-04,\n",
       "        9.25680975e-04, 2.10245793e-04, 2.54810793e-04, 1.68174078e-04,\n",
       "        1.40362942e-03, 9.08758470e-04, 2.77188404e-03, 1.49663396e-03,\n",
       "        6.87734747e-05, 5.25697096e-04, 1.31024743e-03, 2.46277109e-04,\n",
       "        9.48500072e-04, 1.14852382e-03, 1.93681609e-03, 2.31820817e-04,\n",
       "        2.20300102e-03, 7.03227358e-04, 4.74936546e-04, 3.66530416e-04,\n",
       "        1.20607786e-03, 1.69875404e-04, 2.13309914e-04, 1.53044791e-04,\n",
       "        2.07125316e-03, 5.86597688e-04, 8.16274123e-04, 8.23041398e-04,\n",
       "        2.26342019e-03, 2.64314480e-04, 2.52536335e-04, 1.66039286e-04,\n",
       "        1.28200860e-03, 2.63840623e-04, 2.67588676e-04, 2.64020648e-04,\n",
       "        4.47484665e-04, 1.34966134e-03, 1.57992889e-03, 9.93943436e-04,\n",
       "        8.99674704e-04, 4.55744023e-04, 2.77625527e-04, 2.50662523e-04,\n",
       "        1.39619788e-03, 3.39365732e-04, 9.07173707e-04, 6.60236736e-04,\n",
       "        1.31437178e-03, 4.04749846e-04, 2.91061862e-04, 3.48094979e-04,\n",
       "        1.81587249e-03, 4.79036015e-04, 2.84695988e-03, 1.32351933e-03,\n",
       "        5.97846981e-04, 5.11869185e-04, 1.19188763e-03, 1.02512004e-03,\n",
       "        5.23842416e-04, 5.00169187e-03, 8.56156588e-04, 6.98701478e-04,\n",
       "        5.30315694e-03, 2.96703445e-04, 4.47739265e-04, 9.56227949e-04,\n",
       "        2.35554441e-03, 1.31749615e-03, 6.41096997e-03, 1.55930146e-03,\n",
       "        1.52403898e-03, 2.00286519e-03, 7.84902546e-04, 1.95674507e-04,\n",
       "        1.65257594e-03, 1.51218182e-04, 3.54971760e-03, 2.41598723e-03,\n",
       "        1.91667091e-03, 5.46678290e-04, 1.22593046e-03, 5.20087419e-04]),\n",
       " 'param_model': masked_array(data=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec': masked_array(data=[CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__max_features': masked_array(data=[1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700, 1500, 1500, 1500, 1500, 1500, 1500,\n",
       "                    2000, 2000, 2000, 2000, 2000, 2000, 2500, 2500, 2500,\n",
       "                    2500, 2500, 2500, 2700, 2700, 2700, 2700, 2700, 2700,\n",
       "                    1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700, 1500, 1500, 1500, 1500, 1500, 1500,\n",
       "                    2000, 2000, 2000, 2000, 2000, 2000, 2500, 2500, 2500,\n",
       "                    2500, 2500, 2500, 2700, 2700, 2700, 2700, 2700, 2700,\n",
       "                    1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__min_df': masked_array(data=[2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__ngram_range': masked_array(data=[(1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)}],\n",
       " 'split0_test_score': array([0.90657439, 0.90657439, 0.9100346 , 0.89965398, 0.90311419,\n",
       "        0.90657439, 0.90657439, 0.9100346 , 0.9100346 , 0.89965398,\n",
       "        0.90311419, 0.90657439, 0.9100346 , 0.9100346 , 0.9100346 ,\n",
       "        0.89965398, 0.90311419, 0.90657439, 0.91349481, 0.9100346 ,\n",
       "        0.9100346 , 0.89965398, 0.90311419, 0.90657439, 0.90311419,\n",
       "        0.89965398, 0.90657439, 0.89965398, 0.9100346 , 0.90657439,\n",
       "        0.90657439, 0.90311419, 0.90657439, 0.89965398, 0.9100346 ,\n",
       "        0.90657439, 0.90657439, 0.90311419, 0.90657439, 0.89965398,\n",
       "        0.9100346 , 0.90657439, 0.90657439, 0.90311419, 0.90657439,\n",
       "        0.89965398, 0.9100346 , 0.90657439, 0.90311419, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.89965398, 0.91349481, 0.90311419,\n",
       "        0.91349481, 0.90311419, 0.91349481, 0.89965398, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.90311419, 0.91349481, 0.89965398,\n",
       "        0.91349481, 0.90311419, 0.91349481, 0.90311419, 0.91349481,\n",
       "        0.89965398, 0.91349481, 0.87889273, 0.87197232, 0.87197232,\n",
       "        0.86851211, 0.86851211, 0.86851211, 0.87543253, 0.8650519 ,\n",
       "        0.87543253, 0.86851211, 0.86851211, 0.86851211, 0.87543253,\n",
       "        0.8650519 , 0.87543253, 0.86851211, 0.86851211, 0.86851211,\n",
       "        0.86851211, 0.8650519 , 0.87543253, 0.86851211, 0.86851211,\n",
       "        0.86851211, 0.90657439, 0.90657439, 0.9100346 , 0.89965398,\n",
       "        0.90311419, 0.90657439, 0.90657439, 0.9100346 , 0.9100346 ,\n",
       "        0.89965398, 0.90311419, 0.90657439, 0.9100346 , 0.9100346 ,\n",
       "        0.9100346 , 0.89965398, 0.90311419, 0.90657439, 0.91349481,\n",
       "        0.9100346 , 0.9100346 , 0.89965398, 0.90311419, 0.90657439,\n",
       "        0.90311419, 0.89965398, 0.90657439, 0.89965398, 0.9100346 ,\n",
       "        0.90657439, 0.90657439, 0.90311419, 0.90657439, 0.89965398,\n",
       "        0.9100346 , 0.90657439, 0.90657439, 0.90311419, 0.90657439,\n",
       "        0.89965398, 0.9100346 , 0.90657439, 0.90657439, 0.90311419,\n",
       "        0.90657439, 0.89965398, 0.9100346 , 0.90657439, 0.9100346 ,\n",
       "        0.89965398, 0.91349481, 0.90311419, 0.91349481, 0.90657439,\n",
       "        0.9100346 , 0.89619377, 0.91349481, 0.90311419, 0.91349481,\n",
       "        0.90657439, 0.90657439, 0.89619377, 0.91349481, 0.90311419,\n",
       "        0.91349481, 0.90657439, 0.91349481, 0.89619377, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.90657439, 0.87543253, 0.85121107,\n",
       "        0.87889273, 0.8615917 , 0.87889273, 0.88235294, 0.88927336,\n",
       "        0.84775087, 0.87889273, 0.8615917 , 0.87889273, 0.88235294,\n",
       "        0.87889273, 0.84775087, 0.87889273, 0.8615917 , 0.87889273,\n",
       "        0.88235294, 0.88581315, 0.84775087, 0.87889273, 0.8615917 ,\n",
       "        0.87889273, 0.88235294]),\n",
       " 'split1_test_score': array([0.93425606, 0.92041522, 0.92387543, 0.92041522, 0.93079585,\n",
       "        0.92733564, 0.92387543, 0.91695502, 0.92387543, 0.92041522,\n",
       "        0.93079585, 0.92733564, 0.92041522, 0.91695502, 0.92387543,\n",
       "        0.92041522, 0.93079585, 0.92733564, 0.91695502, 0.91695502,\n",
       "        0.92387543, 0.92041522, 0.93079585, 0.92733564, 0.94809689,\n",
       "        0.93771626, 0.94117647, 0.93079585, 0.94809689, 0.93425606,\n",
       "        0.93771626, 0.93079585, 0.94463668, 0.93079585, 0.94809689,\n",
       "        0.93425606, 0.94117647, 0.93079585, 0.94463668, 0.93079585,\n",
       "        0.94809689, 0.93425606, 0.93771626, 0.93079585, 0.94463668,\n",
       "        0.93079585, 0.94809689, 0.93425606, 0.90657439, 0.90311419,\n",
       "        0.91349481, 0.9100346 , 0.92041522, 0.91349481, 0.9100346 ,\n",
       "        0.90311419, 0.91349481, 0.9100346 , 0.92041522, 0.91349481,\n",
       "        0.9100346 , 0.90311419, 0.91349481, 0.9100346 , 0.92041522,\n",
       "        0.91349481, 0.9100346 , 0.90311419, 0.91349481, 0.9100346 ,\n",
       "        0.92041522, 0.91349481, 0.83044983, 0.83737024, 0.83391003,\n",
       "        0.84429066, 0.85121107, 0.85467128, 0.83044983, 0.83044983,\n",
       "        0.83044983, 0.84429066, 0.85121107, 0.85467128, 0.82698962,\n",
       "        0.83044983, 0.83044983, 0.84429066, 0.85121107, 0.85467128,\n",
       "        0.82698962, 0.83044983, 0.83044983, 0.84429066, 0.85121107,\n",
       "        0.85467128, 0.93425606, 0.92041522, 0.92387543, 0.92041522,\n",
       "        0.93079585, 0.92733564, 0.92387543, 0.91695502, 0.92387543,\n",
       "        0.92041522, 0.93079585, 0.92733564, 0.92041522, 0.91695502,\n",
       "        0.92387543, 0.92041522, 0.93079585, 0.92733564, 0.91695502,\n",
       "        0.91695502, 0.92387543, 0.92041522, 0.93079585, 0.92733564,\n",
       "        0.94809689, 0.93771626, 0.94117647, 0.93079585, 0.94809689,\n",
       "        0.93425606, 0.93771626, 0.93079585, 0.94463668, 0.93079585,\n",
       "        0.94809689, 0.93425606, 0.94117647, 0.93079585, 0.94463668,\n",
       "        0.93079585, 0.94809689, 0.93425606, 0.93771626, 0.93079585,\n",
       "        0.94463668, 0.93079585, 0.94809689, 0.93425606, 0.94809689,\n",
       "        0.94809689, 0.94463668, 0.9550173 , 0.93425606, 0.94117647,\n",
       "        0.95155709, 0.94463668, 0.94809689, 0.9550173 , 0.93425606,\n",
       "        0.94117647, 0.94117647, 0.94463668, 0.94809689, 0.9550173 ,\n",
       "        0.93425606, 0.94117647, 0.94117647, 0.94463668, 0.94809689,\n",
       "        0.9550173 , 0.93425606, 0.94117647, 0.92733564, 0.91695502,\n",
       "        0.92041522, 0.91695502, 0.91695502, 0.90657439, 0.9100346 ,\n",
       "        0.91695502, 0.92733564, 0.91695502, 0.91695502, 0.90657439,\n",
       "        0.9100346 , 0.91695502, 0.92733564, 0.91695502, 0.91695502,\n",
       "        0.90657439, 0.9100346 , 0.91695502, 0.92733564, 0.91695502,\n",
       "        0.91695502, 0.90657439]),\n",
       " 'split2_test_score': array([0.89930556, 0.89930556, 0.89930556, 0.89583333, 0.90625   ,\n",
       "        0.89236111, 0.91319444, 0.89236111, 0.89930556, 0.89583333,\n",
       "        0.90625   , 0.89236111, 0.90625   , 0.89236111, 0.89930556,\n",
       "        0.89583333, 0.90625   , 0.89236111, 0.90625   , 0.89236111,\n",
       "        0.89930556, 0.89583333, 0.90625   , 0.89236111, 0.89236111,\n",
       "        0.88541667, 0.88888889, 0.87152778, 0.89583333, 0.875     ,\n",
       "        0.90972222, 0.88541667, 0.88888889, 0.87152778, 0.89583333,\n",
       "        0.875     , 0.89583333, 0.88541667, 0.88888889, 0.87152778,\n",
       "        0.89583333, 0.875     , 0.89930556, 0.88541667, 0.88888889,\n",
       "        0.87152778, 0.89583333, 0.875     , 0.89583333, 0.89583333,\n",
       "        0.89236111, 0.88888889, 0.88888889, 0.88541667, 0.89930556,\n",
       "        0.89583333, 0.89236111, 0.88888889, 0.88888889, 0.88541667,\n",
       "        0.89930556, 0.89583333, 0.89236111, 0.88888889, 0.88888889,\n",
       "        0.88541667, 0.89930556, 0.89583333, 0.89236111, 0.88888889,\n",
       "        0.88888889, 0.88541667, 0.84722222, 0.84722222, 0.84375   ,\n",
       "        0.85069444, 0.84027778, 0.84722222, 0.83333333, 0.83333333,\n",
       "        0.84375   , 0.85069444, 0.84027778, 0.84722222, 0.82638889,\n",
       "        0.83333333, 0.84375   , 0.85069444, 0.84027778, 0.84722222,\n",
       "        0.82291667, 0.83333333, 0.84375   , 0.85069444, 0.84027778,\n",
       "        0.84722222, 0.89930556, 0.89930556, 0.89930556, 0.89583333,\n",
       "        0.90625   , 0.89236111, 0.91319444, 0.89236111, 0.89930556,\n",
       "        0.89583333, 0.90625   , 0.89236111, 0.90625   , 0.89236111,\n",
       "        0.89930556, 0.89583333, 0.90625   , 0.89236111, 0.90625   ,\n",
       "        0.89236111, 0.89930556, 0.89583333, 0.90625   , 0.89236111,\n",
       "        0.89236111, 0.88541667, 0.88888889, 0.87152778, 0.89583333,\n",
       "        0.875     , 0.90972222, 0.88541667, 0.88888889, 0.87152778,\n",
       "        0.89583333, 0.875     , 0.89583333, 0.88541667, 0.88888889,\n",
       "        0.87152778, 0.89583333, 0.875     , 0.89930556, 0.88541667,\n",
       "        0.88888889, 0.87152778, 0.89583333, 0.875     , 0.88194444,\n",
       "        0.89583333, 0.88194444, 0.89236111, 0.88541667, 0.87847222,\n",
       "        0.88888889, 0.89583333, 0.88541667, 0.89236111, 0.88541667,\n",
       "        0.87847222, 0.88541667, 0.89583333, 0.88541667, 0.89236111,\n",
       "        0.88541667, 0.87847222, 0.88888889, 0.89583333, 0.88541667,\n",
       "        0.89236111, 0.88541667, 0.87847222, 0.86805556, 0.86805556,\n",
       "        0.875     , 0.87152778, 0.86458333, 0.86458333, 0.86805556,\n",
       "        0.875     , 0.87152778, 0.87152778, 0.86458333, 0.86458333,\n",
       "        0.86458333, 0.875     , 0.87152778, 0.87152778, 0.86458333,\n",
       "        0.86458333, 0.875     , 0.875     , 0.87152778, 0.87152778,\n",
       "        0.86458333, 0.86458333]),\n",
       " 'split3_test_score': array([0.90592334, 0.89547038, 0.8989547 , 0.8989547 , 0.89547038,\n",
       "        0.88501742, 0.90592334, 0.88850174, 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.88501742, 0.89547038, 0.88850174, 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.88501742, 0.8989547 , 0.88850174,\n",
       "        0.8989547 , 0.8989547 , 0.89547038, 0.88501742, 0.92334495,\n",
       "        0.91986063, 0.92682927, 0.91637631, 0.91289199, 0.91289199,\n",
       "        0.91986063, 0.91637631, 0.92682927, 0.91637631, 0.91289199,\n",
       "        0.91289199, 0.92334495, 0.91637631, 0.92682927, 0.91637631,\n",
       "        0.91289199, 0.91289199, 0.93031359, 0.91637631, 0.92682927,\n",
       "        0.91637631, 0.91289199, 0.91289199, 0.8989547 , 0.8989547 ,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.89547038, 0.8989547 ,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 , 0.89547038,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.87804878, 0.87804878, 0.87804878,\n",
       "        0.88850174, 0.88501742, 0.90592334, 0.87108014, 0.87456446,\n",
       "        0.87804878, 0.88850174, 0.88501742, 0.90592334, 0.86759582,\n",
       "        0.87456446, 0.87804878, 0.88850174, 0.88501742, 0.90592334,\n",
       "        0.8641115 , 0.87456446, 0.87804878, 0.88850174, 0.88501742,\n",
       "        0.90592334, 0.90592334, 0.89547038, 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.88501742, 0.90592334, 0.88850174, 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.88501742, 0.89547038, 0.88850174,\n",
       "        0.8989547 , 0.8989547 , 0.89547038, 0.88501742, 0.8989547 ,\n",
       "        0.88850174, 0.8989547 , 0.8989547 , 0.89547038, 0.88501742,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.91637631, 0.91289199,\n",
       "        0.91289199, 0.91986063, 0.91637631, 0.92682927, 0.91637631,\n",
       "        0.91289199, 0.91289199, 0.92334495, 0.91637631, 0.92682927,\n",
       "        0.91637631, 0.91289199, 0.91289199, 0.93031359, 0.91637631,\n",
       "        0.92682927, 0.91637631, 0.91289199, 0.91289199, 0.92682927,\n",
       "        0.93379791, 0.92682927, 0.93379791, 0.91986063, 0.91986063,\n",
       "        0.92334495, 0.94076655, 0.92682927, 0.93379791, 0.91986063,\n",
       "        0.91986063, 0.91986063, 0.94076655, 0.92682927, 0.93379791,\n",
       "        0.91986063, 0.91986063, 0.91637631, 0.94076655, 0.92682927,\n",
       "        0.93379791, 0.91986063, 0.91986063, 0.90592334, 0.89547038,\n",
       "        0.90592334, 0.8989547 , 0.90592334, 0.89198606, 0.89547038,\n",
       "        0.90243902, 0.90592334, 0.8989547 , 0.90592334, 0.89198606,\n",
       "        0.89547038, 0.90243902, 0.90592334, 0.8989547 , 0.90592334,\n",
       "        0.89198606, 0.88850174, 0.90243902, 0.90592334, 0.8989547 ,\n",
       "        0.90592334, 0.89198606]),\n",
       " 'split4_test_score': array([0.93031359, 0.92682927, 0.93031359, 0.91986063, 0.91637631,\n",
       "        0.91289199, 0.92682927, 0.92682927, 0.93031359, 0.91986063,\n",
       "        0.91637631, 0.91289199, 0.92682927, 0.92682927, 0.93031359,\n",
       "        0.91986063, 0.91637631, 0.91289199, 0.93031359, 0.92682927,\n",
       "        0.93031359, 0.91986063, 0.91637631, 0.91289199, 0.92334495,\n",
       "        0.92334495, 0.92334495, 0.93379791, 0.92682927, 0.92334495,\n",
       "        0.91986063, 0.92682927, 0.92334495, 0.93379791, 0.92682927,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.92334495, 0.93379791,\n",
       "        0.92682927, 0.92334495, 0.91986063, 0.92682927, 0.92334495,\n",
       "        0.93379791, 0.92682927, 0.92334495, 0.90940767, 0.90592334,\n",
       "        0.90940767, 0.91289199, 0.90940767, 0.91289199, 0.90940767,\n",
       "        0.90592334, 0.90940767, 0.91289199, 0.90940767, 0.91289199,\n",
       "        0.91637631, 0.90592334, 0.90940767, 0.91289199, 0.90940767,\n",
       "        0.91289199, 0.91289199, 0.90592334, 0.90940767, 0.91289199,\n",
       "        0.90940767, 0.91289199, 0.86062718, 0.87108014, 0.85714286,\n",
       "        0.8815331 , 0.87108014, 0.88501742, 0.85714286, 0.86759582,\n",
       "        0.85714286, 0.8815331 , 0.87108014, 0.88501742, 0.85365854,\n",
       "        0.86759582, 0.85714286, 0.8815331 , 0.87108014, 0.88501742,\n",
       "        0.86062718, 0.86759582, 0.85714286, 0.8815331 , 0.87108014,\n",
       "        0.88501742, 0.93031359, 0.92682927, 0.93031359, 0.91986063,\n",
       "        0.91637631, 0.91289199, 0.92682927, 0.92682927, 0.93031359,\n",
       "        0.91986063, 0.91637631, 0.91289199, 0.92682927, 0.92682927,\n",
       "        0.93031359, 0.91986063, 0.91637631, 0.91289199, 0.93031359,\n",
       "        0.92682927, 0.93031359, 0.91986063, 0.91637631, 0.91289199,\n",
       "        0.92334495, 0.92334495, 0.92334495, 0.93379791, 0.92682927,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.92334495, 0.93379791,\n",
       "        0.92682927, 0.92334495, 0.91986063, 0.92682927, 0.92334495,\n",
       "        0.93379791, 0.92682927, 0.92334495, 0.91986063, 0.92682927,\n",
       "        0.92334495, 0.93379791, 0.92682927, 0.92334495, 0.92682927,\n",
       "        0.93031359, 0.92682927, 0.93728223, 0.93031359, 0.93031359,\n",
       "        0.92334495, 0.93728223, 0.92682927, 0.93728223, 0.93031359,\n",
       "        0.93031359, 0.92334495, 0.93728223, 0.92682927, 0.93728223,\n",
       "        0.93031359, 0.93031359, 0.93379791, 0.93728223, 0.92682927,\n",
       "        0.93728223, 0.93031359, 0.93031359, 0.90592334, 0.91289199,\n",
       "        0.90940767, 0.91986063, 0.90243902, 0.90940767, 0.90940767,\n",
       "        0.90592334, 0.90940767, 0.91986063, 0.90243902, 0.90940767,\n",
       "        0.91637631, 0.90592334, 0.90940767, 0.91986063, 0.90243902,\n",
       "        0.90940767, 0.91986063, 0.90592334, 0.90940767, 0.91986063,\n",
       "        0.90243902, 0.90940767]),\n",
       " 'mean_test_score': array([0.91527778, 0.90972222, 0.9125    , 0.90694444, 0.91041667,\n",
       "        0.90486111, 0.91527778, 0.90694444, 0.9125    , 0.90694444,\n",
       "        0.91041667, 0.90486111, 0.91180556, 0.90694444, 0.9125    ,\n",
       "        0.90694444, 0.91041667, 0.90486111, 0.91319444, 0.90694444,\n",
       "        0.9125    , 0.90694444, 0.91041667, 0.90486111, 0.91805556,\n",
       "        0.91319444, 0.91736111, 0.91041667, 0.91875   , 0.91041667,\n",
       "        0.91875   , 0.9125    , 0.91805556, 0.91041667, 0.91875   ,\n",
       "        0.91041667, 0.91736111, 0.9125    , 0.91805556, 0.91041667,\n",
       "        0.91875   , 0.91041667, 0.91875   , 0.9125    , 0.91805556,\n",
       "        0.91041667, 0.91875   , 0.91041667, 0.90277778, 0.90347222,\n",
       "        0.90347222, 0.90486111, 0.90347222, 0.90416667, 0.90416667,\n",
       "        0.90347222, 0.90347222, 0.90486111, 0.90347222, 0.90416667,\n",
       "        0.90555556, 0.90347222, 0.90347222, 0.90486111, 0.90347222,\n",
       "        0.90416667, 0.90486111, 0.90347222, 0.90347222, 0.90486111,\n",
       "        0.90347222, 0.90416667, 0.85902778, 0.86111111, 0.85694444,\n",
       "        0.86666667, 0.86319444, 0.87222222, 0.85347222, 0.85416667,\n",
       "        0.85694444, 0.86666667, 0.86319444, 0.87222222, 0.85      ,\n",
       "        0.85416667, 0.85694444, 0.86666667, 0.86319444, 0.87222222,\n",
       "        0.84861111, 0.85416667, 0.85694444, 0.86666667, 0.86319444,\n",
       "        0.87222222, 0.91527778, 0.90972222, 0.9125    , 0.90694444,\n",
       "        0.91041667, 0.90486111, 0.91527778, 0.90694444, 0.9125    ,\n",
       "        0.90694444, 0.91041667, 0.90486111, 0.91180556, 0.90694444,\n",
       "        0.9125    , 0.90694444, 0.91041667, 0.90486111, 0.91319444,\n",
       "        0.90694444, 0.9125    , 0.90694444, 0.91041667, 0.90486111,\n",
       "        0.91805556, 0.91319444, 0.91736111, 0.91041667, 0.91875   ,\n",
       "        0.91041667, 0.91875   , 0.9125    , 0.91805556, 0.91041667,\n",
       "        0.91875   , 0.91041667, 0.91736111, 0.9125    , 0.91805556,\n",
       "        0.91041667, 0.91875   , 0.91041667, 0.91875   , 0.9125    ,\n",
       "        0.91805556, 0.91041667, 0.91875   , 0.91041667, 0.91875   ,\n",
       "        0.92152778, 0.91875   , 0.92430556, 0.91666667, 0.91527778,\n",
       "        0.91944444, 0.92291667, 0.92013889, 0.92430556, 0.91666667,\n",
       "        0.91527778, 0.91527778, 0.92291667, 0.92013889, 0.92430556,\n",
       "        0.91666667, 0.91527778, 0.91875   , 0.92291667, 0.92013889,\n",
       "        0.92430556, 0.91666667, 0.91527778, 0.89652778, 0.88888889,\n",
       "        0.89791667, 0.89375   , 0.89375   , 0.89097222, 0.89444444,\n",
       "        0.88958333, 0.89861111, 0.89375   , 0.89375   , 0.89097222,\n",
       "        0.89305556, 0.88958333, 0.89861111, 0.89375   , 0.89375   ,\n",
       "        0.89097222, 0.89583333, 0.88958333, 0.89861111, 0.89375   ,\n",
       "        0.89375   , 0.89097222]),\n",
       " 'std_test_score': array([0.01417774, 0.01206012, 0.01272087, 0.01085124, 0.01221176,\n",
       "        0.01498486, 0.00865683, 0.01453135, 0.01272087, 0.01085124,\n",
       "        0.01221176, 0.01498486, 0.01094746, 0.01453135, 0.01272087,\n",
       "        0.01085124, 0.01221176, 0.01498486, 0.01055102, 0.01453135,\n",
       "        0.01272087, 0.01085124, 0.01221176, 0.01498486, 0.0192125 ,\n",
       "        0.01847134, 0.01800879, 0.02291512, 0.01769108, 0.02006046,\n",
       "        0.01089667, 0.01659756, 0.01895558, 0.02291512, 0.01769108,\n",
       "        0.02006046, 0.01543536, 0.01659756, 0.01895558, 0.02291512,\n",
       "        0.01769108, 0.02006046, 0.01429892, 0.01659756, 0.01895558,\n",
       "        0.02291512, 0.01769108, 0.02006046, 0.00492283, 0.00609273,\n",
       "        0.00748105, 0.00954783, 0.01068466, 0.0116398 , 0.00476943,\n",
       "        0.00609273, 0.00748105, 0.00954783, 0.01068466, 0.0116398 ,\n",
       "        0.00671233, 0.00609273, 0.00748105, 0.00954783, 0.01068466,\n",
       "        0.0116398 , 0.00565367, 0.00609273, 0.00748105, 0.00954783,\n",
       "        0.01068466, 0.0116398 , 0.01853637, 0.01588576, 0.01659534,\n",
       "        0.01706641, 0.01571118, 0.02117965, 0.01867493, 0.01850249,\n",
       "        0.01825035, 0.01706641, 0.01571118, 0.02117965, 0.02029648,\n",
       "        0.01850249, 0.01825035, 0.01706641, 0.01571118, 0.02117965,\n",
       "        0.0195451 , 0.01850249, 0.01825035, 0.01706641, 0.01571118,\n",
       "        0.02117965, 0.01417774, 0.01206012, 0.01272087, 0.01085124,\n",
       "        0.01221176, 0.01498486, 0.00865683, 0.01453135, 0.01272087,\n",
       "        0.01085124, 0.01221176, 0.01498486, 0.01094746, 0.01453135,\n",
       "        0.01272087, 0.01085124, 0.01221176, 0.01498486, 0.01055102,\n",
       "        0.01453135, 0.01272087, 0.01085124, 0.01221176, 0.01498486,\n",
       "        0.0192125 , 0.01847134, 0.01800879, 0.02291512, 0.01769108,\n",
       "        0.02006046, 0.01089667, 0.01659756, 0.01895558, 0.02291512,\n",
       "        0.01769108, 0.02006046, 0.01543536, 0.01659756, 0.01895558,\n",
       "        0.02291512, 0.01769108, 0.02006046, 0.01429892, 0.01659756,\n",
       "        0.01895558, 0.02291512, 0.01769108, 0.02006046, 0.02202361,\n",
       "        0.02037476, 0.02090409, 0.0231308 , 0.01727933, 0.02167926,\n",
       "        0.0204334 , 0.02212086, 0.02060827, 0.0231308 , 0.01727933,\n",
       "        0.02167926, 0.01858253, 0.02212086, 0.02060827, 0.0231308 ,\n",
       "        0.01727933, 0.02167926, 0.01819538, 0.02212086, 0.02060827,\n",
       "        0.0231308 , 0.01727933, 0.02167926, 0.021843  , 0.02555907,\n",
       "        0.01784238, 0.02357248, 0.01915962, 0.01646151, 0.0154355 ,\n",
       "        0.0251049 , 0.0205965 , 0.02357248, 0.01915962, 0.01646151,\n",
       "        0.01923028, 0.0251049 , 0.0205965 , 0.02357248, 0.01915962,\n",
       "        0.01646151, 0.01652894, 0.0251049 , 0.0205965 , 0.02357248,\n",
       "        0.01915962, 0.01646151]),\n",
       " 'rank_test_score': array([ 44,  97,  57,  99,  73, 114,  44,  99,  57,  99,  73, 114,  71,\n",
       "         99,  57,  99,  73, 114,  53,  99,  57,  99,  73, 114,  28,  53,\n",
       "         36,  73,  13,  73,  13,  57,  28,  73,  13,  73,  36,  57,  28,\n",
       "         73,  13,  73,  13,  57,  28,  73,  13,  73, 144, 132, 132, 114,\n",
       "        132, 127, 127, 132, 132, 114, 132, 127, 113, 132, 132, 114, 132,\n",
       "        127, 114, 132, 132, 114, 132, 127, 182, 181, 183, 173, 177, 169,\n",
       "        190, 187, 183, 173, 177, 169, 191, 187, 183, 173, 177, 169, 192,\n",
       "        187, 183, 173, 177, 169,  44,  97,  57,  99,  73, 114,  44,  99,\n",
       "         57,  99,  73, 114,  71,  99,  57,  99,  73, 114,  53,  99,  57,\n",
       "         99,  73, 114,  28,  53,  36,  73,  13,  73,  13,  57,  28,  73,\n",
       "         13,  73,  36,  57,  28,  73,  13,  73,  13,  57,  28,  73,  13,\n",
       "         73,  13,   8,  13,   1,  40,  44,  12,   5,   9,   1,  40,  44,\n",
       "         44,   5,   9,   1,  40,  44,  13,   5,   9,   1,  40,  44, 149,\n",
       "        168, 148, 152, 152, 161, 151, 165, 145, 152, 152, 161, 160, 165,\n",
       "        145, 152, 152, 161, 150, 165, 145, 152, 152, 161], dtype=int32),\n",
       " 'split0_train_score': array([0.99218071, 0.99391833, 0.99218071, 0.99218071, 0.99044309,\n",
       "        0.98870547, 0.99304952, 0.99565595, 0.99218071, 0.99218071,\n",
       "        0.99044309, 0.98870547, 0.99913119, 0.99565595, 0.99218071,\n",
       "        0.99218071, 0.99044309, 0.98870547, 0.99913119, 0.99565595,\n",
       "        0.99218071, 0.99218071, 0.99044309, 0.98870547, 0.98436142,\n",
       "        0.98523023, 0.98523023, 0.98349262, 0.98262381, 0.97914857,\n",
       "        0.98696785, 0.98696785, 0.98523023, 0.98349262, 0.98262381,\n",
       "        0.97914857, 0.98957428, 0.98696785, 0.98523023, 0.98349262,\n",
       "        0.98262381, 0.97914857, 0.98957428, 0.98696785, 0.98523023,\n",
       "        0.98349262, 0.98262381, 0.97914857, 0.9643788 , 0.96090356,\n",
       "        0.96350999, 0.96090356, 0.95829713, 0.95916594, 0.9643788 ,\n",
       "        0.96090356, 0.96350999, 0.96090356, 0.95916594, 0.95916594,\n",
       "        0.9643788 , 0.96090356, 0.96350999, 0.96090356, 0.95829713,\n",
       "        0.95916594, 0.9643788 , 0.96090356, 0.96350999, 0.96090356,\n",
       "        0.95916594, 0.95916594, 0.90356212, 0.90616855, 0.90616855,\n",
       "        0.9105126 , 0.91311903, 0.91311903, 0.89661164, 0.90529974,\n",
       "        0.90529974, 0.9105126 , 0.91311903, 0.91311903, 0.88879235,\n",
       "        0.90529974, 0.90529974, 0.9105126 , 0.91311903, 0.91311903,\n",
       "        0.88531712, 0.90529974, 0.90529974, 0.9105126 , 0.91311903,\n",
       "        0.91311903, 0.99218071, 0.99391833, 0.99218071, 0.99218071,\n",
       "        0.99044309, 0.98870547, 0.99304952, 0.99565595, 0.99218071,\n",
       "        0.99218071, 0.99044309, 0.98870547, 0.99913119, 0.99565595,\n",
       "        0.99218071, 0.99218071, 0.99044309, 0.98870547, 0.99913119,\n",
       "        0.99565595, 0.99218071, 0.99218071, 0.99044309, 0.98870547,\n",
       "        0.98436142, 0.98523023, 0.98523023, 0.98349262, 0.98262381,\n",
       "        0.97914857, 0.98696785, 0.98696785, 0.98523023, 0.98349262,\n",
       "        0.98262381, 0.97914857, 0.98957428, 0.98696785, 0.98523023,\n",
       "        0.98349262, 0.98262381, 0.97914857, 0.98957428, 0.98696785,\n",
       "        0.98523023, 0.98349262, 0.98262381, 0.97914857, 0.97219809,\n",
       "        0.97567333, 0.9730669 , 0.97132928, 0.96090356, 0.96524761,\n",
       "        0.97741095, 0.98001738, 0.9730669 , 0.97132928, 0.96090356,\n",
       "        0.96524761, 0.98001738, 0.98001738, 0.9730669 , 0.97132928,\n",
       "        0.96090356, 0.96524761, 0.98262381, 0.98001738, 0.9730669 ,\n",
       "        0.97132928, 0.96090356, 0.96524761, 0.96785404, 0.9730669 ,\n",
       "        0.96872285, 0.96872285, 0.96177237, 0.95829713, 0.97393571,\n",
       "        0.97741095, 0.96785404, 0.96872285, 0.96177237, 0.95829713,\n",
       "        0.98001738, 0.97741095, 0.96785404, 0.96872285, 0.96177237,\n",
       "        0.95829713, 0.98088619, 0.97741095, 0.96785404, 0.96872285,\n",
       "        0.96177237, 0.95829713]),\n",
       " 'split1_train_score': array([0.99391833, 0.99478714, 0.99478714, 0.99391833, 0.99044309,\n",
       "        0.98957428, 0.99565595, 0.99478714, 0.99478714, 0.99391833,\n",
       "        0.99044309, 0.98957428, 0.99565595, 0.99478714, 0.99478714,\n",
       "        0.99391833, 0.99044309, 0.98957428, 0.99565595, 0.99478714,\n",
       "        0.99478714, 0.99391833, 0.99044309, 0.98957428, 0.98436142,\n",
       "        0.98609904, 0.98609904, 0.98001738, 0.9730669 , 0.96872285,\n",
       "        0.98957428, 0.98783666, 0.98523023, 0.98001738, 0.9730669 ,\n",
       "        0.96872285, 0.98957428, 0.98783666, 0.98523023, 0.98001738,\n",
       "        0.9730669 , 0.96872285, 0.9913119 , 0.98783666, 0.98523023,\n",
       "        0.98001738, 0.9730669 , 0.96872285, 0.96698523, 0.96959166,\n",
       "        0.96611642, 0.96872285, 0.9643788 , 0.96524761, 0.96785404,\n",
       "        0.96959166, 0.96698523, 0.96872285, 0.9643788 , 0.96524761,\n",
       "        0.96785404, 0.96959166, 0.96698523, 0.96872285, 0.9643788 ,\n",
       "        0.96524761, 0.96785404, 0.96959166, 0.96698523, 0.96872285,\n",
       "        0.9643788 , 0.96524761, 0.87054735, 0.87141616, 0.87141616,\n",
       "        0.87749783, 0.88705474, 0.89487402, 0.86707211, 0.86794092,\n",
       "        0.87054735, 0.87749783, 0.88705474, 0.89487402, 0.86272806,\n",
       "        0.86794092, 0.87054735, 0.87749783, 0.88705474, 0.89487402,\n",
       "        0.85664639, 0.86794092, 0.87054735, 0.87749783, 0.88705474,\n",
       "        0.89487402, 0.99391833, 0.99478714, 0.99478714, 0.99391833,\n",
       "        0.99044309, 0.98957428, 0.99565595, 0.99478714, 0.99478714,\n",
       "        0.99391833, 0.99044309, 0.98957428, 0.99565595, 0.99478714,\n",
       "        0.99478714, 0.99391833, 0.99044309, 0.98957428, 0.99565595,\n",
       "        0.99478714, 0.99478714, 0.99391833, 0.99044309, 0.98957428,\n",
       "        0.98436142, 0.98609904, 0.98609904, 0.98001738, 0.9730669 ,\n",
       "        0.96872285, 0.98957428, 0.98783666, 0.98523023, 0.98001738,\n",
       "        0.9730669 , 0.96872285, 0.98957428, 0.98783666, 0.98523023,\n",
       "        0.98001738, 0.9730669 , 0.96872285, 0.9913119 , 0.98783666,\n",
       "        0.98523023, 0.98001738, 0.9730669 , 0.96872285, 0.96264118,\n",
       "        0.96524761, 0.96524761, 0.96177237, 0.95829713, 0.95308427,\n",
       "        0.9730669 , 0.96872285, 0.96524761, 0.96177237, 0.95829713,\n",
       "        0.95308427, 0.97393571, 0.96872285, 0.96524761, 0.96177237,\n",
       "        0.95829713, 0.95308427, 0.97480452, 0.96872285, 0.96524761,\n",
       "        0.96177237, 0.95829713, 0.95308427, 0.96524761, 0.96959166,\n",
       "        0.96785404, 0.96177237, 0.95482189, 0.95134666, 0.97393571,\n",
       "        0.97132928, 0.96785404, 0.96177237, 0.95482189, 0.95134666,\n",
       "        0.97654214, 0.97132928, 0.96785404, 0.96177237, 0.95482189,\n",
       "        0.95134666, 0.97654214, 0.97132928, 0.96785404, 0.96177237,\n",
       "        0.95482189, 0.95134666]),\n",
       " 'split2_train_score': array([0.99826389, 0.99826389, 0.99826389, 0.99479167, 0.99479167,\n",
       "        0.9921875 , 0.99913194, 0.99913194, 0.99739583, 0.99479167,\n",
       "        0.99479167, 0.9921875 , 0.99913194, 0.99913194, 0.99739583,\n",
       "        0.99479167, 0.99479167, 0.9921875 , 0.99913194, 0.99913194,\n",
       "        0.99739583, 0.99479167, 0.99479167, 0.9921875 , 0.98784722,\n",
       "        0.98871528, 0.98784722, 0.98697917, 0.98350694, 0.98090278,\n",
       "        0.98784722, 0.98871528, 0.98784722, 0.98697917, 0.98350694,\n",
       "        0.98090278, 0.98871528, 0.98871528, 0.98784722, 0.98697917,\n",
       "        0.98350694, 0.98090278, 0.99131944, 0.98871528, 0.98784722,\n",
       "        0.98697917, 0.98350694, 0.98090278, 0.97482639, 0.97569444,\n",
       "        0.97309028, 0.97395833, 0.97048611, 0.97048611, 0.97569444,\n",
       "        0.97569444, 0.97309028, 0.97395833, 0.97048611, 0.97048611,\n",
       "        0.97569444, 0.97569444, 0.97309028, 0.97395833, 0.97048611,\n",
       "        0.97048611, 0.97569444, 0.97569444, 0.97309028, 0.97395833,\n",
       "        0.97048611, 0.97048611, 0.90190972, 0.90190972, 0.90017361,\n",
       "        0.90798611, 0.90625   , 0.91753472, 0.89496528, 0.8984375 ,\n",
       "        0.90017361, 0.90798611, 0.90711806, 0.91753472, 0.88628472,\n",
       "        0.8984375 , 0.90017361, 0.90798611, 0.90711806, 0.91753472,\n",
       "        0.87760417, 0.8984375 , 0.90017361, 0.90798611, 0.90711806,\n",
       "        0.91753472, 0.99826389, 0.99826389, 0.99826389, 0.99479167,\n",
       "        0.99479167, 0.9921875 , 0.99913194, 0.99913194, 0.99739583,\n",
       "        0.99479167, 0.99479167, 0.9921875 , 0.99913194, 0.99913194,\n",
       "        0.99739583, 0.99479167, 0.99479167, 0.9921875 , 0.99913194,\n",
       "        0.99913194, 0.99739583, 0.99479167, 0.99479167, 0.9921875 ,\n",
       "        0.98784722, 0.98871528, 0.98784722, 0.98697917, 0.98350694,\n",
       "        0.98090278, 0.98784722, 0.98871528, 0.98784722, 0.98697917,\n",
       "        0.98350694, 0.98090278, 0.98871528, 0.98871528, 0.98784722,\n",
       "        0.98697917, 0.98350694, 0.98090278, 0.99131944, 0.98871528,\n",
       "        0.98784722, 0.98697917, 0.98350694, 0.98090278, 0.97309028,\n",
       "        0.97743056, 0.97309028, 0.97222222, 0.96527778, 0.96440972,\n",
       "        0.98003472, 0.98003472, 0.97309028, 0.97222222, 0.96527778,\n",
       "        0.96440972, 0.98090278, 0.98003472, 0.97309028, 0.97222222,\n",
       "        0.96527778, 0.96440972, 0.98090278, 0.98003472, 0.97309028,\n",
       "        0.97222222, 0.96527778, 0.96440972, 0.97482639, 0.96961806,\n",
       "        0.97309028, 0.96961806, 0.96267361, 0.95572917, 0.97395833,\n",
       "        0.97482639, 0.97309028, 0.96961806, 0.96267361, 0.95572917,\n",
       "        0.97569444, 0.97482639, 0.97309028, 0.96961806, 0.96267361,\n",
       "        0.95572917, 0.97743056, 0.97482639, 0.97309028, 0.96961806,\n",
       "        0.96267361, 0.95572917]),\n",
       " 'split3_train_score': array([0.9991327 , 0.99826539, 0.9991327 , 0.99826539, 0.99739809,\n",
       "        0.99566349, 0.9991327 , 0.99826539, 0.9991327 , 0.99826539,\n",
       "        0.99739809, 0.99566349, 0.9991327 , 0.99826539, 0.9991327 ,\n",
       "        0.99826539, 0.99739809, 0.99566349, 0.9991327 , 0.99826539,\n",
       "        0.9991327 , 0.99826539, 0.99739809, 0.99566349, 0.98785776,\n",
       "        0.98785776, 0.98785776, 0.98525585, 0.98265395, 0.98005204,\n",
       "        0.99045967, 0.98785776, 0.98785776, 0.98525585, 0.98265395,\n",
       "        0.98005204, 0.98959237, 0.98785776, 0.98785776, 0.98525585,\n",
       "        0.98265395, 0.98005204, 0.99045967, 0.98785776, 0.98785776,\n",
       "        0.98525585, 0.98265395, 0.98005204, 0.97137901, 0.97398092,\n",
       "        0.97137901, 0.97398092, 0.97051171, 0.97311362, 0.97137901,\n",
       "        0.97398092, 0.97137901, 0.97398092, 0.97051171, 0.97311362,\n",
       "        0.97137901, 0.97398092, 0.97137901, 0.97398092, 0.97051171,\n",
       "        0.97311362, 0.97137901, 0.97398092, 0.97137901, 0.97398092,\n",
       "        0.97051171, 0.97311362, 0.88551605, 0.88985256, 0.88551605,\n",
       "        0.89418907, 0.89852559, 0.90633131, 0.87944493, 0.88291414,\n",
       "        0.88551605, 0.89418907, 0.89852559, 0.90633131, 0.87857762,\n",
       "        0.88291414, 0.88551605, 0.89418907, 0.89852559, 0.90633131,\n",
       "        0.87597572, 0.88291414, 0.88551605, 0.89418907, 0.89852559,\n",
       "        0.90633131, 0.9991327 , 0.99826539, 0.9991327 , 0.99826539,\n",
       "        0.99739809, 0.99566349, 0.9991327 , 0.99826539, 0.9991327 ,\n",
       "        0.99826539, 0.99739809, 0.99566349, 0.9991327 , 0.99826539,\n",
       "        0.9991327 , 0.99826539, 0.99739809, 0.99566349, 0.9991327 ,\n",
       "        0.99826539, 0.9991327 , 0.99826539, 0.99739809, 0.99566349,\n",
       "        0.98785776, 0.98785776, 0.98785776, 0.98525585, 0.98265395,\n",
       "        0.98005204, 0.99045967, 0.98785776, 0.98785776, 0.98525585,\n",
       "        0.98265395, 0.98005204, 0.98959237, 0.98785776, 0.98785776,\n",
       "        0.98525585, 0.98265395, 0.98005204, 0.99045967, 0.98785776,\n",
       "        0.98785776, 0.98525585, 0.98265395, 0.98005204, 0.9679098 ,\n",
       "        0.97137901, 0.9679098 , 0.9670425 , 0.96010408, 0.96097138,\n",
       "        0.97571552, 0.97311362, 0.9679098 , 0.9670425 , 0.96010408,\n",
       "        0.96097138, 0.97745013, 0.97311362, 0.9679098 , 0.9670425 ,\n",
       "        0.96010408, 0.96097138, 0.97745013, 0.97311362, 0.9679098 ,\n",
       "        0.9670425 , 0.96010408, 0.96097138, 0.96097138, 0.9661752 ,\n",
       "        0.96097138, 0.95836947, 0.95403296, 0.95229835, 0.96964441,\n",
       "        0.9687771 , 0.96097138, 0.95836947, 0.95403296, 0.95229835,\n",
       "        0.97745013, 0.9687771 , 0.96097138, 0.95836947, 0.95403296,\n",
       "        0.95229835, 0.98005204, 0.9687771 , 0.96097138, 0.95836947,\n",
       "        0.95403296, 0.95229835]),\n",
       " 'split4_train_score': array([0.99739809, 0.9991327 , 0.99653079, 0.99392888, 0.99045967,\n",
       "        0.98959237, 0.99739809, 0.9991327 , 0.99653079, 0.99392888,\n",
       "        0.99045967, 0.98959237, 0.9991327 , 0.9991327 , 0.99653079,\n",
       "        0.99392888, 0.99045967, 0.98959237, 0.9991327 , 0.9991327 ,\n",
       "        0.99653079, 0.99392888, 0.99045967, 0.98959237, 0.98352125,\n",
       "        0.98612316, 0.98438855, 0.98005204, 0.97484822, 0.97224631,\n",
       "        0.98612316, 0.98872507, 0.98438855, 0.98005204, 0.97484822,\n",
       "        0.97224631, 0.98785776, 0.98872507, 0.98438855, 0.98005204,\n",
       "        0.97484822, 0.97224631, 0.98785776, 0.98872507, 0.98438855,\n",
       "        0.98005204, 0.97484822, 0.97224631, 0.96097138, 0.96270598,\n",
       "        0.96010408, 0.96097138, 0.95923677, 0.95923677, 0.96097138,\n",
       "        0.96270598, 0.96010408, 0.96097138, 0.95923677, 0.95923677,\n",
       "        0.9679098 , 0.96270598, 0.96010408, 0.96097138, 0.95923677,\n",
       "        0.95923677, 0.96964441, 0.96270598, 0.96010408, 0.96097138,\n",
       "        0.95923677, 0.95923677, 0.9045967 , 0.90719861, 0.9037294 ,\n",
       "        0.91066782, 0.90893322, 0.91500434, 0.89679098, 0.9037294 ,\n",
       "        0.9037294 , 0.91066782, 0.90893322, 0.91500434, 0.89071986,\n",
       "        0.9037294 , 0.9037294 , 0.91066782, 0.90980052, 0.91500434,\n",
       "        0.88725065, 0.9037294 , 0.9037294 , 0.91066782, 0.90893322,\n",
       "        0.91500434, 0.99739809, 0.9991327 , 0.99653079, 0.99392888,\n",
       "        0.99045967, 0.98959237, 0.99739809, 0.9991327 , 0.99653079,\n",
       "        0.99392888, 0.99045967, 0.98959237, 0.9991327 , 0.9991327 ,\n",
       "        0.99653079, 0.99392888, 0.99045967, 0.98959237, 0.9991327 ,\n",
       "        0.9991327 , 0.99653079, 0.99392888, 0.99045967, 0.98959237,\n",
       "        0.98352125, 0.98612316, 0.98438855, 0.98005204, 0.97484822,\n",
       "        0.97224631, 0.98612316, 0.98872507, 0.98438855, 0.98005204,\n",
       "        0.97484822, 0.97224631, 0.98785776, 0.98872507, 0.98438855,\n",
       "        0.98005204, 0.97484822, 0.97224631, 0.98785776, 0.98872507,\n",
       "        0.98438855, 0.98005204, 0.97484822, 0.97224631, 0.97051171,\n",
       "        0.97745013, 0.97051171, 0.9661752 , 0.95403296, 0.95056375,\n",
       "        0.97571552, 0.98005204, 0.97051171, 0.9661752 , 0.95403296,\n",
       "        0.95056375, 0.98178664, 0.98005204, 0.97051171, 0.9661752 ,\n",
       "        0.95403296, 0.95056375, 0.98265395, 0.98005204, 0.97051171,\n",
       "        0.9661752 , 0.95403296, 0.95056375, 0.9670425 , 0.9670425 ,\n",
       "        0.9661752 , 0.95663487, 0.94622723, 0.94102342, 0.97051171,\n",
       "        0.9679098 , 0.9661752 , 0.95663487, 0.94622723, 0.94102342,\n",
       "        0.97658283, 0.9679098 , 0.9661752 , 0.95663487, 0.94622723,\n",
       "        0.94102342, 0.97831743, 0.9679098 , 0.9661752 , 0.95663487,\n",
       "        0.94622723, 0.94102342]),\n",
       " 'mean_train_score': array([0.99617874, 0.99687349, 0.99617905, 0.994617  , 0.99270712,\n",
       "        0.99114462, 0.99687364, 0.99739463, 0.99600543, 0.994617  ,\n",
       "        0.99270712, 0.99114462, 0.9984369 , 0.99739463, 0.99600543,\n",
       "        0.994617  , 0.99270712, 0.99114462, 0.9984369 , 0.99739463,\n",
       "        0.99600543, 0.994617  , 0.99270712, 0.99114462, 0.98558982,\n",
       "        0.9868051 , 0.98628456, 0.98315941, 0.97933996, 0.97621451,\n",
       "        0.98819444, 0.98802052, 0.9861108 , 0.98315941, 0.97933996,\n",
       "        0.97621451, 0.98906279, 0.98802052, 0.9861108 , 0.98315941,\n",
       "        0.97933996, 0.97621451, 0.99010461, 0.98802052, 0.9861108 ,\n",
       "        0.98315941, 0.97933996, 0.97621451, 0.96770816, 0.96857531,\n",
       "        0.96683996, 0.96770741, 0.96458211, 0.96545001, 0.96805554,\n",
       "        0.96857531, 0.96701372, 0.96770741, 0.96475587, 0.96545001,\n",
       "        0.96944322, 0.96857531, 0.96701372, 0.96770741, 0.96458211,\n",
       "        0.96545001, 0.96979014, 0.96857531, 0.96701372, 0.96770741,\n",
       "        0.96475587, 0.96545001, 0.89322639, 0.89530912, 0.89340075,\n",
       "        0.90017069, 0.90277651, 0.90937268, 0.88697699, 0.89166434,\n",
       "        0.89305323, 0.90017069, 0.90295012, 0.90937268, 0.88142052,\n",
       "        0.89166434, 0.89305323, 0.90017069, 0.90312358, 0.90937268,\n",
       "        0.87655881, 0.89166434, 0.89305323, 0.90017069, 0.90295012,\n",
       "        0.90937268, 0.99617874, 0.99687349, 0.99617905, 0.994617  ,\n",
       "        0.99270712, 0.99114462, 0.99687364, 0.99739463, 0.99600543,\n",
       "        0.994617  , 0.99270712, 0.99114462, 0.9984369 , 0.99739463,\n",
       "        0.99600543, 0.994617  , 0.99270712, 0.99114462, 0.9984369 ,\n",
       "        0.99739463, 0.99600543, 0.994617  , 0.99270712, 0.99114462,\n",
       "        0.98558982, 0.9868051 , 0.98628456, 0.98315941, 0.97933996,\n",
       "        0.97621451, 0.98819444, 0.98802052, 0.9861108 , 0.98315941,\n",
       "        0.97933996, 0.97621451, 0.98906279, 0.98802052, 0.9861108 ,\n",
       "        0.98315941, 0.97933996, 0.97621451, 0.99010461, 0.98802052,\n",
       "        0.9861108 , 0.98315941, 0.97933996, 0.97621451, 0.96927021,\n",
       "        0.97343613, 0.96996526, 0.96770831, 0.9597231 , 0.95885535,\n",
       "        0.97638872, 0.97638812, 0.96996526, 0.96770831, 0.9597231 ,\n",
       "        0.95885535, 0.97881853, 0.97638812, 0.96996526, 0.96770831,\n",
       "        0.9597231 , 0.95885535, 0.97968704, 0.97638812, 0.96996526,\n",
       "        0.96770831, 0.9597231 , 0.95885535, 0.96718838, 0.96909886,\n",
       "        0.96736275, 0.96302352, 0.95590561, 0.95173894, 0.97239717,\n",
       "        0.9720507 , 0.96718899, 0.96302352, 0.95590561, 0.95173894,\n",
       "        0.97725738, 0.9720507 , 0.96718899, 0.96302352, 0.95590561,\n",
       "        0.95173894, 0.97864567, 0.9720507 , 0.96718899, 0.96302352,\n",
       "        0.95590561, 0.95173894]),\n",
       " 'std_train_score': array([0.00267036, 0.0021005 , 0.00249473, 0.00201189, 0.00288629,\n",
       "        0.00254294, 0.00230586, 0.00182315, 0.00237073, 0.00201189,\n",
       "        0.00288629, 0.00254294, 0.00139047, 0.00182315, 0.00237073,\n",
       "        0.00201189, 0.00288629, 0.00254294, 0.00139047, 0.00182315,\n",
       "        0.00237073, 0.00201189, 0.00288629, 0.00254294, 0.00187277,\n",
       "        0.00128067, 0.0013898 , 0.00277938, 0.004442  , 0.00484121,\n",
       "        0.00160938, 0.00065536, 0.00145492, 0.00277938, 0.004442  ,\n",
       "        0.00484121, 0.00068943, 0.00065536, 0.00145492, 0.00277938,\n",
       "        0.004442  , 0.00484121, 0.0012953 , 0.00065536, 0.00145492,\n",
       "        0.00277938, 0.004442  , 0.00484121, 0.00492211, 0.00590323,\n",
       "        0.00483019, 0.00585027, 0.00525609, 0.0056961 , 0.00516028,\n",
       "        0.00590323, 0.00481665, 0.00585027, 0.00505599, 0.0056961 ,\n",
       "        0.00383015, 0.00590323, 0.00481665, 0.00585027, 0.00525609,\n",
       "        0.0056961 , 0.00375334, 0.00590323, 0.00481665, 0.00585027,\n",
       "        0.00505599, 0.0056961 , 0.01330639, 0.01344249, 0.01312296,\n",
       "        0.01286906, 0.00918893, 0.00814654, 0.01188183, 0.01425996,\n",
       "        0.01325127, 0.01286906, 0.00926084, 0.00814654, 0.01021736,\n",
       "        0.01425996, 0.01325127, 0.01286906, 0.00937865, 0.00814654,\n",
       "        0.01085321, 0.01425996, 0.01325127, 0.01286906, 0.00926084,\n",
       "        0.00814654, 0.00267036, 0.0021005 , 0.00249473, 0.00201189,\n",
       "        0.00288629, 0.00254294, 0.00230586, 0.00182315, 0.00237073,\n",
       "        0.00201189, 0.00288629, 0.00254294, 0.00139047, 0.00182315,\n",
       "        0.00237073, 0.00201189, 0.00288629, 0.00254294, 0.00139047,\n",
       "        0.00182315, 0.00237073, 0.00201189, 0.00288629, 0.00254294,\n",
       "        0.00187277, 0.00128067, 0.0013898 , 0.00277938, 0.004442  ,\n",
       "        0.00484121, 0.00160938, 0.00065536, 0.00145492, 0.00277938,\n",
       "        0.004442  , 0.00484121, 0.00068943, 0.00065536, 0.00145492,\n",
       "        0.00277938, 0.004442  , 0.00484121, 0.0012953 , 0.00065536,\n",
       "        0.00145492, 0.00277938, 0.004442  , 0.00484121, 0.00375504,\n",
       "        0.00465524, 0.00303861, 0.00378196, 0.00365525, 0.00597069,\n",
       "        0.00229256, 0.00467701, 0.00303861, 0.00378196, 0.00365525,\n",
       "        0.00597069, 0.00283921, 0.00467701, 0.00303861, 0.00378196,\n",
       "        0.00365525, 0.00597069, 0.00309024, 0.00467701, 0.00303861,\n",
       "        0.00378196, 0.00365525, 0.00597069, 0.00449936, 0.00240974,\n",
       "        0.00392825, 0.0052917 , 0.00597624, 0.0059029 , 0.00191333,\n",
       "        0.00360057, 0.0038832 , 0.0052917 , 0.00597624, 0.0059029 ,\n",
       "        0.00148755, 0.00360057, 0.0038832 , 0.0052917 , 0.00597624,\n",
       "        0.0059029 , 0.00161288, 0.00360057, 0.0038832 , 0.0052917 ,\n",
       "        0.00597624, 0.0059029 ])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_['param_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('grad', GradientBoostingClassifier()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vote', vote)\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'vote__tree__max_depth' : [None, 1, 2],\n",
    "    'vote__ada__n_estimators' : [40, 50, 60],\n",
    "    'vote__grad__n_estimators' : [90, 100],\n",
    "    'vote__logreg__penalty' : ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_) # cross val accuracy score\n",
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
