{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Webscrapping: Data Gathering Notebook\n",
    "\n",
    "_Authors: Patrick Wales-Dinan_\n",
    "\n",
    "---\n",
    "\n",
    "This lab was incredibly challenging. We had to extensively clean a date set that was missing a lot of values and had TONS of categorical data. Then we had to decide what features to use to model that data. After that we had to build and fit the models making decisions like whether to use polynomial features, dummy variables etc, log scaling features or log scaling the depended variable.\n",
    "\n",
    "After that we had to re run our model over and over again, looking at the different values of $\\beta$ and seeing if they were contributing to the predictive power of the model. We had to decide if we should throw those values out or if we should leave them. We also had to make judgement calls to see if our model appeared to be over fitting or suffering from bias. \n",
    "\n",
    "## Contents:\n",
    "- [Creating our URLs](#Instantiate-our-URL)\n",
    "- [Data Import](#Data-Import)\n",
    "- [Feature Creation](#Feature-Creation)\n",
    "- [Choosing the Features](#Feature-Choice)\n",
    "- [Log Scaling](#Log-Scaling-Independent-Variables)\n",
    "- [Cleaning the Data and Modifying the Data](#Cleaning-&-Creating-the-Data-Set)\n",
    "- [Modeling the Data](#Modeling-the-Data)\n",
    "- [Model Analysis](#Analyzing-the-model)\n",
    "\n",
    "Please visit the Graphs & Relationships notebook for additional visuals: Notebook - [Here](/Users/pwalesdi/Desktop/GA/GA_Project_2/Project_2_Graphs_&_Relationships.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_url = 'https://www.reddit.com/r/TexasPolitics.json'\n",
    "ca_url = 'https://www.reddit.com/r/California_Politics.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(url):\n",
    "    # Setting up my unique user agent so that I can pull posts from reddit\n",
    "    user_agent = {'User-agent' : 'pat bot 0.1'}\n",
    "    \n",
    "    # Empty posts list\n",
    "    posts = []\n",
    "    \n",
    "    # Setting after to NONE to start as this needs to be there in order to begin each pull\n",
    "    after = None\n",
    "    \n",
    "    for i in range(0,60):\n",
    "        print(i)\n",
    "        url = url\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after' : after}\n",
    "        res = requests.get(url, params=params, headers=user_agent)\n",
    "        if res.status_code == 200:\n",
    "            json = res.json()\n",
    "            posts.extend(json['data']['children'])\n",
    "            after = json['data']['after']\n",
    "        else: \n",
    "            print(tx_res.status_code)\n",
    "            break\n",
    "        time.sleep(2)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "tx_posts = get_posts(tx_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "ca_posts = get_posts(ca_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ca_posts)\n",
    "len(set([p['data']['name'] for p in ca_posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tx_posts)\n",
    "len(set([p['data']['name'] for p in tx_posts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>theProgressiveGOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_3x0d2uzk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>calmatters.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1s4p</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1s4p</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/cb1s4p/state_m...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/GcoPg0hQ78iGU...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://calmatters.org/articles/redistricting-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CALmatters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_kwolsnv</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562668e+09</td>\n",
       "      <td>1.562639e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>calmatters.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>caupt1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_caupt1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/caupt1/new_cal...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>45</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/2FFGh5hCRRYIk...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>https://calmatters.org/articles/ca-passes-dead...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>BlankVerse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_97a3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562634e+09</td>\n",
       "      <td>1.562606e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>thetrace.org</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>canr0y</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_canr0y</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/canr0y/the_nra...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>53</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/5y-0hwerp_6jF...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>https://www.thetrace.org/rounds/california-rea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>travadera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_10ukzyn2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562636e+09</td>\n",
       "      <td>1.562607e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>latimes.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cao5lo</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cao5lo</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/cao5lo/ca15_er...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/EXvjRI9EHJh4w...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>https://www.latimes.com/politics/la-na-pol-202...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>BlankVerse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_97a3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562642e+09</td>\n",
       "      <td>1.562613e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cnn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>capegr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_capegr</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/California_Politics/comments/capegr/eric_sw...</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>{'images': [{'source': {'url': 'https://extern...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>California_Politics</td>\n",
       "      <td>t5_357go</td>\n",
       "      <td>r/California_Politics</td>\n",
       "      <td>7746</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/pKle1lzNaOWeA...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.cnn.com/2019/07/08/politics/eric-s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments approved_at_utc approved_by  archived             author author_cakeday author_flair_background_color author_flair_css_class author_flair_richtext author_flair_template_id author_flair_text author_flair_text_color author_flair_type author_fullname author_patreon_flair banned_at_utc banned_by  can_gild  can_mod_post category  clicked content_categories  contest_mode       created   created_utc crosspost_parent crosspost_parent_list discussion_type distinguished          domain  downs edited  gilded gildings  hidden  hide_score      id  is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  is_robot_indexable  is_self  is_video likes link_flair_background_color link_flair_css_class link_flair_richtext link_flair_text link_flair_text_color link_flair_type  locked media media_embed  media_only mod_note mod_reason_by mod_reason_title mod_reports       name  no_follow  num_comments  num_crossposts num_reports  over_18  \\\n",
       "0            []                False            None        None     False  theProgressiveGOP            NaN                          None                   None                    []                     None              None                    None              text     t2_3x0d2uzk                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None  calmatters.org      0  False       0       {}   False        True  cb1s4p             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_cb1s4p      False             0               0        None    False   \n",
       "1            []                False            None        None     False         CALmatters            NaN                          None                   None                    []                     None              None                    None              text      t2_kwolsnv                False          None      None     False         False     None    False               None         False  1.562668e+09  1.562639e+09              NaN                   NaN            None          None  calmatters.org      0  False       0       {}   False       False  caupt1             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_caupt1      False             3               1        None    False   \n",
       "2            []                False            None        None     False         BlankVerse            NaN                          None                   None                    []                     None              None                    None              text         t2_97a3                False          None      None     False         False     None    False               None         False  1.562634e+09  1.562606e+09              NaN                   NaN            None          None    thetrace.org      0  False       0       {}   False       False  canr0y             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_canr0y      False            21               0        None    False   \n",
       "3            []                False            None        None     False          travadera            NaN                          None                   None                    []                     None              None                    None              text     t2_10ukzyn2                False          None      None     False         False     None    False               None         False  1.562636e+09  1.562607e+09              NaN                   NaN            None          None     latimes.com      0  False       0       {}   False       False  cao5lo             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_cao5lo      False             5               1        None    False   \n",
       "4            []                False            None        None     False         BlankVerse            NaN                          None                   None                    []                     None              None                    None              text         t2_97a3                False          None      None     False         False     None    False               None         False  1.562642e+09  1.562613e+09              NaN                   NaN            None          None         cnn.com      0  False       0       {}   False       False  capegr             False    False                False                   False                True    False     False  None                                             None                  []            None                  dark            text   False  None          {}       False     None          None             None          []  t3_capegr      False             4               0        None    False   \n",
       "\n",
       "  parent_whitelist_status                                          permalink  pinned post_hint                                            preview  pwls  quarantine removal_reason report_reasons  saved  score secure_media secure_media_embed selftext selftext_html  send_replies  spoiler  stickied            subreddit subreddit_id subreddit_name_prefixed  subreddit_subscribers subreddit_type suggested_sort                                          thumbnail  thumbnail_height  thumbnail_width                                              title  total_awards_received  ups                                                url user_reports view_count  visited whitelist_status   wls  \n",
       "0                    None  /r/California_Politics/comments/cb1s4p/state_m...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     13         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/GcoPg0hQ78iGU...              93.0            140.0  State May Push Cities and Counties to Draw \"fa...                      0   13  https://calmatters.org/articles/redistricting-...           []       None    False             None  None  \n",
       "1                    None  /r/California_Politics/comments/caupt1/new_cal...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     45         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/2FFGh5hCRRYIk...              93.0            140.0  New California rules for deadly police force g...                      0   45  https://calmatters.org/articles/ca-passes-dead...           []       None    False             None  None  \n",
       "2                    None  /r/California_Politics/comments/canr0y/the_nra...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     53         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/5y-0hwerp_6jF...              93.0            140.0  The NRA Opposes A California Gun Regulation It...                      0   53  https://www.thetrace.org/rounds/california-rea...           []       None    False             None  None  \n",
       "3                    None  /r/California_Politics/comments/cao5lo/ca15_er...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     20         None                 {}                   None         False    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/EXvjRI9EHJh4w...              78.0            140.0  [CA-15] Eric Swalwell is expected to withdraw ...                      0   20  https://www.latimes.com/politics/la-na-pol-202...           []       None    False             None  None  \n",
       "4                    None  /r/California_Politics/comments/capegr/eric_sw...   False      link  {'images': [{'source': {'url': 'https://extern...  None       False           None           None  False     12         None                 {}                   None          True    False     False  California_Politics     t5_357go   r/California_Politics                   7746         public           None  https://b.thumbs.redditmedia.com/pKle1lzNaOWeA...              78.0            140.0  Eric Swalwell expected to end presidential bid...                      0   12  https://www.cnn.com/2019/07/08/politics/eric-s...           []       None    False             None  None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_post_new = []\n",
    "ca_post_names = set()\n",
    "for post_dict in ca_posts:\n",
    "    keep_data = post_dict['data']\n",
    "    if keep_data['name'] not in ca_post_names:\n",
    "        ca_post_new.append(keep_data)\n",
    "        ca_post_names.add(keep_data['name'])\n",
    "df_ca = pd.DataFrame(ca_post_new)\n",
    "df_ca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>category</th>\n",
       "      <th>clicked</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>created</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>domain</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>gilded</th>\n",
       "      <th>gildings</th>\n",
       "      <th>hidden</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>id</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>likes</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>locked</th>\n",
       "      <th>media</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>media_metadata</th>\n",
       "      <th>media_only</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>name</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>over_18</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>permalink</th>\n",
       "      <th>pinned</th>\n",
       "      <th>pwls</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>saved</th>\n",
       "      <th>score</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>selftext</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>title</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>arcanition</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>17553cd2-9c63-11e7-b44c-0e30f0006cb4</td>\n",
       "      <td>3rd District (Northern Dallas Suburbs)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_5d5mc</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.559800e+09</td>\n",
       "      <td>1.559771e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>moderator</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.55984e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>bx8cik</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_bx8cik</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/bx8cik/welcome_new_r...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Hey all,\\n\\nAfter much time reading applicatio...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Welcome New /r/TexasPolitics Moderators - Q&amp;amp;A</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Texas_Monthly</td>\n",
       "      <td></td>\n",
       "      <td>verified</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>Verified - Texas Monthly</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_3x7xx9qc</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.561003e+09</td>\n",
       "      <td>1.560974e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.56107e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>c2lven</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>ama</td>\n",
       "      <td>[]</td>\n",
       "      <td>b8855642-9c62-11e7-ae9f-0e71ceb054c0</td>\n",
       "      <td>AMA</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_c2lven</td>\n",
       "      <td>False</td>\n",
       "      <td>243</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/c2lven/im_chris_hook...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>85</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Hey, r/TexasPolitics! I’m Chris Hooks, a write...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>qa</td>\n",
       "      <td></td>\n",
       "      <td>I’m Chris Hooks, a Texas Monthly writer who wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>beanzamillion21</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>2584f856-9c63-11e7-93b7-0e2bf15991f0</td>\n",
       "      <td>12th Congressional District (Western Fort Worth)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_50w01</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dallasnews.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1r8d</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1r8d</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cb1r8d/ross_perot_se...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Ross Perot, self-made billionaire, patriot and...</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>https://www.dallasnews.com/business/business/2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>irony_glazed</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_442pim8f</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562678e+09</td>\n",
       "      <td>1.562649e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>self.TexasPolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>1.56268e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cawekm</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>discussion</td>\n",
       "      <td>[]</td>\n",
       "      <td>ac3a0f90-9c62-11e7-9e00-0e65ddf91c6e</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cawekm</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cawekm/lets_talk_abo...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>37</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>Disclaimer: I am not a lawyer, this is not leg...</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Let's Talk About Why Texas's Hemp Law is Stupid</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>https://www.reddit.com/r/TexasPolitics/comment...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>beanzamillion21</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>[]</td>\n",
       "      <td>2584f856-9c63-11e7-93b7-0e2bf15991f0</td>\n",
       "      <td>12th Congressional District (Western Fort Worth)</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_50w01</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1.562712e+09</td>\n",
       "      <td>1.562683e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>housingwire.com</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>cb1sin</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>dark</td>\n",
       "      <td>text</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>t3_cb1sin</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/TexasPolitics/comments/cb1sin/this_texas_to...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>TexasPolitics</td>\n",
       "      <td>t5_2t47s</td>\n",
       "      <td>r/TexasPolitics</td>\n",
       "      <td>5415</td>\n",
       "      <td>public</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>This Texas town is the most affordable housing...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.housingwire.com/articles/49504-thi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments approved_at_utc approved_by  archived           author author_flair_background_color author_flair_css_class author_flair_richtext              author_flair_template_id                                 author_flair_text author_flair_text_color author_flair_type author_fullname author_patreon_flair banned_at_utc banned_by  can_gild  can_mod_post category  clicked content_categories  contest_mode       created   created_utc crosspost_parent crosspost_parent_list discussion_type distinguished              domain  downs       edited  gilded gildings  hidden  hide_score      id  is_crosspostable  is_meta  is_original_content  is_reddit_media_domain  is_robot_indexable  is_self  is_video likes link_flair_background_color link_flair_css_class link_flair_richtext                link_flair_template_id link_flair_text link_flair_text_color link_flair_type  locked media media_embed media_metadata  media_only mod_note mod_reason_by mod_reason_title mod_reports  \\\n",
       "0            []                False            None        None     False       arcanition                          None                      3                    []  17553cd2-9c63-11e7-b44c-0e30f0006cb4            3rd District (Northern Dallas Suburbs)                    dark              text        t2_5d5mc                False          None      None     False         False     None    False               None         False  1.559800e+09  1.559771e+09              NaN                   NaN            None     moderator  self.TexasPolitics      0  1.55984e+09       0       {}   False       False  bx8cik             False    False                False                   False                True     True     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "1            []                 True            None        None     False    Texas_Monthly                                             verified                    []                                  None                          Verified - Texas Monthly                    dark              text     t2_3x7xx9qc                False          None      None     False         False     None    False               None         False  1.561003e+09  1.560974e+09              NaN                   NaN            None          None  self.TexasPolitics      0  1.56107e+09       0       {}   False       False  c2lven             False    False                False                   False                True     True     False  None                                              ama                  []  b8855642-9c62-11e7-ae9f-0e71ceb054c0             AMA                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "2            []                False            None        None     False  beanzamillion21                          None                     12                    []  2584f856-9c63-11e7-93b7-0e2bf15991f0  12th Congressional District (Western Fort Worth)                    dark              text        t2_50w01                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None      dallasnews.com      0        False       0       {}   False        True  cb1r8d             False    False                False                   False                True    False     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "3            []                False            None        None     False     irony_glazed                          None                   None                    []                                  None                                              None                    None              text     t2_442pim8f                False          None      None     False         False     None    False               None         False  1.562678e+09  1.562649e+09              NaN                   NaN            None          None  self.TexasPolitics      0  1.56268e+09       0       {}   False       False  cawekm             False    False                False                   False                True     True     False  None                                       discussion                  []  ac3a0f90-9c62-11e7-9e00-0e65ddf91c6e      Discussion                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "4            []                False            None        None     False  beanzamillion21                          None                     12                    []  2584f856-9c63-11e7-93b7-0e2bf15991f0  12th Congressional District (Western Fort Worth)                    dark              text        t2_50w01                False          None      None     False         False     None    False               None         False  1.562712e+09  1.562683e+09              NaN                   NaN            None          None     housingwire.com      0        False       0       {}   False        True  cb1sin             False    False                False                   False                True    False     False  None                                             None                  []                                   NaN            None                  dark            text   False  None          {}            NaN       False     None          None             None          []   \n",
       "\n",
       "        name  no_follow  num_comments  num_crossposts num_reports  over_18 parent_whitelist_status                                          permalink  pinned  pwls  quarantine removal_reason report_reasons  saved  score secure_media secure_media_embed                                           selftext                                      selftext_html  send_replies  spoiler  stickied      subreddit subreddit_id subreddit_name_prefixed  subreddit_subscribers subreddit_type suggested_sort thumbnail                                              title  total_awards_received  ups                                                url user_reports view_count  visited whitelist_status   wls  \n",
       "0  t3_bx8cik      False            22               0        None    False                    None  /r/TexasPolitics/comments/bx8cik/welcome_new_r...   False  None       False           None           None  False     13         None                 {}  Hey all,\\n\\nAfter much time reading applicatio...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False      True  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            Welcome New /r/TexasPolitics Moderators - Q&amp;A                      0   13  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "1  t3_c2lven      False           243               0        None    False                    None  /r/TexasPolitics/comments/c2lven/im_chris_hook...   False  None       False           None           None  False     85         None                 {}  Hey, r/TexasPolitics! I’m Chris Hooks, a write...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False      True  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public             qa            I’m Chris Hooks, a Texas Monthly writer who wo...                      0   85  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "2  t3_cb1r8d      False             8               0        None    False                    None  /r/TexasPolitics/comments/cb1r8d/ross_perot_se...   False  None       False           None           None  False     25         None                 {}                                                                                                  None          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            Ross Perot, self-made billionaire, patriot and...                      0   25  https://www.dallasnews.com/business/business/2...           []       None    False             None  None  \n",
       "3  t3_cawekm      False            14               1        None    False                    None  /r/TexasPolitics/comments/cawekm/lets_talk_abo...   False  None       False           None           None  False     37         None                 {}  Disclaimer: I am not a lawyer, this is not leg...  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None              Let's Talk About Why Texas's Hemp Law is Stupid                      0   37  https://www.reddit.com/r/TexasPolitics/comment...           []       None    False             None  None  \n",
       "4  t3_cb1sin      False             3               0        None    False                    None  /r/TexasPolitics/comments/cb1sin/this_texas_to...   False  None       False           None           None  False      2         None                 {}                                                                                                  None          True    False     False  TexasPolitics     t5_2t47s         r/TexasPolitics                   5415         public           None            This Texas town is the most affordable housing...                      0    2  https://www.housingwire.com/articles/49504-thi...           []       None    False             None  None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_post_new = []\n",
    "tx_post_names = set()\n",
    "for post_dict in tx_posts:\n",
    "    keep_data = post_dict['data']\n",
    "    if keep_data['name'] not in tx_post_names:\n",
    "        tx_post_new.append(keep_data)\n",
    "        tx_post_names.add(keep_data['name'])\n",
    "df_tx = pd.DataFrame(tx_post_new)\n",
    "df_tx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tx = df_tx[['subreddit', 'title', 'num_comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca[['subreddit', 'title', 'num_comments']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = df_ca.append(df_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California_Politics</td>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit                                              title  num_comments\n",
       "0  California_Politics  State May Push Cities and Counties to Draw \"fa...             0\n",
       "1  California_Politics  New California rules for deadly police force g...             3\n",
       "2  California_Politics  The NRA Opposes A California Gun Regulation It...            21\n",
       "3  California_Politics  [CA-15] Eric Swalwell is expected to withdraw ...             5\n",
       "4  California_Politics  Eric Swalwell expected to end presidential bid...             4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit['ca'] = df_reddit['subreddit'].map({'California_Politics':1,\n",
    "                                                 'TexasPolitics':0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.drop(labels='subreddit', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>ca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State May Push Cities and Counties to Draw \"fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New California rules for deadly police force g...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The NRA Opposes A California Gun Regulation It...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CA-15] Eric Swalwell is expected to withdraw ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Swalwell expected to end presidential bid...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tom Steyer Is Telling Allies He’s Running for ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>California's Governor is Asking Trump for Emer...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>State Promises to Rebuild: Ridgecrest Will Not...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How California made a 'dramatic' impact on kin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California's Politically Powerful Unions Aim T...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New state budget a windfall for unions</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>End the federal prohibition of marijuana: vote...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overregulation is one reason for the high cost...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How ‘Big Soda’ used its clout to stop 5 of 5 C...</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kamala Harris Flip Flops on the Elimination of...</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Democrats to allow non-citizens to ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Trump Thinks Putin’s Attack on ‘Western-Style ...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Police accountability: Six months after new Ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>One year ago, L.A. approved an ambitious housi...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Newsom calls out California’s racist first gov...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>California’s proposed opioid tax would hurt pa...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Megan Dahle announces run for husband’s state ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[CA-50] Duncan Hunter 'Marital Spat,' Italian ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Indictments Charge Widespread Voting Fraud Sch...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter's Affairs Were Simp...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Berkeley moves to the forefront in California ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter Will Resign, Says D...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gavin Newsom’s biggest accomplishment as gover...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[CA-AD-73] Orange County Supervisor Lisa Bartl...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>California Gov. Gavin Newsom has signed his fi...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Why can’t California pass more housing legisla...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Why California is so liberal and progressive i...</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[CA-50] Former Hill staffer alleges Rep. Dunca...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Are California utilities doing enough to firep...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Many people are moving from California to Texa...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Lawmakers—cutting big checks to combat the hou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>California trails in regulating short-term len...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Commentary: Gavin Newsom faces his first defin...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A coworker told me that, due to a proposition ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Meet Rev. Andy Bales — The Man Who Lost A Leg ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Housing barbs spark backlash against Cupertino...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[CA-50] Rep. Duncan Hunter used campaign funds...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Who Can Be On The California Redistricting Com...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Why a California state senator wants to ban ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Any spike in repeat crimes after California sp...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>State Unemployment Drops; Job Growth, Shrinkin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>[San Diego Mayor] Former Gov. Jerry Brown, ex-...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>California program to track state worker haras...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>California Gov. Gavin Newsom’s not-so-spectacu...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NCAA says California schools could be banned f...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>How can California solve its homeless crisis? ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Treat workers as employees? Uber, Lyft and oth...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>California gas tax goes up July 1, but leaders...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>California’s budget offers new help for millio...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[CA-AD-73] Orange County Supervisor Lisa Bartl...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Berkeley helps to push back against excessive ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>The Future of the California Republican Party?</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>California vaccine bill clears Assembly panel ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>California political parties couldn’t use ‘ind...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Gov. Newsom, Bay Area leaders respond to propo...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>[CA-50] Darrell Issa, the former Oversight Com...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>It’s been a mess for decades. Can Gov. Newsom ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>California State University stashed $1.5 billi...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>California is one step closer to passing the m...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>A million independent voters risk being irrele...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>George Takei: 'At Least During the Internment ...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>a chart of what Californians will pay annually...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Poster for the upcoming CNP convention (June 2...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>In Los Angeles, only people of color are sente...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>California Activists Participate in “Muslim Da...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>California Legislature must act to protect env...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Immigrant entrepreneurs continue to shape Cali...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>How Redding, California, became an unlikely ep...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Crime fighting robot deployed in California</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Commentary: Cities pledge to find solutions to...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Wet California winter is a boon for skiers and...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Rent Control Measure Returns, Legislative Anal...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Bay Area Foothills College student homelessnes...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>California needs a big pot of money for wildfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Redding Town Hall receives criticism after \"Ch...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>The Homeless Are Dying in Record Numbers on th...</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Open Forum: Exaggerating California crime to p...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>California Legislation to Limit Predatory Lend...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Duncan Hunter's wife agrees to cooperate with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Warren rises to second in California poll</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Where Gov. Gavin Newsom wins and loses in newl...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>California just passed a $215 billion budget. ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>California Democratic 2020 presidential primar...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>New report: Google campus will lead to $235M m...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>California may automatically expunge 1 million...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Abuse in the Disabled Care Indjstry in California</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>California National Party Convention on June 22nd</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Towing a car can be financially ruinous. Shoul...</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[CA-50] Margaret Hunter to plead out in case a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Leslie Marshall: California is right to give i...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>LGBTQ nightclub in Fresno targeted by threats,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>L.A. council members propose taxing landlords ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>A new Citizens Redistricting Commission is bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Homeless crisis: Los Angeles County seeks help...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[CA-39, CA-45, CA-48, CA-49] Orange County Rep...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>California considers ban on facial recognition...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Here’s how California can become a tuition-fre...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Should community colleges in California build ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Does this vaccine bill go too far? Concerned f...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Report links evictions, homelessness in Los An...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Keeping an eye on sheriffs: California Democra...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>White House rejects carmakers’ plea for a deal...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Gov. Newsom stepped into a vaccine debate we s...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>In two California Senate special elections, Go...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>California’s top bullet train consultant is su...</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>‘New California’ Could Become 51st State</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>‘Swinging at Every Pitch’: California’s Govern...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>In The Dreamt Land, Mark Arax unpacks the Gold...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>California Democratic voters disagree with Nan...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Startup rents bunkbeds in San Francisco for $1...</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>How California became far more energy-efficien...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Californians favor dramatic changes to build m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Gov. Newsom criticized the new vaccine bill. A...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Landlords win, renters take a hit. Just one te...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>How California’s big plans to address housing ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>California’s long-overlooked Central Valley ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Brace for a Spike in Homelessness in LA County</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>California Democratic Party elects L.A. labor ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Northern California state Senate special elect...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Say goodbye to your local precinct. Voting in ...</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>The New Front in the SB 50 Battle Is Toni Atki...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>The looming California challenge for Kamala Ha...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>California’s state universities are a path to ...</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>The soul-crushing cost of college in Californi...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Death Watch: Keeping track of the bills Califo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Cruel and Unusual: A Guide to California’s Bro...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>California plan to prevent big rent hikes adva...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>A California bill to ban flavored tobacco prod...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>California gasoline prices increase following ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Rethinking Disaster Recovery After A Californi...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Los Angeles County Bans Use of Roundup Weed Ki...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Think California’s too big and influential? Wa...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>California probably will pick next president</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>In need of teacher housing, more California sc...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Two tax hikes for schools could end up on Cali...</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>[CA-50] Rep. Hunter on War-Crimes Suspect Gall...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>California says it's now in compliance with US...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>In California, Agreement On New Rules For When...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Why California's Efforts To Limit Soda Keep Fi...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Congress reaches deal on disaster aid: Califor...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Editorial: California needs to resume investme...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>California Senate passes legislation to create...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>California GOP picks favorites for re-flipping...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>California regulators aren’t taking action aga...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Trump threatens to cut millions from fire depa...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Poll: Two-thirds of California voters back SB ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>SF District Attorney Gascón questions SF polic...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>How Legalization Changed Humboldt County Marij...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Brawl erupts at convention for local-governmen...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Retired Oil Rigs off the California Coast Coul...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>“It’s time to politically destroy Kevin McCart...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Pipeline Shutdown Prevented 27 Million Tons of...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Trump tears into California for high-speed rai...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Guns, gas and soda – most California tax propo...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Commentary: These are the key conflicts in Cal...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Alabama and Georgia passed abortion bans. Cali...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Report: Gas price hike could be due to manipul...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Assemblyman Marc Berman’s college car camping ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>High-profile California housing bill dies with...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Trump Wants to Open Public Lands to Oil Drilli...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Kamala Harris still not catching on with Democ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>California’s New Prohibitionists</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Trump pardons media tycoon, former GOP leader ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Oakland Unified’s journey: When the state step...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>California Primary Becomes a Tantalizing Prize...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Simon Liu Isn’t A Sex Offender. But He’s Still...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>California is bringing law and order to big da...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>California files 50th lawsuit against Trump ad...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>California might triple number of marijuana sh...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>California on track to lose at least one congr...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Bill to stiffen California’s vaccine law must ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>California could bring radical change to singl...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>White House Hopefuls Swarm Rival’s Home Turf o...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>My patient was homeless. I knew she was going ...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Only one California Republican defied Trump on...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>CA Democratic Convention—photography questions</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>California governor unveils record $213 billio...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Trump Finalizes Plan to Open 725,500 Acres of ...</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Potential Impact of California’s ‘Split Roll’ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Revenge of the Coastal Elites: How California,...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>California defies Trump to ban pesticide linke...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Biden meets with big-dollar California donors,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Gavin Newsom’s California budget rises to $213...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Make California map chart with your data.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Mayor Pete blindsides Kamala Harris in California</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Gavin Newsom wants to end California taxes on ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Hot off the grille: Is California ready to leg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>In Trump vs. California, the state is winning ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>California Activists Take First Steps To Decri...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>State officials keep hiring their relatives. W...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>More reasons to be concerned with election int...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>The Trump Administration Stopped Working On Ca...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Swastika leaflets calling press ‘the enemy’ dr...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>In a Galaxy Not So Far Away: California Offici...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>When the next recession hits, will California ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>California’s 2018 midterm election: A dive int...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Reminder: Only post matters that are specific ...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>How powerful lawmakers are killing California ...</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>California bill fighting discrimination of 'na...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>The kingmakers in California’s 2020 elections ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>California Economy Soars Above U.K., France an...</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Los Angeles sets dramatic new goals for electr...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>A doctored photo and a lawsuit: California GOP...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Freshman Rep. Katie Hill, who was part of the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Trump abortion policy targeting Planned Parent...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>New Election Ordered After Ca. Dems Caught Che...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>California’s Hidden Corporate Tax Cuts — If ci...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Trump fracking plan targets over 1 million acr...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>San Francisco approves homeless shelter despit...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Joe Biden lacks big-name California allies as ...</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>California rent control moves forward with Gav...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Morgan Stanley to pay $150 million to settle C...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>California May Ban Schools From Suspending Stu...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Andrew Yang’s Campaign Grows with Large LA Rally</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Anti-vaccine families crowd California Capitol...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Rep. Katie Hill has Nancy Pelosi’s favor, but ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Newsom seeks an explanation for California’s h...</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>L.A. students want to lower voting age in scho...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Sideshows and the Extractive Economy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Combating homelessness</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Temecula will weigh city resolution to describ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>California Cities Have Shredded Decades of Pol...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Republicans Lining Up for 2020 House Fights in...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Beto O’Rourke opens his California campaign Sa...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>How Trump factors into California's charter sc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Bill To End Hair Discrimination In The Workpla...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Use of Force bills AB 392 and SB 230 now linke...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Julián Castro Is First 2020 Democratic Preside...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>CA-15: State Sen. Bob Wieckowski is forming an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>California housing bill targeting wealthy citi...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>California Democrats are awash in cash as the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>How Gov. Gavin Newsom is progressing on his ke...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Green Solutions for Governor Newsom's Climate ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Kamala Harris regrets California truancy law t...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>The Public Banking Revolution Is Upon Us: Cali...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>[CA-50] Rep. Hunter pretends to cross the Mexi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>[L.A. County Supervisor (District 4)] Former a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Buttigieg plans aggressive fundraising push in...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Former California water lobbyist, Trump's Inte...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Fastest litigant in the west: California’s on ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>U.S. appeals court upholds most of California’...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>California gives out too many tax breaks. And ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Swalwell calls for Barr to resign</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Years of bad rules led to California’s unfixab...</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Kamala Harris has collected millions from big ...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>In Wake Of Measles Cases, Health Advocates Wan...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>Proposed law would make hemp products legal in...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Red-Light Camera Ban Filed in House</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>MJ Hegar — Texas veteran behind viral \"Doors\" ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>How Will Texas Lawmakers Pay For School Financ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>Go big (with our bandwidth) or go home, Verizo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>Thousands of Texans were shocked by surprise m...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Quorum Report: Business effort begins at the C...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>McAllen orders Catholic Charities to vacate im...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Texas Senate Advances Property Tax Reform To F...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>El Paso Fire Department denies Trump's crowd c...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>HD125 with 100% reporting: Republican Fred Ran...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Images From the Dueling Trump and Beto O'Rourk...</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Four Democrats and one Republican vie for Just...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Judge’s Ruling Could Have Big Implications For...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>The AG's office told lawmakers it isn’t invest...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Beto O’Rourke to hold counter-speech same time...</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Texas Attorney General won’t investigate voter...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>Nearly 100 Texans have submitted ideas for wha...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Texas Public Opinion on Donald Trump, Immigrat...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Texas’ Dan Patrick claims taxes are too high. ...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>Richardson councilman, Scott Dunn, issues apol...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>Bill: HB 1408, introduced by Texas Rep. Jared ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>Texas Has Been Just a Prop for Trump From the ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>Texas bill would ban throttling in disaster ar...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>Texas AG Ken Paxton Says He Hasn’t Launched Cr...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>Texas Secretary of State David Whitley defends...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>Fact Checking the Voter Fraud Debacle. With st...</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>Analysis: A Green Appointee’s [Sec. of State D...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Navy veteran challenges fellow Navy veteran Re...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>Some Texas Democrats want Beto O'Rourke to shu...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>Trump's underwater (-1) in Texas for the first...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>Despite running in a midterm year, Beto O’Rour...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>Texas Republican Accused of Sending Student Nu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>A border fence did not lower crime rates in El...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>Native Americans protest building border wall ...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Beto O’Rourke Was Once Adrift in New York City...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>Texas Gov. Greg Abbott give his State of the S...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>Rick Perry is the “Designated Survivor” for to...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>Woman who filmed shooting of Botham Jean fired...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>Rattled by CBD Debate, Shop Owners React: “I D...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>Texas lawmaker wants cars with MSRP over $60k ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>More Civil Rights Groups Sue Over Secretary Of...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>Texas Republicans Are Lying About Voter Fraud ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>Abbott Names School Finance, Property Taxes, M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>Texas Public Opinion and Governor Abbott's Eme...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Elizabeth Warren APOLOGIZES for Native America...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Ted Cruz just compared rape victims to a man w...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Hispanics are propping up Texas economy, workf...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Property tax relief, but only for some: small ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>Audio: Border Patrol Plans to Light Up Butterf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>Texas AG asks federal judge to end DACA program</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>Texans Can Appeal Surprise Medical Bills, But ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Cornyn braces for brutal Texas reelection battle</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Despite beer and lobby ties, Speaker Dennis Bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>Can Oprah help restore the Beto O’Rourke glow?</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Texas Attorney General Ken Paxton is seeking m...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>President Trump’s Texas-size whoppers</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Far-Right Texas Legislator Files Bill to Compl...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>A No-Knock Raid in Houston Led to Deaths and P...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>US prepares to start building portion of Texas...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>Rural Texas needs more veterinarians, and Texa...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Texas optometrists \"just roll our eyes\" over t...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Texas judge says Sutherland Springs families c...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>“Should someone propose a 70% tax on the Patri...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Dems are headed to Texas to probe suspected vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>What goes on in the secret world of private pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Americans like me don't belong on Texas' botch...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Naturalized citizens suing over Texas voter ci...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Naturalized citizen is angry to find her name ...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Civil rights group sues Texas over order to in...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>100 Richest people in Texas</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>The Texas voter purge (Vox Overview of Recent ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Healing Power or a Dose of Trouble? CBD Oil Ta...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>State: All 366 on local list of potential nonc...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Question for the sub conservatives</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>In Harris County, Thousands Of Registered Vote...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Armed Man Disrupts Houston Library During Drag...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Texas lawmakers file identical companion bills...</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Indigenous Activists Set Up Protest Camp at So...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Texas Gov. Greg Abbott Downplays Concerns Abou...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>The Woman Behind the Kamala Harris Presidentia...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Texas Officials Begin Walking Back Allegations...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Many see \"Robin Hood\" as a villain. But lawmak...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>New legislation would allow Texas liquor store...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Former ‘Proud Boy’s’ trial canceled after witn...</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Analysis: Texas election officials serve up a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Here’s What You Need to Know About the Texas V...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Texas Republicans fear Trump could lose the st...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>The Poor in Texas Have Been Vastly Undercounte...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Top Texas Democrat rules out funding Trump's b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Democrat Art Fierro wins #HD79 special electio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Texas quietly tells counties that some of the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>Texas Republican introduces bill to make discr...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>2 Democrats headed to runoff in race to replac...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>[Special Election Today HD 79 &amp;amp; HD 145] Vo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>President Donald Trump: \"58,000 non-citizens v...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Texas officials sued by Latino group over sugg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>After Ted Cruz's close race against Beto O'Rou...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>'We were not welcomed': Gay couple rejected by...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>Harris County GOP draws fire for post blaming ...</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>Seliger, West Texas deserve better from Patrick.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>Winners and losers: When Texas House's powerfu...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>Texas says it has found 95,000 non citizens on...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>As it ponders where to put a Confederate plaqu...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>3 air traffic controllers in Texas resign over...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>TexSoS: Use of Non-U.S. Citizen Data obtained ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>[Meta] What happened to Moderator /u/Lemon_Lym...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Texas Sec. of State: 58K non-US citizens voted...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Texas Border Sheriffs: There is No Crisis and ...</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Opinion | I joined the GOP because it stood fo...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Most members of the Texas Legislature are whit...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>AG Announcement: 56,000 non-citizens have vote...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>Bennet Rips Cruz | User Clip | C-SPAN.org</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>Exclusive: White House preparing draft nationa...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>Texas conservatives claim LGBTQ equality bill ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>Where Texans in Congress come down on the gove...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>Texas House committee appointments bode well f...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>Every TX GOP House Member (except for William ...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>Texas Comptroller releases new report on Schoo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>Johnson Space Center workers being asked to cl...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Texas man organizes 'search party' event to lo...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>Texas House Speaker Dennis Bonnen names commit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>Sheila Jackson Lee Leaves 2 Posts After Aide S...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>Sheila Jackson Lee steps down from key posts a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>Cornyn and Cruz vote against motion to keep Ru...</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>Texas House Speaker appoints three Dallas lawm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>Dan Crenshaw: Only six Democrats voted to pay ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Texas Rep. Eric Johnson running for Dallas mayor</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>Some Parents Concerned Increased School Fundin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>To combat opioid addiction crisis, Texas AG Ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>Lt. Gov. Dan Patrick pulls Sen. Kel Seliger’s ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>“Bullshit” is how one Texan describes Trump an...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2019's Most Educated States in America: Texas #39</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>Texas coal power plants leaching toxic polluta...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>Trump Touts Border Wall In San Antonio, Which ...</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>Texas Judge Tells Jury God Says Defendant Is N...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>Beto O’Rourke’s road trip drives home his message</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>Rep. Will Hurd (R-Texas), calls Trump's border...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>The Big Dogs Of Texas Local Government: 18,000...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>Court rules against Planned Parenthood in Texa...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>Texas Unions are ready for this session.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>7 Feb marijuana lobbying day in Austin</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>GOP Rep. Will Hurd calls wall \"least effective...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>Texas Rep. Castro wants Trump impeached after ...</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>GOP tries to re-create its special-election ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>Court rules Texas can bar Planned Parenthood f...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>Both Cornyn and Cruz voted to protect Russia s...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>During the shutdown, government lawyers in Sou...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>How will the Texas Legislature address school ...</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>Report: Power Plants are Leaking Cancer-Causin...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Harris County judges unveil drastic new plan f...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>During the shutdown, government lawyers in Sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>“We Just Listened to Him Talk”: Sister Norma P...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Texas has most people without health insurance...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Tyler Congressman Louie Gohmert says Steve Kin...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Will 2019 Be The Year Texas Breweries Can Sell...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Can Texas Build a Working Medical Cannabis Pro...</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Governor, top Texans in Congress ask Trump not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>Texas House proposes massive increase for publ...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>Beto O’Rourke’s Washington Post Interview Spel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Congresswoman Alexandria Ocasio-Cortez Joins S...</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>Texas Senate wants billions to fund $5,000 pay...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Texas Rep. Dan Crenshaw under fire for shutdow...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Just 87 people voted today in the HD145 specia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>The Rise and Fall of the Tornillo Tent City fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>Why America’s Largest Migrant Youth Detention ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Beto O’Rourke’s immigration plan: No wall but ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Ted Cruz defends Trump on Russia: \"I don't see...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>We're up to five candidates filed for #HD125 s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Medicaid, opioids and abortion: Health care is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>[slimy former US Rep] Farenthold resigns as Ca...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Chip Roy of CD21. Workers will forever remembe...</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Julián Castro, Former Housing Secretary, Annou...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>El Paso Times column refutes AG Paxton's claim...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Tarrant County GOP votes to retain Muslim vice...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Texas officials vote to remove Confederate pla...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>Texas lawmakers indicate they may use rainy da...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>New Anti-LGBT Initiative Pops up Despite What ...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>Democrat Julian Castro expected to launch 2020...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>Texas ranked No.1 state for women entrepreneurs</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>Donald Trump says Lt. Gov. Dan Patrick offered...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>Trump Plans To Use Eminent Domain Against Priv...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>Texas Republicans Rally Behind Muslim Official...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Tarrant County GOP votes to keep vice chair de...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>Interesting response from Senator Cornyn's office</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>Cruz defends eminent domain for border wall</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>Tarrant County GOP’s vice-chairman survives re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>One small group of private citizens is posted ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>What some border residents feel might be the b...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>Rep Bonnen replaces coffee cups in the break r...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Climate change science presented to uncertain ...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>Victims of DV having right to a Court Appointe...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Newly elected House Speaker Dennis Bonnen says...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Beto O'Rourke plans nationwide road trip to me...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>GOP operatives dig for dirt against rising sta...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>Scientists To Abbott: “Climate Change Is Happe...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>San Antonio lands Texas’ first ‘opportunity zo...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>The Texas Legislature Gaveled In Today Without...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>Border ‘crisis’ echo in Texas is faint as lawm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>If the Texas Legislature won’t help solve Dall...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Texas libertarian Ron Paul: We don't need Trum...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Texas economy is ‘robust,’ giving lawmakers $9...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>Austin Council Member Delia Garza Elected Mayo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>In Latest Development About [Harris County] Ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>Live coverage: Texas lawmakers meet for the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Live coverage: Texas lawmakers meet for the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>‘I don’t think the two are mutually exclusive,...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Five things to watch in the Texas Legislature ...</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>A Texas State Senator has filed a bill that co...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>I'm Cassi Pollock, a reporter for The Texas Tr...</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>AMA Announcement: Cassi Pollock of the Texas T...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Hidalgo to refuse donations from Harris County...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>Federal judge closes book on Houston’s drag qu...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>Returning Texas Republicans In Congress Prepar...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>Quality Pre-K Can Improve Children’s Health, B...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Sen. Cruz, Rep. Rooney Introduce Constitutiona...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>What new marijuana laws might pass in Texas th...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>“School Finance Reform &amp;amp; Property Tax Refo...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>How An Oil Boom in West Texas Is Reshaping the...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>The Texas Education Challenge</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Joe Straus: I'm leaving the Texas Legislature ...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Dennis Bonnen has spent half his life in the T...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>One Texas county just swore in 17 black female...</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>Texas Democrat Introduces Bill to Remove Ban o...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>AG Pax­ton Releas­es State­ment on U.S. Dis­tr...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>ACLU sues Texas over law that says contractors...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>Rice Professor Mark Jones says Texas would be ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Texan of the Year 2018: Laura W. Bush | Dallas...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>Voting question: Voting in primaries, moving b...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>Beto’s viral video explains the overlooked rea...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>Discipline rates higher for Texas special educ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>Ken Paxton's allies are trying to kill case ag...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>AP Exclusive: Tornillo facility staying open i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>Trump threatens to shut U.S. border with Mexic...</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>In fiery filing, Ken Paxton prosecutors ask Te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>Is It OK To Criticize Politicians For Things T...</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>Texas Gov. Greg Abbott vacations in Japan, tak...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>Taxpayers need to know how money is spent, say...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>San Antonio Congressman Joaquin Castro Calls f...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>Texas Takes The Next Step To Make College More...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>Ted Cruz’s anti-Obamacare crusade continues wi...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>ICE Quietly Drops 200 Asylum Seekers at El Pas...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>Texas Tent City Housing 2,500 Migrant Children...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Texas’ One-Stop Shopping for Judge in Health C...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Beto O'Rourke Demands Closure Of Migrant Camp—...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>The 2020 Democratic frontrunner is a Republican</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Inside Bernie-world's war on Beto O'Rourke</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>Closed Until Further Notice, a letter from Bet...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>Beto O’Rourke Reflects On the Border as His Te...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  num_comments  ca\n",
       "0    State May Push Cities and Counties to Draw \"fa...             0   1\n",
       "1    New California rules for deadly police force g...             3   1\n",
       "2    The NRA Opposes A California Gun Regulation It...            21   1\n",
       "3    [CA-15] Eric Swalwell is expected to withdraw ...             5   1\n",
       "4    Eric Swalwell expected to end presidential bid...             4   1\n",
       "5    Tom Steyer Is Telling Allies He’s Running for ...             9   1\n",
       "6    California's Governor is Asking Trump for Emer...            29   1\n",
       "7    State Promises to Rebuild: Ridgecrest Will Not...             4   1\n",
       "8    How California made a 'dramatic' impact on kin...             0   1\n",
       "9    California's Politically Powerful Unions Aim T...            12   1\n",
       "10              New state budget a windfall for unions            15   1\n",
       "11   End the federal prohibition of marijuana: vote...             2   1\n",
       "12   Overregulation is one reason for the high cost...            59   1\n",
       "13   How ‘Big Soda’ used its clout to stop 5 of 5 C...            19   1\n",
       "14   Kamala Harris Flip Flops on the Elimination of...            64   1\n",
       "15   California Democrats to allow non-citizens to ...             6   1\n",
       "16   Trump Thinks Putin’s Attack on ‘Western-Style ...            34   1\n",
       "17   Police accountability: Six months after new Ca...             2   1\n",
       "18   One year ago, L.A. approved an ambitious housi...            24   1\n",
       "19   Newsom calls out California’s racist first gov...            16   1\n",
       "20   California’s proposed opioid tax would hurt pa...             2   1\n",
       "21   Megan Dahle announces run for husband’s state ...             2   1\n",
       "22   [CA-50] Duncan Hunter 'Marital Spat,' Italian ...             0   1\n",
       "23   Indictments Charge Widespread Voting Fraud Sch...             4   1\n",
       "24   [CA-50] Rep. Duncan Hunter's Affairs Were Simp...             1   1\n",
       "25   Berkeley moves to the forefront in California ...             1   1\n",
       "26   [CA-50] Rep. Duncan Hunter Will Resign, Says D...             2   1\n",
       "27   Gavin Newsom’s biggest accomplishment as gover...            15   1\n",
       "28   [CA-AD-73] Orange County Supervisor Lisa Bartl...             0   1\n",
       "29   California Gov. Gavin Newsom has signed his fi...            30   1\n",
       "30   Why can’t California pass more housing legisla...            26   1\n",
       "31   Why California is so liberal and progressive i...            84   1\n",
       "32   [CA-50] Former Hill staffer alleges Rep. Dunca...             7   1\n",
       "33   Are California utilities doing enough to firep...             1   1\n",
       "34   Many people are moving from California to Texa...            13   1\n",
       "35   Lawmakers—cutting big checks to combat the hou...             1   1\n",
       "36   California trails in regulating short-term len...             0   1\n",
       "37   Commentary: Gavin Newsom faces his first defin...             8   1\n",
       "38   A coworker told me that, due to a proposition ...             6   1\n",
       "39   Meet Rev. Andy Bales — The Man Who Lost A Leg ...             2   1\n",
       "40   Housing barbs spark backlash against Cupertino...             2   1\n",
       "41   [CA-50] Rep. Duncan Hunter used campaign funds...            25   1\n",
       "42   Who Can Be On The California Redistricting Com...             0   1\n",
       "43   Why a California state senator wants to ban ci...             2   1\n",
       "44   Any spike in repeat crimes after California sp...             3   1\n",
       "45   State Unemployment Drops; Job Growth, Shrinkin...             0   1\n",
       "46   [San Diego Mayor] Former Gov. Jerry Brown, ex-...            13   1\n",
       "47   California program to track state worker haras...             0   1\n",
       "48   California Gov. Gavin Newsom’s not-so-spectacu...            16   1\n",
       "49   NCAA says California schools could be banned f...             9   1\n",
       "50   How can California solve its homeless crisis? ...             3   1\n",
       "51   Treat workers as employees? Uber, Lyft and oth...             8   1\n",
       "52   California gas tax goes up July 1, but leaders...            22   1\n",
       "53   California’s budget offers new help for millio...             5   1\n",
       "54   [CA-AD-73] Orange County Supervisor Lisa Bartl...             2   1\n",
       "55   Berkeley helps to push back against excessive ...             0   1\n",
       "56      The Future of the California Republican Party?            12   1\n",
       "57   California vaccine bill clears Assembly panel ...            18   1\n",
       "58   California political parties couldn’t use ‘ind...            10   1\n",
       "59   Gov. Newsom, Bay Area leaders respond to propo...             7   1\n",
       "60   [CA-50] Darrell Issa, the former Oversight Com...             5   1\n",
       "61   It’s been a mess for decades. Can Gov. Newsom ...             8   1\n",
       "62   California State University stashed $1.5 billi...            11   1\n",
       "63   California is one step closer to passing the m...            12   1\n",
       "64   A million independent voters risk being irrele...             4   1\n",
       "65   George Takei: 'At Least During the Internment ...            12   1\n",
       "66   a chart of what Californians will pay annually...             0   1\n",
       "67   Poster for the upcoming CNP convention (June 2...            16   1\n",
       "68   In Los Angeles, only people of color are sente...             1   1\n",
       "69   California Activists Participate in “Muslim Da...             4   1\n",
       "70   California Legislature must act to protect env...             0   1\n",
       "71   Immigrant entrepreneurs continue to shape Cali...             0   1\n",
       "72   How Redding, California, became an unlikely ep...            12   1\n",
       "73         Crime fighting robot deployed in California             5   1\n",
       "74   Commentary: Cities pledge to find solutions to...            10   1\n",
       "75   Wet California winter is a boon for skiers and...             0   1\n",
       "76   Rent Control Measure Returns, Legislative Anal...            70   1\n",
       "77   Bay Area Foothills College student homelessnes...             4   1\n",
       "78   California needs a big pot of money for wildfi...             1   1\n",
       "79   Redding Town Hall receives criticism after \"Ch...            17   1\n",
       "80   The Homeless Are Dying in Record Numbers on th...            68   1\n",
       "81   Open Forum: Exaggerating California crime to p...            13   1\n",
       "82   California Legislation to Limit Predatory Lend...             9   1\n",
       "83   Duncan Hunter's wife agrees to cooperate with ...             1   1\n",
       "84           Warren rises to second in California poll            62   1\n",
       "85   Where Gov. Gavin Newsom wins and loses in newl...            27   1\n",
       "86   California just passed a $215 billion budget. ...             2   1\n",
       "87   California Democratic 2020 presidential primar...             0   1\n",
       "88   New report: Google campus will lead to $235M m...             1   1\n",
       "89   California may automatically expunge 1 million...             0   1\n",
       "90   Abuse in the Disabled Care Indjstry in California             3   1\n",
       "91   California National Party Convention on June 22nd             4   1\n",
       "92   Towing a car can be financially ruinous. Shoul...            22   1\n",
       "93   [CA-50] Margaret Hunter to plead out in case a...             1   1\n",
       "94   Leslie Marshall: California is right to give i...             5   1\n",
       "95   LGBTQ nightclub in Fresno targeted by threats,...             2   1\n",
       "96   L.A. council members propose taxing landlords ...             1   1\n",
       "97   A new Citizens Redistricting Commission is bei...             1   1\n",
       "98   Homeless crisis: Los Angeles County seeks help...             4   1\n",
       "99   [CA-39, CA-45, CA-48, CA-49] Orange County Rep...            20   1\n",
       "100  California considers ban on facial recognition...             1   1\n",
       "101  Here’s how California can become a tuition-fre...             5   1\n",
       "102  Should community colleges in California build ...            41   1\n",
       "103  Does this vaccine bill go too far? Concerned f...             7   1\n",
       "104  Report links evictions, homelessness in Los An...             1   1\n",
       "105  Keeping an eye on sheriffs: California Democra...             4   1\n",
       "106  White House rejects carmakers’ plea for a deal...            25   1\n",
       "107  Gov. Newsom stepped into a vaccine debate we s...             8   1\n",
       "108  In two California Senate special elections, Go...             0   1\n",
       "109  California’s top bullet train consultant is su...            30   1\n",
       "110           ‘New California’ Could Become 51st State            24   1\n",
       "111  ‘Swinging at Every Pitch’: California’s Govern...            15   1\n",
       "112  In The Dreamt Land, Mark Arax unpacks the Gold...             1   1\n",
       "113  California Democratic voters disagree with Nan...             3   1\n",
       "114  Startup rents bunkbeds in San Francisco for $1...            33   1\n",
       "115  How California became far more energy-efficien...             0   1\n",
       "116  Californians favor dramatic changes to build m...             0   1\n",
       "117  Gov. Newsom criticized the new vaccine bill. A...            11   1\n",
       "118  Landlords win, renters take a hit. Just one te...            65   1\n",
       "119  How California’s big plans to address housing ...             3   1\n",
       "120  California’s long-overlooked Central Valley ho...             0   1\n",
       "121     Brace for a Spike in Homelessness in LA County             5   1\n",
       "122  California Democratic Party elects L.A. labor ...             4   1\n",
       "123  Northern California state Senate special elect...             1   1\n",
       "124  Say goodbye to your local precinct. Voting in ...            29   1\n",
       "125  The New Front in the SB 50 Battle Is Toni Atki...             1   1\n",
       "126  The looming California challenge for Kamala Ha...             6   1\n",
       "127  California’s state universities are a path to ...            49   1\n",
       "128  The soul-crushing cost of college in Californi...             5   1\n",
       "129  Death Watch: Keeping track of the bills Califo...             1   1\n",
       "130  Cruel and Unusual: A Guide to California’s Bro...             2   1\n",
       "131  California plan to prevent big rent hikes adva...             2   1\n",
       "132  A California bill to ban flavored tobacco prod...            23   1\n",
       "133  California gasoline prices increase following ...             2   1\n",
       "134  Rethinking Disaster Recovery After A Californi...             5   1\n",
       "135  Los Angeles County Bans Use of Roundup Weed Ki...            36   1\n",
       "136  Think California’s too big and influential? Wa...            26   1\n",
       "137       California probably will pick next president            29   1\n",
       "138  In need of teacher housing, more California sc...            12   1\n",
       "139  Two tax hikes for schools could end up on Cali...            47   1\n",
       "140  [CA-50] Rep. Hunter on War-Crimes Suspect Gall...             9   1\n",
       "141  California says it's now in compliance with US...             5   1\n",
       "142  In California, Agreement On New Rules For When...            13   1\n",
       "143  Why California's Efforts To Limit Soda Keep Fi...            27   1\n",
       "144  Congress reaches deal on disaster aid: Califor...             3   1\n",
       "145  Editorial: California needs to resume investme...             3   1\n",
       "146  California Senate passes legislation to create...             5   1\n",
       "147  California GOP picks favorites for re-flipping...            14   1\n",
       "148  California regulators aren’t taking action aga...             0   1\n",
       "149  Trump threatens to cut millions from fire depa...            13   1\n",
       "150  Poll: Two-thirds of California voters back SB ...             9   1\n",
       "151  SF District Attorney Gascón questions SF polic...             3   1\n",
       "152  How Legalization Changed Humboldt County Marij...             0   1\n",
       "153  Brawl erupts at convention for local-governmen...             0   1\n",
       "154  Retired Oil Rigs off the California Coast Coul...             0   1\n",
       "155  “It’s time to politically destroy Kevin McCart...            38   1\n",
       "156  Pipeline Shutdown Prevented 27 Million Tons of...             4   1\n",
       "157  Trump tears into California for high-speed rai...             2   1\n",
       "158  Guns, gas and soda – most California tax propo...             8   1\n",
       "159  Commentary: These are the key conflicts in Cal...             0   1\n",
       "160  Alabama and Georgia passed abortion bans. Cali...             9   1\n",
       "161  Report: Gas price hike could be due to manipul...             1   1\n",
       "162  Assemblyman Marc Berman’s college car camping ...             0   1\n",
       "163  High-profile California housing bill dies with...            57   1\n",
       "164  Trump Wants to Open Public Lands to Oil Drilli...            12   1\n",
       "165  Kamala Harris still not catching on with Democ...            10   1\n",
       "166                   California’s New Prohibitionists             0   1\n",
       "167  Trump pardons media tycoon, former GOP leader ...             0   1\n",
       "168  Oakland Unified’s journey: When the state step...             0   1\n",
       "169  California Primary Becomes a Tantalizing Prize...            16   1\n",
       "170  Simon Liu Isn’t A Sex Offender. But He’s Still...             1   1\n",
       "171  California is bringing law and order to big da...             0   1\n",
       "172  California files 50th lawsuit against Trump ad...             2   1\n",
       "173  California might triple number of marijuana sh...            11   1\n",
       "174  California on track to lose at least one congr...             5   1\n",
       "175  Bill to stiffen California’s vaccine law must ...             4   1\n",
       "176  California could bring radical change to singl...            12   1\n",
       "177  White House Hopefuls Swarm Rival’s Home Turf o...             5   1\n",
       "178  My patient was homeless. I knew she was going ...            15   1\n",
       "179  Only one California Republican defied Trump on...            13   1\n",
       "180     CA Democratic Convention—photography questions             5   1\n",
       "181  California governor unveils record $213 billio...             7   1\n",
       "182  Trump Finalizes Plan to Open 725,500 Acres of ...            27   1\n",
       "183  Potential Impact of California’s ‘Split Roll’ ...             5   1\n",
       "184  Revenge of the Coastal Elites: How California,...             2   1\n",
       "185  California defies Trump to ban pesticide linke...             2   1\n",
       "186  Biden meets with big-dollar California donors,...             1   1\n",
       "187  Gavin Newsom’s California budget rises to $213...             5   1\n",
       "188          Make California map chart with your data.             0   1\n",
       "189  Mayor Pete blindsides Kamala Harris in California             5   1\n",
       "190  Gavin Newsom wants to end California taxes on ...            18   1\n",
       "191  Hot off the grille: Is California ready to leg...             1   1\n",
       "192  In Trump vs. California, the state is winning ...             8   1\n",
       "193  California Activists Take First Steps To Decri...            12   1\n",
       "194  State officials keep hiring their relatives. W...            11   1\n",
       "195  More reasons to be concerned with election int...             5   1\n",
       "196  The Trump Administration Stopped Working On Ca...            34   1\n",
       "197  Swastika leaflets calling press ‘the enemy’ dr...            14   1\n",
       "198  In a Galaxy Not So Far Away: California Offici...            24   1\n",
       "199  When the next recession hits, will California ...             9   1\n",
       "200  California’s 2018 midterm election: A dive int...             0   1\n",
       "201  Reminder: Only post matters that are specific ...            21   1\n",
       "202  How powerful lawmakers are killing California ...            14   1\n",
       "203  California bill fighting discrimination of 'na...            12   1\n",
       "204  The kingmakers in California’s 2020 elections ...             6   1\n",
       "205  California Economy Soars Above U.K., France an...            36   1\n",
       "206  Los Angeles sets dramatic new goals for electr...             4   1\n",
       "207  A doctored photo and a lawsuit: California GOP...            16   1\n",
       "208  Freshman Rep. Katie Hill, who was part of the ...             6   1\n",
       "209  Trump abortion policy targeting Planned Parent...            13   1\n",
       "210  New Election Ordered After Ca. Dems Caught Che...             5   1\n",
       "211  California’s Hidden Corporate Tax Cuts — If ci...             1   1\n",
       "212  Trump fracking plan targets over 1 million acr...             9   1\n",
       "213  San Francisco approves homeless shelter despit...            50   1\n",
       "214  Joe Biden lacks big-name California allies as ...            63   1\n",
       "215  California rent control moves forward with Gav...             7   1\n",
       "216  Morgan Stanley to pay $150 million to settle C...             1   1\n",
       "217  California May Ban Schools From Suspending Stu...             4   1\n",
       "218   Andrew Yang’s Campaign Grows with Large LA Rally             8   1\n",
       "219  Anti-vaccine families crowd California Capitol...             8   1\n",
       "220  Rep. Katie Hill has Nancy Pelosi’s favor, but ...             0   1\n",
       "221  Newsom seeks an explanation for California’s h...            57   1\n",
       "222  L.A. students want to lower voting age in scho...            31   1\n",
       "223               Sideshows and the Extractive Economy             0   1\n",
       "224                             Combating homelessness             0   1\n",
       "225  Temecula will weigh city resolution to describ...            26   1\n",
       "226  California Cities Have Shredded Decades of Pol...             1   1\n",
       "227  Republicans Lining Up for 2020 House Fights in...             9   1\n",
       "228  Beto O’Rourke opens his California campaign Sa...             3   1\n",
       "229  How Trump factors into California's charter sc...             0   1\n",
       "230  Bill To End Hair Discrimination In The Workpla...             1   1\n",
       "231  Use of Force bills AB 392 and SB 230 now linke...             1   1\n",
       "232  Julián Castro Is First 2020 Democratic Preside...             4   1\n",
       "233  CA-15: State Sen. Bob Wieckowski is forming an...             1   1\n",
       "234  California housing bill targeting wealthy citi...             6   1\n",
       "235  California Democrats are awash in cash as the ...             0   1\n",
       "236  How Gov. Gavin Newsom is progressing on his ke...            18   1\n",
       "237  Green Solutions for Governor Newsom's Climate ...             0   1\n",
       "238  Kamala Harris regrets California truancy law t...            17   1\n",
       "239  The Public Banking Revolution Is Upon Us: Cali...            18   1\n",
       "240  [CA-50] Rep. Hunter pretends to cross the Mexi...             1   1\n",
       "241  [L.A. County Supervisor (District 4)] Former a...             0   1\n",
       "242  Buttigieg plans aggressive fundraising push in...            38   1\n",
       "243  Former California water lobbyist, Trump's Inte...             0   1\n",
       "244  Fastest litigant in the west: California’s on ...             6   1\n",
       "245  U.S. appeals court upholds most of California’...             2   1\n",
       "246  California gives out too many tax breaks. And ...             6   1\n",
       "247                  Swalwell calls for Barr to resign             3   1\n",
       "248  Years of bad rules led to California’s unfixab...            52   1\n",
       "249  Kamala Harris has collected millions from big ...             9   1\n",
       "..                                                 ...           ...  ..\n",
       "732  In Wake Of Measles Cases, Health Advocates Wan...             9   0\n",
       "733  Proposed law would make hemp products legal in...            23   0\n",
       "734                Red-Light Camera Ban Filed in House            13   0\n",
       "735  MJ Hegar — Texas veteran behind viral \"Doors\" ...            13   0\n",
       "736  How Will Texas Lawmakers Pay For School Financ...             3   0\n",
       "737  Go big (with our bandwidth) or go home, Verizo...             0   0\n",
       "738  Thousands of Texans were shocked by surprise m...             7   0\n",
       "739  Quorum Report: Business effort begins at the C...            19   0\n",
       "740  McAllen orders Catholic Charities to vacate im...            33   0\n",
       "741  Texas Senate Advances Property Tax Reform To F...            10   0\n",
       "742  El Paso Fire Department denies Trump's crowd c...             4   0\n",
       "743  HD125 with 100% reporting: Republican Fred Ran...             1   0\n",
       "744  Images From the Dueling Trump and Beto O'Rourk...            39   0\n",
       "745  Four Democrats and one Republican vie for Just...             0   0\n",
       "746  Judge’s Ruling Could Have Big Implications For...             8   0\n",
       "747  The AG's office told lawmakers it isn’t invest...            17   0\n",
       "748  Beto O’Rourke to hold counter-speech same time...            58   0\n",
       "749  Texas Attorney General won’t investigate voter...             5   0\n",
       "750  Nearly 100 Texans have submitted ideas for wha...             5   0\n",
       "751  Texas Public Opinion on Donald Trump, Immigrat...             7   0\n",
       "752  Texas’ Dan Patrick claims taxes are too high. ...            41   0\n",
       "753  Richardson councilman, Scott Dunn, issues apol...            64   0\n",
       "754  Bill: HB 1408, introduced by Texas Rep. Jared ...             1   0\n",
       "755  Texas Has Been Just a Prop for Trump From the ...            11   0\n",
       "756  Texas bill would ban throttling in disaster ar...             5   0\n",
       "757  Texas AG Ken Paxton Says He Hasn’t Launched Cr...             3   0\n",
       "758  Texas Secretary of State David Whitley defends...            14   0\n",
       "759  Fact Checking the Voter Fraud Debacle. With st...            49   0\n",
       "760  Analysis: A Green Appointee’s [Sec. of State D...             0   0\n",
       "761  Navy veteran challenges fellow Navy veteran Re...             6   0\n",
       "762  Some Texas Democrats want Beto O'Rourke to shu...            47   0\n",
       "763  Trump's underwater (-1) in Texas for the first...            10   0\n",
       "764  Despite running in a midterm year, Beto O’Rour...             9   0\n",
       "765  Texas Republican Accused of Sending Student Nu...             0   0\n",
       "766  A border fence did not lower crime rates in El...             5   0\n",
       "767  Native Americans protest building border wall ...            30   0\n",
       "768  Beto O’Rourke Was Once Adrift in New York City...             7   0\n",
       "769  Texas Gov. Greg Abbott give his State of the S...            14   0\n",
       "770  Rick Perry is the “Designated Survivor” for to...            10   0\n",
       "771  Woman who filmed shooting of Botham Jean fired...             9   0\n",
       "772  Rattled by CBD Debate, Shop Owners React: “I D...             0   0\n",
       "773  Texas lawmaker wants cars with MSRP over $60k ...            11   0\n",
       "774  More Civil Rights Groups Sue Over Secretary Of...            10   0\n",
       "775  Texas Republicans Are Lying About Voter Fraud ...             4   0\n",
       "776  Abbott Names School Finance, Property Taxes, M...             2   0\n",
       "777  Texas Public Opinion and Governor Abbott's Eme...             1   0\n",
       "778  Elizabeth Warren APOLOGIZES for Native America...             7   0\n",
       "779  Ted Cruz just compared rape victims to a man w...             9   0\n",
       "780  Hispanics are propping up Texas economy, workf...            10   0\n",
       "781  Property tax relief, but only for some: small ...             2   0\n",
       "782  Audio: Border Patrol Plans to Light Up Butterf...             0   0\n",
       "783    Texas AG asks federal judge to end DACA program             6   0\n",
       "784  Texans Can Appeal Surprise Medical Bills, But ...             0   0\n",
       "785   Cornyn braces for brutal Texas reelection battle             1   0\n",
       "786  Despite beer and lobby ties, Speaker Dennis Bo...             2   0\n",
       "787     Can Oprah help restore the Beto O’Rourke glow?             2   0\n",
       "788  Texas Attorney General Ken Paxton is seeking m...            26   0\n",
       "789              President Trump’s Texas-size whoppers            14   0\n",
       "790  Far-Right Texas Legislator Files Bill to Compl...             2   0\n",
       "791  A No-Knock Raid in Houston Led to Deaths and P...            10   0\n",
       "792  US prepares to start building portion of Texas...             2   0\n",
       "793  Rural Texas needs more veterinarians, and Texa...             4   0\n",
       "794  Texas optometrists \"just roll our eyes\" over t...             6   0\n",
       "795  Texas judge says Sutherland Springs families c...             3   0\n",
       "796  “Should someone propose a 70% tax on the Patri...             4   0\n",
       "797  Dems are headed to Texas to probe suspected vo...             7   0\n",
       "798  What goes on in the secret world of private pr...             0   0\n",
       "799  Americans like me don't belong on Texas' botch...            11   0\n",
       "800  Naturalized citizens suing over Texas voter ci...             2   0\n",
       "801  Naturalized citizen is angry to find her name ...            32   0\n",
       "802  Civil rights group sues Texas over order to in...             1   0\n",
       "803                        100 Richest people in Texas            43   0\n",
       "804  The Texas voter purge (Vox Overview of Recent ...             1   0\n",
       "805  Healing Power or a Dose of Trouble? CBD Oil Ta...             4   0\n",
       "806  State: All 366 on local list of potential nonc...            18   0\n",
       "807                 Question for the sub conservatives            86   0\n",
       "808  In Harris County, Thousands Of Registered Vote...             7   0\n",
       "809  Armed Man Disrupts Houston Library During Drag...             3   0\n",
       "810  Texas lawmakers file identical companion bills...            30   0\n",
       "811  Indigenous Activists Set Up Protest Camp at So...             0   0\n",
       "812  Texas Gov. Greg Abbott Downplays Concerns Abou...             4   0\n",
       "813  The Woman Behind the Kamala Harris Presidentia...             4   0\n",
       "814  Texas Officials Begin Walking Back Allegations...            20   0\n",
       "815  Many see \"Robin Hood\" as a villain. But lawmak...             3   0\n",
       "816  New legislation would allow Texas liquor store...            73   0\n",
       "817  Former ‘Proud Boy’s’ trial canceled after witn...            25   0\n",
       "818  Analysis: Texas election officials serve up a ...             1   0\n",
       "819  Here’s What You Need to Know About the Texas V...             0   0\n",
       "820  Texas Republicans fear Trump could lose the st...            24   0\n",
       "821  The Poor in Texas Have Been Vastly Undercounte...            12   0\n",
       "822  Top Texas Democrat rules out funding Trump's b...             1   0\n",
       "823  Democrat Art Fierro wins #HD79 special electio...             0   0\n",
       "824  Texas quietly tells counties that some of the ...             6   0\n",
       "825  Texas Republican introduces bill to make discr...             4   0\n",
       "826  2 Democrats headed to runoff in race to replac...             0   0\n",
       "827  [Special Election Today HD 79 &amp; HD 145] Vo...             1   0\n",
       "828  President Donald Trump: \"58,000 non-citizens v...             9   0\n",
       "829  Texas officials sued by Latino group over sugg...             0   0\n",
       "830  After Ted Cruz's close race against Beto O'Rou...            19   0\n",
       "831  'We were not welcomed': Gay couple rejected by...            21   0\n",
       "832  Harris County GOP draws fire for post blaming ...            77   0\n",
       "833   Seliger, West Texas deserve better from Patrick.             1   0\n",
       "834  Winners and losers: When Texas House's powerfu...             4   0\n",
       "835  Texas says it has found 95,000 non citizens on...            29   0\n",
       "836  As it ponders where to put a Confederate plaqu...            47   0\n",
       "837  3 air traffic controllers in Texas resign over...            36   0\n",
       "838  TexSoS: Use of Non-U.S. Citizen Data obtained ...             5   0\n",
       "839  [Meta] What happened to Moderator /u/Lemon_Lym...             3   0\n",
       "840  Texas Sec. of State: 58K non-US citizens voted...            13   0\n",
       "841  Texas Border Sheriffs: There is No Crisis and ...            42   0\n",
       "842  Opinion | I joined the GOP because it stood fo...            10   0\n",
       "843  Most members of the Texas Legislature are whit...            18   0\n",
       "844  AG Announcement: 56,000 non-citizens have vote...            36   0\n",
       "845          Bennet Rips Cruz | User Clip | C-SPAN.org            13   0\n",
       "846  Exclusive: White House preparing draft nationa...             2   0\n",
       "847  Texas conservatives claim LGBTQ equality bill ...             2   0\n",
       "848  Where Texans in Congress come down on the gove...             5   0\n",
       "849  Texas House committee appointments bode well f...             4   0\n",
       "850  Every TX GOP House Member (except for William ...            13   0\n",
       "851  Texas Comptroller releases new report on Schoo...             1   0\n",
       "852  Johnson Space Center workers being asked to cl...             3   0\n",
       "853  Texas man organizes 'search party' event to lo...             8   0\n",
       "854  Texas House Speaker Dennis Bonnen names commit...             0   0\n",
       "855  Sheila Jackson Lee Leaves 2 Posts After Aide S...             0   0\n",
       "856  Sheila Jackson Lee steps down from key posts a...             0   0\n",
       "857  Cornyn and Cruz vote against motion to keep Ru...            44   0\n",
       "858  Texas House Speaker appoints three Dallas lawm...             0   0\n",
       "859  Dan Crenshaw: Only six Democrats voted to pay ...            12   0\n",
       "860   Texas Rep. Eric Johnson running for Dallas mayor             3   0\n",
       "861  Some Parents Concerned Increased School Fundin...             5   0\n",
       "862  To combat opioid addiction crisis, Texas AG Ke...             1   0\n",
       "863  Lt. Gov. Dan Patrick pulls Sen. Kel Seliger’s ...             3   0\n",
       "864  “Bullshit” is how one Texan describes Trump an...            36   0\n",
       "865  2019's Most Educated States in America: Texas #39             6   0\n",
       "866  Texas coal power plants leaching toxic polluta...            16   0\n",
       "867  Trump Touts Border Wall In San Antonio, Which ...            48   0\n",
       "868  Texas Judge Tells Jury God Says Defendant Is N...            11   0\n",
       "869  Beto O’Rourke’s road trip drives home his message             4   0\n",
       "870  Rep. Will Hurd (R-Texas), calls Trump's border...            14   0\n",
       "871  The Big Dogs Of Texas Local Government: 18,000...             3   0\n",
       "872  Court rules against Planned Parenthood in Texa...             3   0\n",
       "873           Texas Unions are ready for this session.             0   0\n",
       "874             7 Feb marijuana lobbying day in Austin            17   0\n",
       "875  GOP Rep. Will Hurd calls wall \"least effective...             2   0\n",
       "876  Texas Rep. Castro wants Trump impeached after ...            32   0\n",
       "877  GOP tries to re-create its special-election ma...             1   0\n",
       "878  Court rules Texas can bar Planned Parenthood f...            13   0\n",
       "879  Both Cornyn and Cruz voted to protect Russia s...            19   0\n",
       "880  During the shutdown, government lawyers in Sou...             2   0\n",
       "881  How will the Texas Legislature address school ...            81   0\n",
       "882  Report: Power Plants are Leaking Cancer-Causin...             4   0\n",
       "883  Harris County judges unveil drastic new plan f...            15   0\n",
       "884  During the shutdown, government lawyers in Sou...             0   0\n",
       "885  “We Just Listened to Him Talk”: Sister Norma P...             0   0\n",
       "886  Texas has most people without health insurance...            19   0\n",
       "887  Tyler Congressman Louie Gohmert says Steve Kin...             3   0\n",
       "888  Will 2019 Be The Year Texas Breweries Can Sell...            17   0\n",
       "889  Can Texas Build a Working Medical Cannabis Pro...            22   0\n",
       "890  Governor, top Texans in Congress ask Trump not...             0   0\n",
       "891  Texas House proposes massive increase for publ...            67   0\n",
       "892  Beto O’Rourke’s Washington Post Interview Spel...             3   0\n",
       "893  Congresswoman Alexandria Ocasio-Cortez Joins S...            69   0\n",
       "894  Texas Senate wants billions to fund $5,000 pay...            16   0\n",
       "895  Texas Rep. Dan Crenshaw under fire for shutdow...             5   0\n",
       "896  Just 87 people voted today in the HD145 specia...             0   0\n",
       "897  The Rise and Fall of the Tornillo Tent City fo...             0   0\n",
       "898  Why America’s Largest Migrant Youth Detention ...             0   0\n",
       "899  Beto O’Rourke’s immigration plan: No wall but ...             3   0\n",
       "900  Ted Cruz defends Trump on Russia: \"I don't see...            57   0\n",
       "901  We're up to five candidates filed for #HD125 s...             0   0\n",
       "902  Medicaid, opioids and abortion: Health care is...             0   0\n",
       "903  [slimy former US Rep] Farenthold resigns as Ca...             5   0\n",
       "904  Chip Roy of CD21. Workers will forever remembe...            45   0\n",
       "905  Julián Castro, Former Housing Secretary, Annou...            28   0\n",
       "906  El Paso Times column refutes AG Paxton's claim...             7   0\n",
       "907  Tarrant County GOP votes to retain Muslim vice...             6   0\n",
       "908  Texas officials vote to remove Confederate pla...            14   0\n",
       "909  Texas lawmakers indicate they may use rainy da...             5   0\n",
       "910  New Anti-LGBT Initiative Pops up Despite What ...            11   0\n",
       "911  Democrat Julian Castro expected to launch 2020...             1   0\n",
       "912    Texas ranked No.1 state for women entrepreneurs            31   0\n",
       "913  Donald Trump says Lt. Gov. Dan Patrick offered...             7   0\n",
       "914  Trump Plans To Use Eminent Domain Against Priv...            67   0\n",
       "915  Texas Republicans Rally Behind Muslim Official...             0   0\n",
       "916  Tarrant County GOP votes to keep vice chair de...             3   0\n",
       "917  Interesting response from Senator Cornyn's office            12   0\n",
       "918        Cruz defends eminent domain for border wall            32   0\n",
       "919  Tarrant County GOP’s vice-chairman survives re...             0   0\n",
       "920  One small group of private citizens is posted ...             2   0\n",
       "921  What some border residents feel might be the b...            34   0\n",
       "922  Rep Bonnen replaces coffee cups in the break r...            19   0\n",
       "923  Climate change science presented to uncertain ...            18   0\n",
       "924  Victims of DV having right to a Court Appointe...             3   0\n",
       "925  Newly elected House Speaker Dennis Bonnen says...             9   0\n",
       "926  Beto O'Rourke plans nationwide road trip to me...             0   0\n",
       "927  GOP operatives dig for dirt against rising sta...            27   0\n",
       "928  Scientists To Abbott: “Climate Change Is Happe...             6   0\n",
       "929  San Antonio lands Texas’ first ‘opportunity zo...            11   0\n",
       "930  The Texas Legislature Gaveled In Today Without...             0   0\n",
       "931  Border ‘crisis’ echo in Texas is faint as lawm...             0   0\n",
       "932  If the Texas Legislature won’t help solve Dall...             2   0\n",
       "933  Texas libertarian Ron Paul: We don't need Trum...             1   0\n",
       "934  Texas economy is ‘robust,’ giving lawmakers $9...             5   0\n",
       "935  Austin Council Member Delia Garza Elected Mayo...             0   0\n",
       "936  In Latest Development About [Harris County] Ba...             0   0\n",
       "937  Live coverage: Texas lawmakers meet for the st...             0   0\n",
       "938  Live coverage: Texas lawmakers meet for the st...             0   0\n",
       "939  ‘I don’t think the two are mutually exclusive,...            41   0\n",
       "940  Five things to watch in the Texas Legislature ...            29   0\n",
       "941  A Texas State Senator has filed a bill that co...            14   0\n",
       "942  I'm Cassi Pollock, a reporter for The Texas Tr...            57   0\n",
       "943  AMA Announcement: Cassi Pollock of the Texas T...             2   0\n",
       "944  Hidalgo to refuse donations from Harris County...             2   0\n",
       "945  Federal judge closes book on Houston’s drag qu...            19   0\n",
       "946  Returning Texas Republicans In Congress Prepar...             0   0\n",
       "947  Quality Pre-K Can Improve Children’s Health, B...             0   0\n",
       "948  Sen. Cruz, Rep. Rooney Introduce Constitutiona...            33   0\n",
       "949  What new marijuana laws might pass in Texas th...            65   0\n",
       "950  “School Finance Reform &amp; Property Tax Refo...            14   0\n",
       "951  How An Oil Boom in West Texas Is Reshaping the...            15   0\n",
       "952                      The Texas Education Challenge            10   0\n",
       "953  Joe Straus: I'm leaving the Texas Legislature ...            17   0\n",
       "954  Dennis Bonnen has spent half his life in the T...             4   0\n",
       "955  One Texas county just swore in 17 black female...            53   0\n",
       "956  Texas Democrat Introduces Bill to Remove Ban o...            16   0\n",
       "957  AG Pax­ton Releas­es State­ment on U.S. Dis­tr...            13   0\n",
       "958  ACLU sues Texas over law that says contractors...            26   0\n",
       "959  Rice Professor Mark Jones says Texas would be ...            12   0\n",
       "960  Texan of the Year 2018: Laura W. Bush | Dallas...             3   0\n",
       "961  Voting question: Voting in primaries, moving b...             3   0\n",
       "962  Beto’s viral video explains the overlooked rea...            15   0\n",
       "963  Discipline rates higher for Texas special educ...             2   0\n",
       "964  Ken Paxton's allies are trying to kill case ag...             3   0\n",
       "965  AP Exclusive: Tornillo facility staying open i...             1   0\n",
       "966  Trump threatens to shut U.S. border with Mexic...            36   0\n",
       "967  In fiery filing, Ken Paxton prosecutors ask Te...             1   0\n",
       "968  Is It OK To Criticize Politicians For Things T...            33   0\n",
       "969  Texas Gov. Greg Abbott vacations in Japan, tak...            19   0\n",
       "970  Taxpayers need to know how money is spent, say...             1   0\n",
       "971  San Antonio Congressman Joaquin Castro Calls f...             5   0\n",
       "972  Texas Takes The Next Step To Make College More...             9   0\n",
       "973  Ted Cruz’s anti-Obamacare crusade continues wi...            12   0\n",
       "974  ICE Quietly Drops 200 Asylum Seekers at El Pas...            21   0\n",
       "975  Texas Tent City Housing 2,500 Migrant Children...             2   0\n",
       "976  Texas’ One-Stop Shopping for Judge in Health C...             1   0\n",
       "977  Beto O'Rourke Demands Closure Of Migrant Camp—...            17   0\n",
       "978    The 2020 Democratic frontrunner is a Republican             3   0\n",
       "979         Inside Bernie-world's war on Beto O'Rourke            51   0\n",
       "980  Closed Until Further Notice, a letter from Bet...            27   0\n",
       "981  Beto O’Rourke Reflects On the Border as His Te...             1   0\n",
       "\n",
       "[1920 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_reddit).to_csv('reddit.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.511458\n",
       "1    0.488542\n",
       "Name: ca, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our baseline accuracy\n",
    "df_reddit['ca'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title']\n",
    "y = df_reddit['ca']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=79)\n",
    "pipe = Pipeline([\n",
    "            ('vec', CountVectorizer()),\n",
    "            ('model', LogisticRegression())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  61 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=2)]: Done 361 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=2)]: Done 717 out of 720 | elapsed:   35.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done 720 out of 720 | elapsed:   35.9s finished\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False), 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1500, min_df=4,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'vec__max_features': 1500, 'vec__min_df': 4, 'vec__ngram_range': (1, 2)}\n",
      "\n",
      " Cross Validation Accuracy Score: 0.91875\n",
      " Training Data Accuracy Score: 0.98125\n",
      " Testing Data Accuracy Score: 0.9208333333333333\n"
     ]
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'vec' : [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__max_features': [1500, 2000, 2500, 2700],\n",
    "    'vec__min_df': [2, 3, 4],\n",
    "#     'vec__max_df': [0.5, .60, .70],\n",
    "    'vec__ngram_range': [(1,2), (1,1)],\n",
    "    'model' : [LogisticRegression(), LogisticRegression(penalty='l1', solver='liblinear'), LogisticRegression(penalty='l2', solver='liblinear'), MultinomialNB]\n",
    "#     'vec__stop_words': ['english']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5, verbose=1, n_jobs=2)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(f' Best Parameters: {gs.best_params_}')\n",
    "print('')\n",
    "print(f' Cross Validation Accuracy Score: {gs.best_score_}')\n",
    "print(f' Training Data Accuracy Score: {gs.score(X_train, y_train)}')\n",
    "print(f' Testing Data Accuracy Score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.18865619, 0.03053555, 0.07005663, 0.03046961, 0.07289577,\n",
       "        0.02966752, 0.07243733, 0.03045788, 0.0748312 , 0.03077445,\n",
       "        0.06893911, 0.03069229, 0.07196903, 0.03061595, 0.07140865,\n",
       "        0.03032808, 0.07067137, 0.03006439, 0.07305241, 0.03057675,\n",
       "        0.07306881, 0.02957802, 0.07030916, 0.03063912, 0.06786838,\n",
       "        0.02912092, 0.06871009, 0.0287528 , 0.06891675, 0.02798734,\n",
       "        0.06915956, 0.02865872, 0.06998906, 0.03007245, 0.06853371,\n",
       "        0.03061614, 0.07114568, 0.03089533, 0.07210298, 0.0287921 ,\n",
       "        0.07042923, 0.02851915, 0.07878366, 0.02995596, 0.06798496,\n",
       "        0.02878227, 0.07238388, 0.02834811, 0.07018409, 0.02945461,\n",
       "        0.06937637, 0.02930965, 0.06950932, 0.02811322, 0.06867752,\n",
       "        0.02877254, 0.07135801, 0.02861009, 0.07596879, 0.02905493,\n",
       "        0.07254014, 0.02970834, 0.06847043, 0.02909288, 0.07176232,\n",
       "        0.02889848, 0.07128401, 0.03021598, 0.07185802, 0.03041396,\n",
       "        0.07037158, 0.02943029, 0.07186995, 0.03003359, 0.07369375,\n",
       "        0.03050823, 0.07050505, 0.02840695, 0.06912012, 0.03105593,\n",
       "        0.07231956, 0.02873302, 0.07130466, 0.02906137, 0.07051001,\n",
       "        0.0310082 , 0.0707458 , 0.02872462, 0.06819558, 0.03142185,\n",
       "        0.07247744, 0.02942257, 0.06972322, 0.03062849, 0.07683296,\n",
       "        0.0317306 , 0.07106447, 0.03043609, 0.0765295 , 0.02910686,\n",
       "        0.1054956 , 0.05844369, 0.09783301, 0.03137932, 0.06979661,\n",
       "        0.03069386, 0.06990542, 0.03002076, 0.07209954, 0.03002725,\n",
       "        0.07042155, 0.03229604, 0.08237081, 0.03799596, 0.0865849 ,\n",
       "        0.03469462, 0.08746018, 0.03196392, 0.09126606, 0.03249607,\n",
       "        0.07168107, 0.03202748, 0.07983766, 0.02984715, 0.0746006 ,\n",
       "        0.03348379, 0.07571564, 0.03015523, 0.07121153, 0.02848921,\n",
       "        0.06928535, 0.0280127 , 0.08050218, 0.03204002, 0.08110437,\n",
       "        0.03484755, 0.08439455, 0.02962794, 0.07389045, 0.03320661,\n",
       "        0.08224983, 0.03031158, 0.07600923, 0.03048639, 0.07973566,\n",
       "        0.03021855, 0.07196546, 0.02982311, 0.08131795, 0.03143153,\n",
       "        0.07864122, 0.03028798, 0.07530298, 0.03051343, 0.07932343,\n",
       "        0.03024707, 0.07747951, 0.03304501, 0.07541494, 0.02945046,\n",
       "        0.07953191, 0.03154898, 0.08516822, 0.03189411, 0.08476782,\n",
       "        0.03123407, 0.07533698, 0.03189187, 0.09247217, 0.03487329,\n",
       "        0.08748031, 0.03212562, 0.08039322, 0.03318958, 0.08026118,\n",
       "        0.03300734, 0.08256345, 0.03678885, 0.09099054, 0.03885484,\n",
       "        0.08125596, 0.0332818 , 0.08412056, 0.02907629, 0.07115684,\n",
       "        0.02951937, 0.10697241, 0.03097181, 0.08310938, 0.02986045,\n",
       "        0.07177706, 0.03389187]),\n",
       " 'std_fit_time': array([0.14465076, 0.00106576, 0.00124527, 0.00044418, 0.00338671,\n",
       "        0.00170225, 0.00235424, 0.00166945, 0.00290333, 0.00170284,\n",
       "        0.00179593, 0.00207138, 0.0025728 , 0.00178704, 0.00488449,\n",
       "        0.00142418, 0.00095994, 0.00091685, 0.00473583, 0.00165994,\n",
       "        0.00477863, 0.00172816, 0.00262185, 0.00194594, 0.00193279,\n",
       "        0.00161664, 0.00117796, 0.00116827, 0.00297302, 0.00089667,\n",
       "        0.00116186, 0.00108657, 0.00241561, 0.00307369, 0.00257364,\n",
       "        0.00278126, 0.00330305, 0.00146395, 0.00255592, 0.00140332,\n",
       "        0.00239847, 0.00087145, 0.00800027, 0.00068801, 0.00277404,\n",
       "        0.00143143, 0.00379318, 0.00108263, 0.0018716 , 0.00092836,\n",
       "        0.00184051, 0.00136678, 0.00158395, 0.00102682, 0.00229014,\n",
       "        0.00094958, 0.00264575, 0.00123279, 0.01006507, 0.00186608,\n",
       "        0.00274117, 0.00151351, 0.00293259, 0.00234277, 0.00411317,\n",
       "        0.0010963 , 0.00429123, 0.00174534, 0.00344594, 0.00106048,\n",
       "        0.00461871, 0.00270613, 0.00242154, 0.0015974 , 0.00744516,\n",
       "        0.0026324 , 0.0007431 , 0.00122002, 0.00155478, 0.00211995,\n",
       "        0.00331637, 0.00099613, 0.00227503, 0.0014595 , 0.00233543,\n",
       "        0.00289911, 0.00145701, 0.00102752, 0.00302528, 0.00176385,\n",
       "        0.00178872, 0.00132048, 0.00282317, 0.00261222, 0.00996472,\n",
       "        0.00482622, 0.00103607, 0.00136863, 0.00456957, 0.00082086,\n",
       "        0.02629471, 0.01765573, 0.0332631 , 0.00157635, 0.00175975,\n",
       "        0.00260893, 0.00143852, 0.00150252, 0.00267884, 0.00196735,\n",
       "        0.00259371, 0.00400333, 0.00576926, 0.00165608, 0.00327187,\n",
       "        0.00290102, 0.01435531, 0.00311495, 0.0125936 , 0.00281042,\n",
       "        0.00198562, 0.00252765, 0.00440268, 0.00150524, 0.00162087,\n",
       "        0.00352597, 0.00483707, 0.00222125, 0.00747332, 0.00225464,\n",
       "        0.00199479, 0.00103518, 0.00909462, 0.00221728, 0.00523672,\n",
       "        0.00269242, 0.00506996, 0.00294345, 0.00307486, 0.00049605,\n",
       "        0.00693688, 0.00252409, 0.00368271, 0.00121783, 0.00219855,\n",
       "        0.00190587, 0.00168104, 0.00202884, 0.00350057, 0.00217299,\n",
       "        0.00118301, 0.00134497, 0.00311955, 0.00187067, 0.00318758,\n",
       "        0.00065413, 0.00172769, 0.00553899, 0.00757353, 0.00149816,\n",
       "        0.01110868, 0.00296873, 0.00661732, 0.0045998 , 0.00495966,\n",
       "        0.00226625, 0.0021533 , 0.00498884, 0.01929662, 0.00450276,\n",
       "        0.01327505, 0.00316283, 0.00225387, 0.00572389, 0.00741136,\n",
       "        0.00474413, 0.00361118, 0.01130851, 0.00848964, 0.00670213,\n",
       "        0.00585379, 0.00385476, 0.00695912, 0.00090631, 0.00197719,\n",
       "        0.00163671, 0.02915676, 0.00203017, 0.01011885, 0.00140631,\n",
       "        0.00194996, 0.00211879]),\n",
       " 'mean_score_time': array([0.00858006, 0.00499678, 0.00871439, 0.00529337, 0.0082046 ,\n",
       "        0.00526848, 0.00850301, 0.00522833, 0.00875258, 0.00513892,\n",
       "        0.00879107, 0.00495214, 0.00910778, 0.00522947, 0.00887914,\n",
       "        0.00499654, 0.00812821, 0.00585022, 0.00839663, 0.00558481,\n",
       "        0.0082243 , 0.00529575, 0.00817003, 0.00516438, 0.00895286,\n",
       "        0.00619516, 0.0085196 , 0.00532885, 0.00950513, 0.00540919,\n",
       "        0.00858641, 0.00585647, 0.00931907, 0.00549989, 0.00837297,\n",
       "        0.00558958, 0.00868196, 0.00554562, 0.00865297, 0.00541887,\n",
       "        0.00874653, 0.00608125, 0.00888243, 0.00543599, 0.00985422,\n",
       "        0.0060452 , 0.00842214, 0.00533204, 0.00870514, 0.00547409,\n",
       "        0.00921054, 0.00503993, 0.00801406, 0.00492187, 0.0093925 ,\n",
       "        0.00536227, 0.00816832, 0.00510011, 0.01050439, 0.00499964,\n",
       "        0.00843525, 0.00506291, 0.00917411, 0.00510473, 0.01040721,\n",
       "        0.00499854, 0.00923576, 0.00511637, 0.00814538, 0.00518303,\n",
       "        0.00801797, 0.00499249, 0.00883889, 0.00546904, 0.01060967,\n",
       "        0.00535684, 0.00855203, 0.0056798 , 0.00893741, 0.00626955,\n",
       "        0.00874257, 0.00615082, 0.00829053, 0.0063303 , 0.00872006,\n",
       "        0.00591178, 0.00868206, 0.00535536, 0.01004224, 0.00544319,\n",
       "        0.00868592, 0.00604401, 0.00885987, 0.00535297, 0.01235666,\n",
       "        0.00546861, 0.00833793, 0.00550442, 0.0081459 , 0.00498676,\n",
       "        0.01366396, 0.01363411, 0.00937338, 0.00506992, 0.00867023,\n",
       "        0.00533576, 0.00893016, 0.00489836, 0.00875845, 0.00508614,\n",
       "        0.00822062, 0.00504203, 0.01041179, 0.00560622, 0.0111403 ,\n",
       "        0.00629191, 0.00822215, 0.00530496, 0.0091424 , 0.00512047,\n",
       "        0.00932388, 0.00644011, 0.00933771, 0.00543232, 0.00993519,\n",
       "        0.00564122, 0.00875421, 0.00558457, 0.00926995, 0.00529575,\n",
       "        0.00834198, 0.00525537, 0.01121359, 0.00592856, 0.01014705,\n",
       "        0.00678487, 0.01195078, 0.00539885, 0.00917397, 0.00625153,\n",
       "        0.01062074, 0.00566163, 0.00882602, 0.00561881, 0.00947447,\n",
       "        0.00585957, 0.00960221, 0.00588565, 0.00954084, 0.0057672 ,\n",
       "        0.00935535, 0.00600748, 0.00981159, 0.00576577, 0.00983934,\n",
       "        0.0057765 , 0.01037817, 0.00631118, 0.00868087, 0.00565195,\n",
       "        0.01031551, 0.00569983, 0.01129198, 0.00673413, 0.0096621 ,\n",
       "        0.00636883, 0.00966225, 0.00641041, 0.00950289, 0.00890861,\n",
       "        0.00988503, 0.00716338, 0.01212921, 0.00593085, 0.00998483,\n",
       "        0.00708961, 0.01230054, 0.00653968, 0.01288676, 0.00671506,\n",
       "        0.012498  , 0.00696235, 0.01011138, 0.00556288, 0.00987649,\n",
       "        0.00545726, 0.01335182, 0.00757942, 0.01064334, 0.00602422,\n",
       "        0.01017494, 0.00628963]),\n",
       " 'std_score_time': array([5.36897330e-04, 1.66599219e-04, 4.46136740e-04, 6.58997992e-04,\n",
       "        4.56590704e-04, 6.46336851e-04, 4.29432021e-04, 1.40780742e-04,\n",
       "        8.97539780e-04, 1.70720915e-04, 1.00560468e-03, 8.96526648e-05,\n",
       "        1.08832079e-03, 2.61944016e-04, 7.68969391e-04, 1.92870050e-04,\n",
       "        2.54483546e-04, 1.66336984e-03, 2.66491699e-04, 5.95252588e-04,\n",
       "        1.40774686e-04, 3.61442692e-04, 3.97158943e-04, 5.34685417e-04,\n",
       "        7.63284203e-04, 5.89609187e-04, 1.99884847e-04, 2.04293611e-04,\n",
       "        1.41842946e-03, 3.88668163e-04, 1.66778321e-04, 5.40475145e-04,\n",
       "        7.06521541e-04, 1.67272096e-04, 3.08067487e-04, 5.06371418e-04,\n",
       "        2.74806405e-04, 1.71043223e-04, 3.81755168e-04, 1.44002027e-04,\n",
       "        7.19880042e-04, 1.22246838e-03, 3.47401694e-04, 2.30123579e-04,\n",
       "        1.66050328e-03, 1.30165276e-03, 2.55416567e-04, 1.29439587e-04,\n",
       "        7.02569189e-04, 6.86826897e-04, 1.34258113e-03, 1.58811544e-04,\n",
       "        1.82632902e-04, 1.80608984e-04, 1.21857682e-03, 6.31028034e-04,\n",
       "        2.24462177e-04, 2.22991897e-04, 1.60011483e-03, 2.05788797e-04,\n",
       "        4.25697371e-04, 1.41268479e-04, 1.24067317e-03, 2.95728139e-04,\n",
       "        3.59391357e-03, 1.56449581e-04, 1.13640515e-03, 1.03586066e-04,\n",
       "        1.73264042e-04, 2.72315284e-04, 2.12445826e-04, 1.07458827e-04,\n",
       "        7.44561243e-04, 1.72581946e-04, 2.88821922e-03, 1.58745629e-04,\n",
       "        2.89651450e-04, 7.05805776e-04, 6.49685202e-04, 6.46998771e-04,\n",
       "        2.38004468e-04, 9.21054569e-04, 1.87631029e-04, 1.39108523e-03,\n",
       "        2.48302419e-04, 3.79673226e-04, 1.70279971e-04, 1.47123931e-04,\n",
       "        1.66877630e-03, 2.57197521e-04, 2.34077977e-04, 6.22051328e-04,\n",
       "        5.19543593e-04, 1.73924940e-04, 4.70834936e-03, 3.47897136e-04,\n",
       "        1.58525257e-04, 6.86097595e-04, 2.02962285e-04, 1.35762247e-04,\n",
       "        4.67230909e-03, 5.73363672e-03, 1.98509142e-03, 1.14590864e-04,\n",
       "        9.86769707e-04, 6.35397070e-04, 1.86676917e-03, 1.20199989e-04,\n",
       "        9.25680975e-04, 2.10245793e-04, 2.54810793e-04, 1.68174078e-04,\n",
       "        1.40362942e-03, 9.08758470e-04, 2.77188404e-03, 1.49663396e-03,\n",
       "        6.87734747e-05, 5.25697096e-04, 1.31024743e-03, 2.46277109e-04,\n",
       "        9.48500072e-04, 1.14852382e-03, 1.93681609e-03, 2.31820817e-04,\n",
       "        2.20300102e-03, 7.03227358e-04, 4.74936546e-04, 3.66530416e-04,\n",
       "        1.20607786e-03, 1.69875404e-04, 2.13309914e-04, 1.53044791e-04,\n",
       "        2.07125316e-03, 5.86597688e-04, 8.16274123e-04, 8.23041398e-04,\n",
       "        2.26342019e-03, 2.64314480e-04, 2.52536335e-04, 1.66039286e-04,\n",
       "        1.28200860e-03, 2.63840623e-04, 2.67588676e-04, 2.64020648e-04,\n",
       "        4.47484665e-04, 1.34966134e-03, 1.57992889e-03, 9.93943436e-04,\n",
       "        8.99674704e-04, 4.55744023e-04, 2.77625527e-04, 2.50662523e-04,\n",
       "        1.39619788e-03, 3.39365732e-04, 9.07173707e-04, 6.60236736e-04,\n",
       "        1.31437178e-03, 4.04749846e-04, 2.91061862e-04, 3.48094979e-04,\n",
       "        1.81587249e-03, 4.79036015e-04, 2.84695988e-03, 1.32351933e-03,\n",
       "        5.97846981e-04, 5.11869185e-04, 1.19188763e-03, 1.02512004e-03,\n",
       "        5.23842416e-04, 5.00169187e-03, 8.56156588e-04, 6.98701478e-04,\n",
       "        5.30315694e-03, 2.96703445e-04, 4.47739265e-04, 9.56227949e-04,\n",
       "        2.35554441e-03, 1.31749615e-03, 6.41096997e-03, 1.55930146e-03,\n",
       "        1.52403898e-03, 2.00286519e-03, 7.84902546e-04, 1.95674507e-04,\n",
       "        1.65257594e-03, 1.51218182e-04, 3.54971760e-03, 2.41598723e-03,\n",
       "        1.91667091e-03, 5.46678290e-04, 1.22593046e-03, 5.20087419e-04]),\n",
       " 'param_model': masked_array(data=[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "           tol=0.0001, verbose=0, warm_start=False),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "                    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec': masked_array(data=[CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       "                    TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__max_features': masked_array(data=[1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700, 1500, 1500, 1500, 1500, 1500, 1500,\n",
       "                    2000, 2000, 2000, 2000, 2000, 2000, 2500, 2500, 2500,\n",
       "                    2500, 2500, 2500, 2700, 2700, 2700, 2700, 2700, 2700,\n",
       "                    1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700, 1500, 1500, 1500, 1500, 1500, 1500,\n",
       "                    2000, 2000, 2000, 2000, 2000, 2000, 2500, 2500, 2500,\n",
       "                    2500, 2500, 2500, 2700, 2700, 2700, 2700, 2700, 2700,\n",
       "                    1500, 1500, 1500, 1500, 1500, 1500, 2000, 2000, 2000,\n",
       "                    2000, 2000, 2000, 2500, 2500, 2500, 2500, 2500, 2500,\n",
       "                    2700, 2700, 2700, 2700, 2700, 2700, 1500, 1500, 1500,\n",
       "                    1500, 1500, 1500, 2000, 2000, 2000, 2000, 2000, 2000,\n",
       "                    2500, 2500, 2500, 2500, 2500, 2500, 2700, 2700, 2700,\n",
       "                    2700, 2700, 2700],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__min_df': masked_array(data=[2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4,\n",
       "                    2, 2, 3, 3, 4, 4, 2, 2, 3, 3, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_vec__ngram_range': masked_array(data=[(1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "             n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "             tol=0.0001, verbose=0, warm_start=False),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=1500, min_df=3,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 1500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2000,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2500,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 2,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 3,\n",
       "   'vec__ngram_range': (1, 1)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 2)},\n",
       "  {'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "   'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None),\n",
       "   'vec__max_features': 2700,\n",
       "   'vec__min_df': 4,\n",
       "   'vec__ngram_range': (1, 1)}],\n",
       " 'split0_test_score': array([0.90657439, 0.90657439, 0.9100346 , 0.89965398, 0.90311419,\n",
       "        0.90657439, 0.90657439, 0.9100346 , 0.9100346 , 0.89965398,\n",
       "        0.90311419, 0.90657439, 0.9100346 , 0.9100346 , 0.9100346 ,\n",
       "        0.89965398, 0.90311419, 0.90657439, 0.91349481, 0.9100346 ,\n",
       "        0.9100346 , 0.89965398, 0.90311419, 0.90657439, 0.90311419,\n",
       "        0.89965398, 0.90657439, 0.89965398, 0.9100346 , 0.90657439,\n",
       "        0.90657439, 0.90311419, 0.90657439, 0.89965398, 0.9100346 ,\n",
       "        0.90657439, 0.90657439, 0.90311419, 0.90657439, 0.89965398,\n",
       "        0.9100346 , 0.90657439, 0.90657439, 0.90311419, 0.90657439,\n",
       "        0.89965398, 0.9100346 , 0.90657439, 0.90311419, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.89965398, 0.91349481, 0.90311419,\n",
       "        0.91349481, 0.90311419, 0.91349481, 0.89965398, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.90311419, 0.91349481, 0.89965398,\n",
       "        0.91349481, 0.90311419, 0.91349481, 0.90311419, 0.91349481,\n",
       "        0.89965398, 0.91349481, 0.87889273, 0.87197232, 0.87197232,\n",
       "        0.86851211, 0.86851211, 0.86851211, 0.87543253, 0.8650519 ,\n",
       "        0.87543253, 0.86851211, 0.86851211, 0.86851211, 0.87543253,\n",
       "        0.8650519 , 0.87543253, 0.86851211, 0.86851211, 0.86851211,\n",
       "        0.86851211, 0.8650519 , 0.87543253, 0.86851211, 0.86851211,\n",
       "        0.86851211, 0.90657439, 0.90657439, 0.9100346 , 0.89965398,\n",
       "        0.90311419, 0.90657439, 0.90657439, 0.9100346 , 0.9100346 ,\n",
       "        0.89965398, 0.90311419, 0.90657439, 0.9100346 , 0.9100346 ,\n",
       "        0.9100346 , 0.89965398, 0.90311419, 0.90657439, 0.91349481,\n",
       "        0.9100346 , 0.9100346 , 0.89965398, 0.90311419, 0.90657439,\n",
       "        0.90311419, 0.89965398, 0.90657439, 0.89965398, 0.9100346 ,\n",
       "        0.90657439, 0.90657439, 0.90311419, 0.90657439, 0.89965398,\n",
       "        0.9100346 , 0.90657439, 0.90657439, 0.90311419, 0.90657439,\n",
       "        0.89965398, 0.9100346 , 0.90657439, 0.90657439, 0.90311419,\n",
       "        0.90657439, 0.89965398, 0.9100346 , 0.90657439, 0.9100346 ,\n",
       "        0.89965398, 0.91349481, 0.90311419, 0.91349481, 0.90657439,\n",
       "        0.9100346 , 0.89619377, 0.91349481, 0.90311419, 0.91349481,\n",
       "        0.90657439, 0.90657439, 0.89619377, 0.91349481, 0.90311419,\n",
       "        0.91349481, 0.90657439, 0.91349481, 0.89619377, 0.91349481,\n",
       "        0.90311419, 0.91349481, 0.90657439, 0.87543253, 0.85121107,\n",
       "        0.87889273, 0.8615917 , 0.87889273, 0.88235294, 0.88927336,\n",
       "        0.84775087, 0.87889273, 0.8615917 , 0.87889273, 0.88235294,\n",
       "        0.87889273, 0.84775087, 0.87889273, 0.8615917 , 0.87889273,\n",
       "        0.88235294, 0.88581315, 0.84775087, 0.87889273, 0.8615917 ,\n",
       "        0.87889273, 0.88235294]),\n",
       " 'split1_test_score': array([0.93425606, 0.92041522, 0.92387543, 0.92041522, 0.93079585,\n",
       "        0.92733564, 0.92387543, 0.91695502, 0.92387543, 0.92041522,\n",
       "        0.93079585, 0.92733564, 0.92041522, 0.91695502, 0.92387543,\n",
       "        0.92041522, 0.93079585, 0.92733564, 0.91695502, 0.91695502,\n",
       "        0.92387543, 0.92041522, 0.93079585, 0.92733564, 0.94809689,\n",
       "        0.93771626, 0.94117647, 0.93079585, 0.94809689, 0.93425606,\n",
       "        0.93771626, 0.93079585, 0.94463668, 0.93079585, 0.94809689,\n",
       "        0.93425606, 0.94117647, 0.93079585, 0.94463668, 0.93079585,\n",
       "        0.94809689, 0.93425606, 0.93771626, 0.93079585, 0.94463668,\n",
       "        0.93079585, 0.94809689, 0.93425606, 0.90657439, 0.90311419,\n",
       "        0.91349481, 0.9100346 , 0.92041522, 0.91349481, 0.9100346 ,\n",
       "        0.90311419, 0.91349481, 0.9100346 , 0.92041522, 0.91349481,\n",
       "        0.9100346 , 0.90311419, 0.91349481, 0.9100346 , 0.92041522,\n",
       "        0.91349481, 0.9100346 , 0.90311419, 0.91349481, 0.9100346 ,\n",
       "        0.92041522, 0.91349481, 0.83044983, 0.83737024, 0.83391003,\n",
       "        0.84429066, 0.85121107, 0.85467128, 0.83044983, 0.83044983,\n",
       "        0.83044983, 0.84429066, 0.85121107, 0.85467128, 0.82698962,\n",
       "        0.83044983, 0.83044983, 0.84429066, 0.85121107, 0.85467128,\n",
       "        0.82698962, 0.83044983, 0.83044983, 0.84429066, 0.85121107,\n",
       "        0.85467128, 0.93425606, 0.92041522, 0.92387543, 0.92041522,\n",
       "        0.93079585, 0.92733564, 0.92387543, 0.91695502, 0.92387543,\n",
       "        0.92041522, 0.93079585, 0.92733564, 0.92041522, 0.91695502,\n",
       "        0.92387543, 0.92041522, 0.93079585, 0.92733564, 0.91695502,\n",
       "        0.91695502, 0.92387543, 0.92041522, 0.93079585, 0.92733564,\n",
       "        0.94809689, 0.93771626, 0.94117647, 0.93079585, 0.94809689,\n",
       "        0.93425606, 0.93771626, 0.93079585, 0.94463668, 0.93079585,\n",
       "        0.94809689, 0.93425606, 0.94117647, 0.93079585, 0.94463668,\n",
       "        0.93079585, 0.94809689, 0.93425606, 0.93771626, 0.93079585,\n",
       "        0.94463668, 0.93079585, 0.94809689, 0.93425606, 0.94809689,\n",
       "        0.94809689, 0.94463668, 0.9550173 , 0.93425606, 0.94117647,\n",
       "        0.95155709, 0.94463668, 0.94809689, 0.9550173 , 0.93425606,\n",
       "        0.94117647, 0.94117647, 0.94463668, 0.94809689, 0.9550173 ,\n",
       "        0.93425606, 0.94117647, 0.94117647, 0.94463668, 0.94809689,\n",
       "        0.9550173 , 0.93425606, 0.94117647, 0.92733564, 0.91695502,\n",
       "        0.92041522, 0.91695502, 0.91695502, 0.90657439, 0.9100346 ,\n",
       "        0.91695502, 0.92733564, 0.91695502, 0.91695502, 0.90657439,\n",
       "        0.9100346 , 0.91695502, 0.92733564, 0.91695502, 0.91695502,\n",
       "        0.90657439, 0.9100346 , 0.91695502, 0.92733564, 0.91695502,\n",
       "        0.91695502, 0.90657439]),\n",
       " 'split2_test_score': array([0.89930556, 0.89930556, 0.89930556, 0.89583333, 0.90625   ,\n",
       "        0.89236111, 0.91319444, 0.89236111, 0.89930556, 0.89583333,\n",
       "        0.90625   , 0.89236111, 0.90625   , 0.89236111, 0.89930556,\n",
       "        0.89583333, 0.90625   , 0.89236111, 0.90625   , 0.89236111,\n",
       "        0.89930556, 0.89583333, 0.90625   , 0.89236111, 0.89236111,\n",
       "        0.88541667, 0.88888889, 0.87152778, 0.89583333, 0.875     ,\n",
       "        0.90972222, 0.88541667, 0.88888889, 0.87152778, 0.89583333,\n",
       "        0.875     , 0.89583333, 0.88541667, 0.88888889, 0.87152778,\n",
       "        0.89583333, 0.875     , 0.89930556, 0.88541667, 0.88888889,\n",
       "        0.87152778, 0.89583333, 0.875     , 0.89583333, 0.89583333,\n",
       "        0.89236111, 0.88888889, 0.88888889, 0.88541667, 0.89930556,\n",
       "        0.89583333, 0.89236111, 0.88888889, 0.88888889, 0.88541667,\n",
       "        0.89930556, 0.89583333, 0.89236111, 0.88888889, 0.88888889,\n",
       "        0.88541667, 0.89930556, 0.89583333, 0.89236111, 0.88888889,\n",
       "        0.88888889, 0.88541667, 0.84722222, 0.84722222, 0.84375   ,\n",
       "        0.85069444, 0.84027778, 0.84722222, 0.83333333, 0.83333333,\n",
       "        0.84375   , 0.85069444, 0.84027778, 0.84722222, 0.82638889,\n",
       "        0.83333333, 0.84375   , 0.85069444, 0.84027778, 0.84722222,\n",
       "        0.82291667, 0.83333333, 0.84375   , 0.85069444, 0.84027778,\n",
       "        0.84722222, 0.89930556, 0.89930556, 0.89930556, 0.89583333,\n",
       "        0.90625   , 0.89236111, 0.91319444, 0.89236111, 0.89930556,\n",
       "        0.89583333, 0.90625   , 0.89236111, 0.90625   , 0.89236111,\n",
       "        0.89930556, 0.89583333, 0.90625   , 0.89236111, 0.90625   ,\n",
       "        0.89236111, 0.89930556, 0.89583333, 0.90625   , 0.89236111,\n",
       "        0.89236111, 0.88541667, 0.88888889, 0.87152778, 0.89583333,\n",
       "        0.875     , 0.90972222, 0.88541667, 0.88888889, 0.87152778,\n",
       "        0.89583333, 0.875     , 0.89583333, 0.88541667, 0.88888889,\n",
       "        0.87152778, 0.89583333, 0.875     , 0.89930556, 0.88541667,\n",
       "        0.88888889, 0.87152778, 0.89583333, 0.875     , 0.88194444,\n",
       "        0.89583333, 0.88194444, 0.89236111, 0.88541667, 0.87847222,\n",
       "        0.88888889, 0.89583333, 0.88541667, 0.89236111, 0.88541667,\n",
       "        0.87847222, 0.88541667, 0.89583333, 0.88541667, 0.89236111,\n",
       "        0.88541667, 0.87847222, 0.88888889, 0.89583333, 0.88541667,\n",
       "        0.89236111, 0.88541667, 0.87847222, 0.86805556, 0.86805556,\n",
       "        0.875     , 0.87152778, 0.86458333, 0.86458333, 0.86805556,\n",
       "        0.875     , 0.87152778, 0.87152778, 0.86458333, 0.86458333,\n",
       "        0.86458333, 0.875     , 0.87152778, 0.87152778, 0.86458333,\n",
       "        0.86458333, 0.875     , 0.875     , 0.87152778, 0.87152778,\n",
       "        0.86458333, 0.86458333]),\n",
       " 'split3_test_score': array([0.90592334, 0.89547038, 0.8989547 , 0.8989547 , 0.89547038,\n",
       "        0.88501742, 0.90592334, 0.88850174, 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.88501742, 0.89547038, 0.88850174, 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.88501742, 0.8989547 , 0.88850174,\n",
       "        0.8989547 , 0.8989547 , 0.89547038, 0.88501742, 0.92334495,\n",
       "        0.91986063, 0.92682927, 0.91637631, 0.91289199, 0.91289199,\n",
       "        0.91986063, 0.91637631, 0.92682927, 0.91637631, 0.91289199,\n",
       "        0.91289199, 0.92334495, 0.91637631, 0.92682927, 0.91637631,\n",
       "        0.91289199, 0.91289199, 0.93031359, 0.91637631, 0.92682927,\n",
       "        0.91637631, 0.91289199, 0.91289199, 0.8989547 , 0.8989547 ,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.89547038, 0.8989547 ,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 , 0.89547038,\n",
       "        0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.8989547 , 0.8989547 , 0.8989547 , 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.87804878, 0.87804878, 0.87804878,\n",
       "        0.88850174, 0.88501742, 0.90592334, 0.87108014, 0.87456446,\n",
       "        0.87804878, 0.88850174, 0.88501742, 0.90592334, 0.86759582,\n",
       "        0.87456446, 0.87804878, 0.88850174, 0.88501742, 0.90592334,\n",
       "        0.8641115 , 0.87456446, 0.87804878, 0.88850174, 0.88501742,\n",
       "        0.90592334, 0.90592334, 0.89547038, 0.8989547 , 0.8989547 ,\n",
       "        0.89547038, 0.88501742, 0.90592334, 0.88850174, 0.8989547 ,\n",
       "        0.8989547 , 0.89547038, 0.88501742, 0.89547038, 0.88850174,\n",
       "        0.8989547 , 0.8989547 , 0.89547038, 0.88501742, 0.8989547 ,\n",
       "        0.88850174, 0.8989547 , 0.8989547 , 0.89547038, 0.88501742,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.91637631, 0.91289199,\n",
       "        0.91289199, 0.91986063, 0.91637631, 0.92682927, 0.91637631,\n",
       "        0.91289199, 0.91289199, 0.92334495, 0.91637631, 0.92682927,\n",
       "        0.91637631, 0.91289199, 0.91289199, 0.93031359, 0.91637631,\n",
       "        0.92682927, 0.91637631, 0.91289199, 0.91289199, 0.92682927,\n",
       "        0.93379791, 0.92682927, 0.93379791, 0.91986063, 0.91986063,\n",
       "        0.92334495, 0.94076655, 0.92682927, 0.93379791, 0.91986063,\n",
       "        0.91986063, 0.91986063, 0.94076655, 0.92682927, 0.93379791,\n",
       "        0.91986063, 0.91986063, 0.91637631, 0.94076655, 0.92682927,\n",
       "        0.93379791, 0.91986063, 0.91986063, 0.90592334, 0.89547038,\n",
       "        0.90592334, 0.8989547 , 0.90592334, 0.89198606, 0.89547038,\n",
       "        0.90243902, 0.90592334, 0.8989547 , 0.90592334, 0.89198606,\n",
       "        0.89547038, 0.90243902, 0.90592334, 0.8989547 , 0.90592334,\n",
       "        0.89198606, 0.88850174, 0.90243902, 0.90592334, 0.8989547 ,\n",
       "        0.90592334, 0.89198606]),\n",
       " 'split4_test_score': array([0.93031359, 0.92682927, 0.93031359, 0.91986063, 0.91637631,\n",
       "        0.91289199, 0.92682927, 0.92682927, 0.93031359, 0.91986063,\n",
       "        0.91637631, 0.91289199, 0.92682927, 0.92682927, 0.93031359,\n",
       "        0.91986063, 0.91637631, 0.91289199, 0.93031359, 0.92682927,\n",
       "        0.93031359, 0.91986063, 0.91637631, 0.91289199, 0.92334495,\n",
       "        0.92334495, 0.92334495, 0.93379791, 0.92682927, 0.92334495,\n",
       "        0.91986063, 0.92682927, 0.92334495, 0.93379791, 0.92682927,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.92334495, 0.93379791,\n",
       "        0.92682927, 0.92334495, 0.91986063, 0.92682927, 0.92334495,\n",
       "        0.93379791, 0.92682927, 0.92334495, 0.90940767, 0.90592334,\n",
       "        0.90940767, 0.91289199, 0.90940767, 0.91289199, 0.90940767,\n",
       "        0.90592334, 0.90940767, 0.91289199, 0.90940767, 0.91289199,\n",
       "        0.91637631, 0.90592334, 0.90940767, 0.91289199, 0.90940767,\n",
       "        0.91289199, 0.91289199, 0.90592334, 0.90940767, 0.91289199,\n",
       "        0.90940767, 0.91289199, 0.86062718, 0.87108014, 0.85714286,\n",
       "        0.8815331 , 0.87108014, 0.88501742, 0.85714286, 0.86759582,\n",
       "        0.85714286, 0.8815331 , 0.87108014, 0.88501742, 0.85365854,\n",
       "        0.86759582, 0.85714286, 0.8815331 , 0.87108014, 0.88501742,\n",
       "        0.86062718, 0.86759582, 0.85714286, 0.8815331 , 0.87108014,\n",
       "        0.88501742, 0.93031359, 0.92682927, 0.93031359, 0.91986063,\n",
       "        0.91637631, 0.91289199, 0.92682927, 0.92682927, 0.93031359,\n",
       "        0.91986063, 0.91637631, 0.91289199, 0.92682927, 0.92682927,\n",
       "        0.93031359, 0.91986063, 0.91637631, 0.91289199, 0.93031359,\n",
       "        0.92682927, 0.93031359, 0.91986063, 0.91637631, 0.91289199,\n",
       "        0.92334495, 0.92334495, 0.92334495, 0.93379791, 0.92682927,\n",
       "        0.92334495, 0.91986063, 0.92682927, 0.92334495, 0.93379791,\n",
       "        0.92682927, 0.92334495, 0.91986063, 0.92682927, 0.92334495,\n",
       "        0.93379791, 0.92682927, 0.92334495, 0.91986063, 0.92682927,\n",
       "        0.92334495, 0.93379791, 0.92682927, 0.92334495, 0.92682927,\n",
       "        0.93031359, 0.92682927, 0.93728223, 0.93031359, 0.93031359,\n",
       "        0.92334495, 0.93728223, 0.92682927, 0.93728223, 0.93031359,\n",
       "        0.93031359, 0.92334495, 0.93728223, 0.92682927, 0.93728223,\n",
       "        0.93031359, 0.93031359, 0.93379791, 0.93728223, 0.92682927,\n",
       "        0.93728223, 0.93031359, 0.93031359, 0.90592334, 0.91289199,\n",
       "        0.90940767, 0.91986063, 0.90243902, 0.90940767, 0.90940767,\n",
       "        0.90592334, 0.90940767, 0.91986063, 0.90243902, 0.90940767,\n",
       "        0.91637631, 0.90592334, 0.90940767, 0.91986063, 0.90243902,\n",
       "        0.90940767, 0.91986063, 0.90592334, 0.90940767, 0.91986063,\n",
       "        0.90243902, 0.90940767]),\n",
       " 'mean_test_score': array([0.91527778, 0.90972222, 0.9125    , 0.90694444, 0.91041667,\n",
       "        0.90486111, 0.91527778, 0.90694444, 0.9125    , 0.90694444,\n",
       "        0.91041667, 0.90486111, 0.91180556, 0.90694444, 0.9125    ,\n",
       "        0.90694444, 0.91041667, 0.90486111, 0.91319444, 0.90694444,\n",
       "        0.9125    , 0.90694444, 0.91041667, 0.90486111, 0.91805556,\n",
       "        0.91319444, 0.91736111, 0.91041667, 0.91875   , 0.91041667,\n",
       "        0.91875   , 0.9125    , 0.91805556, 0.91041667, 0.91875   ,\n",
       "        0.91041667, 0.91736111, 0.9125    , 0.91805556, 0.91041667,\n",
       "        0.91875   , 0.91041667, 0.91875   , 0.9125    , 0.91805556,\n",
       "        0.91041667, 0.91875   , 0.91041667, 0.90277778, 0.90347222,\n",
       "        0.90347222, 0.90486111, 0.90347222, 0.90416667, 0.90416667,\n",
       "        0.90347222, 0.90347222, 0.90486111, 0.90347222, 0.90416667,\n",
       "        0.90555556, 0.90347222, 0.90347222, 0.90486111, 0.90347222,\n",
       "        0.90416667, 0.90486111, 0.90347222, 0.90347222, 0.90486111,\n",
       "        0.90347222, 0.90416667, 0.85902778, 0.86111111, 0.85694444,\n",
       "        0.86666667, 0.86319444, 0.87222222, 0.85347222, 0.85416667,\n",
       "        0.85694444, 0.86666667, 0.86319444, 0.87222222, 0.85      ,\n",
       "        0.85416667, 0.85694444, 0.86666667, 0.86319444, 0.87222222,\n",
       "        0.84861111, 0.85416667, 0.85694444, 0.86666667, 0.86319444,\n",
       "        0.87222222, 0.91527778, 0.90972222, 0.9125    , 0.90694444,\n",
       "        0.91041667, 0.90486111, 0.91527778, 0.90694444, 0.9125    ,\n",
       "        0.90694444, 0.91041667, 0.90486111, 0.91180556, 0.90694444,\n",
       "        0.9125    , 0.90694444, 0.91041667, 0.90486111, 0.91319444,\n",
       "        0.90694444, 0.9125    , 0.90694444, 0.91041667, 0.90486111,\n",
       "        0.91805556, 0.91319444, 0.91736111, 0.91041667, 0.91875   ,\n",
       "        0.91041667, 0.91875   , 0.9125    , 0.91805556, 0.91041667,\n",
       "        0.91875   , 0.91041667, 0.91736111, 0.9125    , 0.91805556,\n",
       "        0.91041667, 0.91875   , 0.91041667, 0.91875   , 0.9125    ,\n",
       "        0.91805556, 0.91041667, 0.91875   , 0.91041667, 0.91875   ,\n",
       "        0.92152778, 0.91875   , 0.92430556, 0.91666667, 0.91527778,\n",
       "        0.91944444, 0.92291667, 0.92013889, 0.92430556, 0.91666667,\n",
       "        0.91527778, 0.91527778, 0.92291667, 0.92013889, 0.92430556,\n",
       "        0.91666667, 0.91527778, 0.91875   , 0.92291667, 0.92013889,\n",
       "        0.92430556, 0.91666667, 0.91527778, 0.89652778, 0.88888889,\n",
       "        0.89791667, 0.89375   , 0.89375   , 0.89097222, 0.89444444,\n",
       "        0.88958333, 0.89861111, 0.89375   , 0.89375   , 0.89097222,\n",
       "        0.89305556, 0.88958333, 0.89861111, 0.89375   , 0.89375   ,\n",
       "        0.89097222, 0.89583333, 0.88958333, 0.89861111, 0.89375   ,\n",
       "        0.89375   , 0.89097222]),\n",
       " 'std_test_score': array([0.01417774, 0.01206012, 0.01272087, 0.01085124, 0.01221176,\n",
       "        0.01498486, 0.00865683, 0.01453135, 0.01272087, 0.01085124,\n",
       "        0.01221176, 0.01498486, 0.01094746, 0.01453135, 0.01272087,\n",
       "        0.01085124, 0.01221176, 0.01498486, 0.01055102, 0.01453135,\n",
       "        0.01272087, 0.01085124, 0.01221176, 0.01498486, 0.0192125 ,\n",
       "        0.01847134, 0.01800879, 0.02291512, 0.01769108, 0.02006046,\n",
       "        0.01089667, 0.01659756, 0.01895558, 0.02291512, 0.01769108,\n",
       "        0.02006046, 0.01543536, 0.01659756, 0.01895558, 0.02291512,\n",
       "        0.01769108, 0.02006046, 0.01429892, 0.01659756, 0.01895558,\n",
       "        0.02291512, 0.01769108, 0.02006046, 0.00492283, 0.00609273,\n",
       "        0.00748105, 0.00954783, 0.01068466, 0.0116398 , 0.00476943,\n",
       "        0.00609273, 0.00748105, 0.00954783, 0.01068466, 0.0116398 ,\n",
       "        0.00671233, 0.00609273, 0.00748105, 0.00954783, 0.01068466,\n",
       "        0.0116398 , 0.00565367, 0.00609273, 0.00748105, 0.00954783,\n",
       "        0.01068466, 0.0116398 , 0.01853637, 0.01588576, 0.01659534,\n",
       "        0.01706641, 0.01571118, 0.02117965, 0.01867493, 0.01850249,\n",
       "        0.01825035, 0.01706641, 0.01571118, 0.02117965, 0.02029648,\n",
       "        0.01850249, 0.01825035, 0.01706641, 0.01571118, 0.02117965,\n",
       "        0.0195451 , 0.01850249, 0.01825035, 0.01706641, 0.01571118,\n",
       "        0.02117965, 0.01417774, 0.01206012, 0.01272087, 0.01085124,\n",
       "        0.01221176, 0.01498486, 0.00865683, 0.01453135, 0.01272087,\n",
       "        0.01085124, 0.01221176, 0.01498486, 0.01094746, 0.01453135,\n",
       "        0.01272087, 0.01085124, 0.01221176, 0.01498486, 0.01055102,\n",
       "        0.01453135, 0.01272087, 0.01085124, 0.01221176, 0.01498486,\n",
       "        0.0192125 , 0.01847134, 0.01800879, 0.02291512, 0.01769108,\n",
       "        0.02006046, 0.01089667, 0.01659756, 0.01895558, 0.02291512,\n",
       "        0.01769108, 0.02006046, 0.01543536, 0.01659756, 0.01895558,\n",
       "        0.02291512, 0.01769108, 0.02006046, 0.01429892, 0.01659756,\n",
       "        0.01895558, 0.02291512, 0.01769108, 0.02006046, 0.02202361,\n",
       "        0.02037476, 0.02090409, 0.0231308 , 0.01727933, 0.02167926,\n",
       "        0.0204334 , 0.02212086, 0.02060827, 0.0231308 , 0.01727933,\n",
       "        0.02167926, 0.01858253, 0.02212086, 0.02060827, 0.0231308 ,\n",
       "        0.01727933, 0.02167926, 0.01819538, 0.02212086, 0.02060827,\n",
       "        0.0231308 , 0.01727933, 0.02167926, 0.021843  , 0.02555907,\n",
       "        0.01784238, 0.02357248, 0.01915962, 0.01646151, 0.0154355 ,\n",
       "        0.0251049 , 0.0205965 , 0.02357248, 0.01915962, 0.01646151,\n",
       "        0.01923028, 0.0251049 , 0.0205965 , 0.02357248, 0.01915962,\n",
       "        0.01646151, 0.01652894, 0.0251049 , 0.0205965 , 0.02357248,\n",
       "        0.01915962, 0.01646151]),\n",
       " 'rank_test_score': array([ 44,  97,  57,  99,  73, 114,  44,  99,  57,  99,  73, 114,  71,\n",
       "         99,  57,  99,  73, 114,  53,  99,  57,  99,  73, 114,  28,  53,\n",
       "         36,  73,  13,  73,  13,  57,  28,  73,  13,  73,  36,  57,  28,\n",
       "         73,  13,  73,  13,  57,  28,  73,  13,  73, 144, 132, 132, 114,\n",
       "        132, 127, 127, 132, 132, 114, 132, 127, 113, 132, 132, 114, 132,\n",
       "        127, 114, 132, 132, 114, 132, 127, 182, 181, 183, 173, 177, 169,\n",
       "        190, 187, 183, 173, 177, 169, 191, 187, 183, 173, 177, 169, 192,\n",
       "        187, 183, 173, 177, 169,  44,  97,  57,  99,  73, 114,  44,  99,\n",
       "         57,  99,  73, 114,  71,  99,  57,  99,  73, 114,  53,  99,  57,\n",
       "         99,  73, 114,  28,  53,  36,  73,  13,  73,  13,  57,  28,  73,\n",
       "         13,  73,  36,  57,  28,  73,  13,  73,  13,  57,  28,  73,  13,\n",
       "         73,  13,   8,  13,   1,  40,  44,  12,   5,   9,   1,  40,  44,\n",
       "         44,   5,   9,   1,  40,  44,  13,   5,   9,   1,  40,  44, 149,\n",
       "        168, 148, 152, 152, 161, 151, 165, 145, 152, 152, 161, 160, 165,\n",
       "        145, 152, 152, 161, 150, 165, 145, 152, 152, 161], dtype=int32),\n",
       " 'split0_train_score': array([0.99218071, 0.99391833, 0.99218071, 0.99218071, 0.99044309,\n",
       "        0.98870547, 0.99304952, 0.99565595, 0.99218071, 0.99218071,\n",
       "        0.99044309, 0.98870547, 0.99913119, 0.99565595, 0.99218071,\n",
       "        0.99218071, 0.99044309, 0.98870547, 0.99913119, 0.99565595,\n",
       "        0.99218071, 0.99218071, 0.99044309, 0.98870547, 0.98436142,\n",
       "        0.98523023, 0.98523023, 0.98349262, 0.98262381, 0.97914857,\n",
       "        0.98696785, 0.98696785, 0.98523023, 0.98349262, 0.98262381,\n",
       "        0.97914857, 0.98957428, 0.98696785, 0.98523023, 0.98349262,\n",
       "        0.98262381, 0.97914857, 0.98957428, 0.98696785, 0.98523023,\n",
       "        0.98349262, 0.98262381, 0.97914857, 0.9643788 , 0.96090356,\n",
       "        0.96350999, 0.96090356, 0.95829713, 0.95916594, 0.9643788 ,\n",
       "        0.96090356, 0.96350999, 0.96090356, 0.95916594, 0.95916594,\n",
       "        0.9643788 , 0.96090356, 0.96350999, 0.96090356, 0.95829713,\n",
       "        0.95916594, 0.9643788 , 0.96090356, 0.96350999, 0.96090356,\n",
       "        0.95916594, 0.95916594, 0.90356212, 0.90616855, 0.90616855,\n",
       "        0.9105126 , 0.91311903, 0.91311903, 0.89661164, 0.90529974,\n",
       "        0.90529974, 0.9105126 , 0.91311903, 0.91311903, 0.88879235,\n",
       "        0.90529974, 0.90529974, 0.9105126 , 0.91311903, 0.91311903,\n",
       "        0.88531712, 0.90529974, 0.90529974, 0.9105126 , 0.91311903,\n",
       "        0.91311903, 0.99218071, 0.99391833, 0.99218071, 0.99218071,\n",
       "        0.99044309, 0.98870547, 0.99304952, 0.99565595, 0.99218071,\n",
       "        0.99218071, 0.99044309, 0.98870547, 0.99913119, 0.99565595,\n",
       "        0.99218071, 0.99218071, 0.99044309, 0.98870547, 0.99913119,\n",
       "        0.99565595, 0.99218071, 0.99218071, 0.99044309, 0.98870547,\n",
       "        0.98436142, 0.98523023, 0.98523023, 0.98349262, 0.98262381,\n",
       "        0.97914857, 0.98696785, 0.98696785, 0.98523023, 0.98349262,\n",
       "        0.98262381, 0.97914857, 0.98957428, 0.98696785, 0.98523023,\n",
       "        0.98349262, 0.98262381, 0.97914857, 0.98957428, 0.98696785,\n",
       "        0.98523023, 0.98349262, 0.98262381, 0.97914857, 0.97219809,\n",
       "        0.97567333, 0.9730669 , 0.97132928, 0.96090356, 0.96524761,\n",
       "        0.97741095, 0.98001738, 0.9730669 , 0.97132928, 0.96090356,\n",
       "        0.96524761, 0.98001738, 0.98001738, 0.9730669 , 0.97132928,\n",
       "        0.96090356, 0.96524761, 0.98262381, 0.98001738, 0.9730669 ,\n",
       "        0.97132928, 0.96090356, 0.96524761, 0.96785404, 0.9730669 ,\n",
       "        0.96872285, 0.96872285, 0.96177237, 0.95829713, 0.97393571,\n",
       "        0.97741095, 0.96785404, 0.96872285, 0.96177237, 0.95829713,\n",
       "        0.98001738, 0.97741095, 0.96785404, 0.96872285, 0.96177237,\n",
       "        0.95829713, 0.98088619, 0.97741095, 0.96785404, 0.96872285,\n",
       "        0.96177237, 0.95829713]),\n",
       " 'split1_train_score': array([0.99391833, 0.99478714, 0.99478714, 0.99391833, 0.99044309,\n",
       "        0.98957428, 0.99565595, 0.99478714, 0.99478714, 0.99391833,\n",
       "        0.99044309, 0.98957428, 0.99565595, 0.99478714, 0.99478714,\n",
       "        0.99391833, 0.99044309, 0.98957428, 0.99565595, 0.99478714,\n",
       "        0.99478714, 0.99391833, 0.99044309, 0.98957428, 0.98436142,\n",
       "        0.98609904, 0.98609904, 0.98001738, 0.9730669 , 0.96872285,\n",
       "        0.98957428, 0.98783666, 0.98523023, 0.98001738, 0.9730669 ,\n",
       "        0.96872285, 0.98957428, 0.98783666, 0.98523023, 0.98001738,\n",
       "        0.9730669 , 0.96872285, 0.9913119 , 0.98783666, 0.98523023,\n",
       "        0.98001738, 0.9730669 , 0.96872285, 0.96698523, 0.96959166,\n",
       "        0.96611642, 0.96872285, 0.9643788 , 0.96524761, 0.96785404,\n",
       "        0.96959166, 0.96698523, 0.96872285, 0.9643788 , 0.96524761,\n",
       "        0.96785404, 0.96959166, 0.96698523, 0.96872285, 0.9643788 ,\n",
       "        0.96524761, 0.96785404, 0.96959166, 0.96698523, 0.96872285,\n",
       "        0.9643788 , 0.96524761, 0.87054735, 0.87141616, 0.87141616,\n",
       "        0.87749783, 0.88705474, 0.89487402, 0.86707211, 0.86794092,\n",
       "        0.87054735, 0.87749783, 0.88705474, 0.89487402, 0.86272806,\n",
       "        0.86794092, 0.87054735, 0.87749783, 0.88705474, 0.89487402,\n",
       "        0.85664639, 0.86794092, 0.87054735, 0.87749783, 0.88705474,\n",
       "        0.89487402, 0.99391833, 0.99478714, 0.99478714, 0.99391833,\n",
       "        0.99044309, 0.98957428, 0.99565595, 0.99478714, 0.99478714,\n",
       "        0.99391833, 0.99044309, 0.98957428, 0.99565595, 0.99478714,\n",
       "        0.99478714, 0.99391833, 0.99044309, 0.98957428, 0.99565595,\n",
       "        0.99478714, 0.99478714, 0.99391833, 0.99044309, 0.98957428,\n",
       "        0.98436142, 0.98609904, 0.98609904, 0.98001738, 0.9730669 ,\n",
       "        0.96872285, 0.98957428, 0.98783666, 0.98523023, 0.98001738,\n",
       "        0.9730669 , 0.96872285, 0.98957428, 0.98783666, 0.98523023,\n",
       "        0.98001738, 0.9730669 , 0.96872285, 0.9913119 , 0.98783666,\n",
       "        0.98523023, 0.98001738, 0.9730669 , 0.96872285, 0.96264118,\n",
       "        0.96524761, 0.96524761, 0.96177237, 0.95829713, 0.95308427,\n",
       "        0.9730669 , 0.96872285, 0.96524761, 0.96177237, 0.95829713,\n",
       "        0.95308427, 0.97393571, 0.96872285, 0.96524761, 0.96177237,\n",
       "        0.95829713, 0.95308427, 0.97480452, 0.96872285, 0.96524761,\n",
       "        0.96177237, 0.95829713, 0.95308427, 0.96524761, 0.96959166,\n",
       "        0.96785404, 0.96177237, 0.95482189, 0.95134666, 0.97393571,\n",
       "        0.97132928, 0.96785404, 0.96177237, 0.95482189, 0.95134666,\n",
       "        0.97654214, 0.97132928, 0.96785404, 0.96177237, 0.95482189,\n",
       "        0.95134666, 0.97654214, 0.97132928, 0.96785404, 0.96177237,\n",
       "        0.95482189, 0.95134666]),\n",
       " 'split2_train_score': array([0.99826389, 0.99826389, 0.99826389, 0.99479167, 0.99479167,\n",
       "        0.9921875 , 0.99913194, 0.99913194, 0.99739583, 0.99479167,\n",
       "        0.99479167, 0.9921875 , 0.99913194, 0.99913194, 0.99739583,\n",
       "        0.99479167, 0.99479167, 0.9921875 , 0.99913194, 0.99913194,\n",
       "        0.99739583, 0.99479167, 0.99479167, 0.9921875 , 0.98784722,\n",
       "        0.98871528, 0.98784722, 0.98697917, 0.98350694, 0.98090278,\n",
       "        0.98784722, 0.98871528, 0.98784722, 0.98697917, 0.98350694,\n",
       "        0.98090278, 0.98871528, 0.98871528, 0.98784722, 0.98697917,\n",
       "        0.98350694, 0.98090278, 0.99131944, 0.98871528, 0.98784722,\n",
       "        0.98697917, 0.98350694, 0.98090278, 0.97482639, 0.97569444,\n",
       "        0.97309028, 0.97395833, 0.97048611, 0.97048611, 0.97569444,\n",
       "        0.97569444, 0.97309028, 0.97395833, 0.97048611, 0.97048611,\n",
       "        0.97569444, 0.97569444, 0.97309028, 0.97395833, 0.97048611,\n",
       "        0.97048611, 0.97569444, 0.97569444, 0.97309028, 0.97395833,\n",
       "        0.97048611, 0.97048611, 0.90190972, 0.90190972, 0.90017361,\n",
       "        0.90798611, 0.90625   , 0.91753472, 0.89496528, 0.8984375 ,\n",
       "        0.90017361, 0.90798611, 0.90711806, 0.91753472, 0.88628472,\n",
       "        0.8984375 , 0.90017361, 0.90798611, 0.90711806, 0.91753472,\n",
       "        0.87760417, 0.8984375 , 0.90017361, 0.90798611, 0.90711806,\n",
       "        0.91753472, 0.99826389, 0.99826389, 0.99826389, 0.99479167,\n",
       "        0.99479167, 0.9921875 , 0.99913194, 0.99913194, 0.99739583,\n",
       "        0.99479167, 0.99479167, 0.9921875 , 0.99913194, 0.99913194,\n",
       "        0.99739583, 0.99479167, 0.99479167, 0.9921875 , 0.99913194,\n",
       "        0.99913194, 0.99739583, 0.99479167, 0.99479167, 0.9921875 ,\n",
       "        0.98784722, 0.98871528, 0.98784722, 0.98697917, 0.98350694,\n",
       "        0.98090278, 0.98784722, 0.98871528, 0.98784722, 0.98697917,\n",
       "        0.98350694, 0.98090278, 0.98871528, 0.98871528, 0.98784722,\n",
       "        0.98697917, 0.98350694, 0.98090278, 0.99131944, 0.98871528,\n",
       "        0.98784722, 0.98697917, 0.98350694, 0.98090278, 0.97309028,\n",
       "        0.97743056, 0.97309028, 0.97222222, 0.96527778, 0.96440972,\n",
       "        0.98003472, 0.98003472, 0.97309028, 0.97222222, 0.96527778,\n",
       "        0.96440972, 0.98090278, 0.98003472, 0.97309028, 0.97222222,\n",
       "        0.96527778, 0.96440972, 0.98090278, 0.98003472, 0.97309028,\n",
       "        0.97222222, 0.96527778, 0.96440972, 0.97482639, 0.96961806,\n",
       "        0.97309028, 0.96961806, 0.96267361, 0.95572917, 0.97395833,\n",
       "        0.97482639, 0.97309028, 0.96961806, 0.96267361, 0.95572917,\n",
       "        0.97569444, 0.97482639, 0.97309028, 0.96961806, 0.96267361,\n",
       "        0.95572917, 0.97743056, 0.97482639, 0.97309028, 0.96961806,\n",
       "        0.96267361, 0.95572917]),\n",
       " 'split3_train_score': array([0.9991327 , 0.99826539, 0.9991327 , 0.99826539, 0.99739809,\n",
       "        0.99566349, 0.9991327 , 0.99826539, 0.9991327 , 0.99826539,\n",
       "        0.99739809, 0.99566349, 0.9991327 , 0.99826539, 0.9991327 ,\n",
       "        0.99826539, 0.99739809, 0.99566349, 0.9991327 , 0.99826539,\n",
       "        0.9991327 , 0.99826539, 0.99739809, 0.99566349, 0.98785776,\n",
       "        0.98785776, 0.98785776, 0.98525585, 0.98265395, 0.98005204,\n",
       "        0.99045967, 0.98785776, 0.98785776, 0.98525585, 0.98265395,\n",
       "        0.98005204, 0.98959237, 0.98785776, 0.98785776, 0.98525585,\n",
       "        0.98265395, 0.98005204, 0.99045967, 0.98785776, 0.98785776,\n",
       "        0.98525585, 0.98265395, 0.98005204, 0.97137901, 0.97398092,\n",
       "        0.97137901, 0.97398092, 0.97051171, 0.97311362, 0.97137901,\n",
       "        0.97398092, 0.97137901, 0.97398092, 0.97051171, 0.97311362,\n",
       "        0.97137901, 0.97398092, 0.97137901, 0.97398092, 0.97051171,\n",
       "        0.97311362, 0.97137901, 0.97398092, 0.97137901, 0.97398092,\n",
       "        0.97051171, 0.97311362, 0.88551605, 0.88985256, 0.88551605,\n",
       "        0.89418907, 0.89852559, 0.90633131, 0.87944493, 0.88291414,\n",
       "        0.88551605, 0.89418907, 0.89852559, 0.90633131, 0.87857762,\n",
       "        0.88291414, 0.88551605, 0.89418907, 0.89852559, 0.90633131,\n",
       "        0.87597572, 0.88291414, 0.88551605, 0.89418907, 0.89852559,\n",
       "        0.90633131, 0.9991327 , 0.99826539, 0.9991327 , 0.99826539,\n",
       "        0.99739809, 0.99566349, 0.9991327 , 0.99826539, 0.9991327 ,\n",
       "        0.99826539, 0.99739809, 0.99566349, 0.9991327 , 0.99826539,\n",
       "        0.9991327 , 0.99826539, 0.99739809, 0.99566349, 0.9991327 ,\n",
       "        0.99826539, 0.9991327 , 0.99826539, 0.99739809, 0.99566349,\n",
       "        0.98785776, 0.98785776, 0.98785776, 0.98525585, 0.98265395,\n",
       "        0.98005204, 0.99045967, 0.98785776, 0.98785776, 0.98525585,\n",
       "        0.98265395, 0.98005204, 0.98959237, 0.98785776, 0.98785776,\n",
       "        0.98525585, 0.98265395, 0.98005204, 0.99045967, 0.98785776,\n",
       "        0.98785776, 0.98525585, 0.98265395, 0.98005204, 0.9679098 ,\n",
       "        0.97137901, 0.9679098 , 0.9670425 , 0.96010408, 0.96097138,\n",
       "        0.97571552, 0.97311362, 0.9679098 , 0.9670425 , 0.96010408,\n",
       "        0.96097138, 0.97745013, 0.97311362, 0.9679098 , 0.9670425 ,\n",
       "        0.96010408, 0.96097138, 0.97745013, 0.97311362, 0.9679098 ,\n",
       "        0.9670425 , 0.96010408, 0.96097138, 0.96097138, 0.9661752 ,\n",
       "        0.96097138, 0.95836947, 0.95403296, 0.95229835, 0.96964441,\n",
       "        0.9687771 , 0.96097138, 0.95836947, 0.95403296, 0.95229835,\n",
       "        0.97745013, 0.9687771 , 0.96097138, 0.95836947, 0.95403296,\n",
       "        0.95229835, 0.98005204, 0.9687771 , 0.96097138, 0.95836947,\n",
       "        0.95403296, 0.95229835]),\n",
       " 'split4_train_score': array([0.99739809, 0.9991327 , 0.99653079, 0.99392888, 0.99045967,\n",
       "        0.98959237, 0.99739809, 0.9991327 , 0.99653079, 0.99392888,\n",
       "        0.99045967, 0.98959237, 0.9991327 , 0.9991327 , 0.99653079,\n",
       "        0.99392888, 0.99045967, 0.98959237, 0.9991327 , 0.9991327 ,\n",
       "        0.99653079, 0.99392888, 0.99045967, 0.98959237, 0.98352125,\n",
       "        0.98612316, 0.98438855, 0.98005204, 0.97484822, 0.97224631,\n",
       "        0.98612316, 0.98872507, 0.98438855, 0.98005204, 0.97484822,\n",
       "        0.97224631, 0.98785776, 0.98872507, 0.98438855, 0.98005204,\n",
       "        0.97484822, 0.97224631, 0.98785776, 0.98872507, 0.98438855,\n",
       "        0.98005204, 0.97484822, 0.97224631, 0.96097138, 0.96270598,\n",
       "        0.96010408, 0.96097138, 0.95923677, 0.95923677, 0.96097138,\n",
       "        0.96270598, 0.96010408, 0.96097138, 0.95923677, 0.95923677,\n",
       "        0.9679098 , 0.96270598, 0.96010408, 0.96097138, 0.95923677,\n",
       "        0.95923677, 0.96964441, 0.96270598, 0.96010408, 0.96097138,\n",
       "        0.95923677, 0.95923677, 0.9045967 , 0.90719861, 0.9037294 ,\n",
       "        0.91066782, 0.90893322, 0.91500434, 0.89679098, 0.9037294 ,\n",
       "        0.9037294 , 0.91066782, 0.90893322, 0.91500434, 0.89071986,\n",
       "        0.9037294 , 0.9037294 , 0.91066782, 0.90980052, 0.91500434,\n",
       "        0.88725065, 0.9037294 , 0.9037294 , 0.91066782, 0.90893322,\n",
       "        0.91500434, 0.99739809, 0.9991327 , 0.99653079, 0.99392888,\n",
       "        0.99045967, 0.98959237, 0.99739809, 0.9991327 , 0.99653079,\n",
       "        0.99392888, 0.99045967, 0.98959237, 0.9991327 , 0.9991327 ,\n",
       "        0.99653079, 0.99392888, 0.99045967, 0.98959237, 0.9991327 ,\n",
       "        0.9991327 , 0.99653079, 0.99392888, 0.99045967, 0.98959237,\n",
       "        0.98352125, 0.98612316, 0.98438855, 0.98005204, 0.97484822,\n",
       "        0.97224631, 0.98612316, 0.98872507, 0.98438855, 0.98005204,\n",
       "        0.97484822, 0.97224631, 0.98785776, 0.98872507, 0.98438855,\n",
       "        0.98005204, 0.97484822, 0.97224631, 0.98785776, 0.98872507,\n",
       "        0.98438855, 0.98005204, 0.97484822, 0.97224631, 0.97051171,\n",
       "        0.97745013, 0.97051171, 0.9661752 , 0.95403296, 0.95056375,\n",
       "        0.97571552, 0.98005204, 0.97051171, 0.9661752 , 0.95403296,\n",
       "        0.95056375, 0.98178664, 0.98005204, 0.97051171, 0.9661752 ,\n",
       "        0.95403296, 0.95056375, 0.98265395, 0.98005204, 0.97051171,\n",
       "        0.9661752 , 0.95403296, 0.95056375, 0.9670425 , 0.9670425 ,\n",
       "        0.9661752 , 0.95663487, 0.94622723, 0.94102342, 0.97051171,\n",
       "        0.9679098 , 0.9661752 , 0.95663487, 0.94622723, 0.94102342,\n",
       "        0.97658283, 0.9679098 , 0.9661752 , 0.95663487, 0.94622723,\n",
       "        0.94102342, 0.97831743, 0.9679098 , 0.9661752 , 0.95663487,\n",
       "        0.94622723, 0.94102342]),\n",
       " 'mean_train_score': array([0.99617874, 0.99687349, 0.99617905, 0.994617  , 0.99270712,\n",
       "        0.99114462, 0.99687364, 0.99739463, 0.99600543, 0.994617  ,\n",
       "        0.99270712, 0.99114462, 0.9984369 , 0.99739463, 0.99600543,\n",
       "        0.994617  , 0.99270712, 0.99114462, 0.9984369 , 0.99739463,\n",
       "        0.99600543, 0.994617  , 0.99270712, 0.99114462, 0.98558982,\n",
       "        0.9868051 , 0.98628456, 0.98315941, 0.97933996, 0.97621451,\n",
       "        0.98819444, 0.98802052, 0.9861108 , 0.98315941, 0.97933996,\n",
       "        0.97621451, 0.98906279, 0.98802052, 0.9861108 , 0.98315941,\n",
       "        0.97933996, 0.97621451, 0.99010461, 0.98802052, 0.9861108 ,\n",
       "        0.98315941, 0.97933996, 0.97621451, 0.96770816, 0.96857531,\n",
       "        0.96683996, 0.96770741, 0.96458211, 0.96545001, 0.96805554,\n",
       "        0.96857531, 0.96701372, 0.96770741, 0.96475587, 0.96545001,\n",
       "        0.96944322, 0.96857531, 0.96701372, 0.96770741, 0.96458211,\n",
       "        0.96545001, 0.96979014, 0.96857531, 0.96701372, 0.96770741,\n",
       "        0.96475587, 0.96545001, 0.89322639, 0.89530912, 0.89340075,\n",
       "        0.90017069, 0.90277651, 0.90937268, 0.88697699, 0.89166434,\n",
       "        0.89305323, 0.90017069, 0.90295012, 0.90937268, 0.88142052,\n",
       "        0.89166434, 0.89305323, 0.90017069, 0.90312358, 0.90937268,\n",
       "        0.87655881, 0.89166434, 0.89305323, 0.90017069, 0.90295012,\n",
       "        0.90937268, 0.99617874, 0.99687349, 0.99617905, 0.994617  ,\n",
       "        0.99270712, 0.99114462, 0.99687364, 0.99739463, 0.99600543,\n",
       "        0.994617  , 0.99270712, 0.99114462, 0.9984369 , 0.99739463,\n",
       "        0.99600543, 0.994617  , 0.99270712, 0.99114462, 0.9984369 ,\n",
       "        0.99739463, 0.99600543, 0.994617  , 0.99270712, 0.99114462,\n",
       "        0.98558982, 0.9868051 , 0.98628456, 0.98315941, 0.97933996,\n",
       "        0.97621451, 0.98819444, 0.98802052, 0.9861108 , 0.98315941,\n",
       "        0.97933996, 0.97621451, 0.98906279, 0.98802052, 0.9861108 ,\n",
       "        0.98315941, 0.97933996, 0.97621451, 0.99010461, 0.98802052,\n",
       "        0.9861108 , 0.98315941, 0.97933996, 0.97621451, 0.96927021,\n",
       "        0.97343613, 0.96996526, 0.96770831, 0.9597231 , 0.95885535,\n",
       "        0.97638872, 0.97638812, 0.96996526, 0.96770831, 0.9597231 ,\n",
       "        0.95885535, 0.97881853, 0.97638812, 0.96996526, 0.96770831,\n",
       "        0.9597231 , 0.95885535, 0.97968704, 0.97638812, 0.96996526,\n",
       "        0.96770831, 0.9597231 , 0.95885535, 0.96718838, 0.96909886,\n",
       "        0.96736275, 0.96302352, 0.95590561, 0.95173894, 0.97239717,\n",
       "        0.9720507 , 0.96718899, 0.96302352, 0.95590561, 0.95173894,\n",
       "        0.97725738, 0.9720507 , 0.96718899, 0.96302352, 0.95590561,\n",
       "        0.95173894, 0.97864567, 0.9720507 , 0.96718899, 0.96302352,\n",
       "        0.95590561, 0.95173894]),\n",
       " 'std_train_score': array([0.00267036, 0.0021005 , 0.00249473, 0.00201189, 0.00288629,\n",
       "        0.00254294, 0.00230586, 0.00182315, 0.00237073, 0.00201189,\n",
       "        0.00288629, 0.00254294, 0.00139047, 0.00182315, 0.00237073,\n",
       "        0.00201189, 0.00288629, 0.00254294, 0.00139047, 0.00182315,\n",
       "        0.00237073, 0.00201189, 0.00288629, 0.00254294, 0.00187277,\n",
       "        0.00128067, 0.0013898 , 0.00277938, 0.004442  , 0.00484121,\n",
       "        0.00160938, 0.00065536, 0.00145492, 0.00277938, 0.004442  ,\n",
       "        0.00484121, 0.00068943, 0.00065536, 0.00145492, 0.00277938,\n",
       "        0.004442  , 0.00484121, 0.0012953 , 0.00065536, 0.00145492,\n",
       "        0.00277938, 0.004442  , 0.00484121, 0.00492211, 0.00590323,\n",
       "        0.00483019, 0.00585027, 0.00525609, 0.0056961 , 0.00516028,\n",
       "        0.00590323, 0.00481665, 0.00585027, 0.00505599, 0.0056961 ,\n",
       "        0.00383015, 0.00590323, 0.00481665, 0.00585027, 0.00525609,\n",
       "        0.0056961 , 0.00375334, 0.00590323, 0.00481665, 0.00585027,\n",
       "        0.00505599, 0.0056961 , 0.01330639, 0.01344249, 0.01312296,\n",
       "        0.01286906, 0.00918893, 0.00814654, 0.01188183, 0.01425996,\n",
       "        0.01325127, 0.01286906, 0.00926084, 0.00814654, 0.01021736,\n",
       "        0.01425996, 0.01325127, 0.01286906, 0.00937865, 0.00814654,\n",
       "        0.01085321, 0.01425996, 0.01325127, 0.01286906, 0.00926084,\n",
       "        0.00814654, 0.00267036, 0.0021005 , 0.00249473, 0.00201189,\n",
       "        0.00288629, 0.00254294, 0.00230586, 0.00182315, 0.00237073,\n",
       "        0.00201189, 0.00288629, 0.00254294, 0.00139047, 0.00182315,\n",
       "        0.00237073, 0.00201189, 0.00288629, 0.00254294, 0.00139047,\n",
       "        0.00182315, 0.00237073, 0.00201189, 0.00288629, 0.00254294,\n",
       "        0.00187277, 0.00128067, 0.0013898 , 0.00277938, 0.004442  ,\n",
       "        0.00484121, 0.00160938, 0.00065536, 0.00145492, 0.00277938,\n",
       "        0.004442  , 0.00484121, 0.00068943, 0.00065536, 0.00145492,\n",
       "        0.00277938, 0.004442  , 0.00484121, 0.0012953 , 0.00065536,\n",
       "        0.00145492, 0.00277938, 0.004442  , 0.00484121, 0.00375504,\n",
       "        0.00465524, 0.00303861, 0.00378196, 0.00365525, 0.00597069,\n",
       "        0.00229256, 0.00467701, 0.00303861, 0.00378196, 0.00365525,\n",
       "        0.00597069, 0.00283921, 0.00467701, 0.00303861, 0.00378196,\n",
       "        0.00365525, 0.00597069, 0.00309024, 0.00467701, 0.00303861,\n",
       "        0.00378196, 0.00365525, 0.00597069, 0.00449936, 0.00240974,\n",
       "        0.00392825, 0.0052917 , 0.00597624, 0.0059029 , 0.00191333,\n",
       "        0.00360057, 0.0038832 , 0.0052917 , 0.00597624, 0.0059029 ,\n",
       "        0.00148755, 0.00360057, 0.0038832 , 0.0052917 , 0.00597624,\n",
       "        0.0059029 , 0.00161288, 0.00360057, 0.0038832 , 0.0052917 ,\n",
       "        0.00597624, 0.0059029 ])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.cv_results_['param_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote = VotingClassifier([\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('grad', GradientBoostingClassifier()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vote', vote)\n",
    "])\n",
    "\n",
    "pipe_params = {\n",
    "    'vote__tree__max_depth' : [None, 1, 2],\n",
    "    'vote__ada__n_estimators' : [40, 50, 60],\n",
    "    'vote__grad__n_estimators' : [90, 100],\n",
    "    'vote__logreg__penalty' : ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_) # cross val accuracy score\n",
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
