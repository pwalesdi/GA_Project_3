{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3 - Webscrapping: Modeling Notebook\n",
    "\n",
    "_Authors: Patrick Wales-Dinan_\n",
    "\n",
    "---\n",
    "\n",
    "This lab was incredibly challenging. We had to extensively clean a date set that was missing a lot of values and had TONS of categorical data. Then we had to decide what features to use to model that data. After that we had to build and fit the models making decisions like whether to use polynomial features, dummy variables etc, log scaling features or log scaling the depended variable.\n",
    "\n",
    "After that we had to re run our model over and over again, looking at the different values of $\\beta$ and seeing if they were contributing to the predictive power of the model. We had to decide if we should throw those values out or if we should leave them. We also had to make judgement calls to see if our model appeared to be over fitting or suffering from bias. \n",
    "\n",
    "## Contents:\n",
    "- [Data Import](#Data-Import)\n",
    "- [Baseline Accuracy](#Calculate-the-Baseline-Accuracy)\n",
    "- [Data Modeling](#Model-Our-Data)\n",
    "- [Data Exploration](#Data-Exploration)\n",
    "- [Cleaning the Data and Modifying the Data](#Cleaning-&-Creating-the-Data-Set)\n",
    "- [Modeling the Data](#Modeling-the-Data)\n",
    "- [Model Analysis](#Analyzing-the-model)\n",
    "\n",
    "Please visit the Graphs & Relationships notebook for additional visuals: Notebook - [Here](/Users/pwalesdi/Desktop/GA/GA_Project_2/Project_2_Graphs_&_Relationships.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from os import path\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data from the reddit csv\n",
    "df_reddit = pd.read_csv('./reddit.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.511458\n",
       "1    0.488542\n",
       "Name: is_ca, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting our baseline accuracy :: So 0.51 is the baseline accuracy for 0 (TX subreddit) and 0.48 for 1 (CA subreddit)\n",
    "df_reddit['is_ca'] = df_reddit['ca']\n",
    "df_reddit['is_ca'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our X & y variables\n",
    "X = df_reddit['title']\n",
    "y = df_reddit['is_ca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deciding which words to remove via stop words\n",
    "stop_words = ['to', 'the', 'in', 'of', 'for', 'and', 'on', 'is', 'it', \n",
    "              'with', 'what', 'about', 'are', 'as', 'from', 'at', 'will', \n",
    "              'that', 'says', 'by', 'be', 'this', 'can', 'has', 'how', \n",
    "#               'california', 'texas'\n",
    "             ]\n",
    "# Setting up our hyperparameters to pass through our pipeline\n",
    "pipe_params = {\n",
    "    'vec' : [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__max_features': [1500, 1600, 1700],\n",
    "    'vec__min_df': [2, 3],\n",
    "    'vec__max_df': [0.4, 0.5],\n",
    "    'vec__ngram_range': [(1,2), (1,1)],\n",
    "    'model' : [LogisticRegression(), \n",
    "               LogisticRegression(penalty='l1', solver='liblinear'), \n",
    "               LogisticRegression(penalty='l2', solver='liblinear'), \n",
    "               MultinomialNB(alpha=1.1),\n",
    "               RandomForestClassifier(n_estimators=1500),\n",
    "               DecisionTreeClassifier()],\n",
    "    'vec__stop_words': [frozenset(stop_words)]\n",
    "}\n",
    "\n",
    "# Defining a function to do our model analysis. This function takes in X, y, and any pipe parameters\n",
    "def model_analysis(X, y, **pipe_params):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "    pipe = Pipeline([\n",
    "            ('vec', CountVectorizer()),\n",
    "            ('model', LogisticRegression())])\n",
    "\n",
    "    gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3, verbose=1, n_jobs=2)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(f' Best Parameters: {gs.best_params_}')\n",
    "    print('')\n",
    "    print(f' Cross Validation Accuracy Score: {gs.best_score_}')\n",
    "    print(f' Training Data Accuracy Score: {gs.score(X_train, y_train)}')\n",
    "    print(f' Testing Data Accuracy Score: {gs.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  65 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=2)]: Done 365 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=2)]: Done 724 tasks      | elapsed:  7.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Parameters: {'model': MultinomialNB(alpha=1.1, class_prior=None, fit_prior=True), 'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.4, max_features=1500, min_df=3,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=frozenset({'of', 'be', 'this', 'are', 'has', 'is', 'at', 'the', 'will', 'that', 'says', 'what', 'and', 'it', 'to', 'can', 'by', 'about', 'in', 'from', 'as', 'with', 'on', 'how', 'for'}),\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vec__max_df': 0.4, 'vec__max_features': 1500, 'vec__min_df': 3, 'vec__ngram_range': (1, 1), 'vec__stop_words': frozenset({'of', 'be', 'this', 'are', 'has', 'is', 'at', 'the', 'will', 'that', 'says', 'what', 'and', 'it', 'to', 'can', 'by', 'about', 'in', 'from', 'as', 'with', 'on', 'how', 'for'})}\n",
      "\n",
      " Cross Validation Accuracy Score: 0.9208333333333333\n",
      " Training Data Accuracy Score: 0.9652777777777778\n",
      " Testing Data Accuracy Score: 0.91875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 864 out of 864 | elapsed:  7.4min finished\n"
     ]
    }
   ],
   "source": [
    "model_analysis(X, y, **pipe_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### What is driving our model and what features are important\n",
    "\n",
    "Here I will use a countvectorizer and a logistic regression model to further examine our ß values for each coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['title']\n",
    "y = df_reddit['is_ca']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=79)\n",
    "\n",
    "stop_words = ['to', 'the', 'in', 'of', 'for', 'and', 'on', 'is', 'it', 'with', 'what', 'about', 'are', 'as', 'from', 'at', 'will', 'that', 'says', 'by', 'be', 'this', 'can', 'has', 'how', 'up', 'not', 'but', 'they']\n",
    "vectorizer = CountVectorizer(tokenizer = None,\n",
    "                            preprocessor = None,\n",
    "                            stop_words = frozenset(stop_words),\n",
    "                            max_features = 1500,\n",
    "                            ngram_range= (1,2),\n",
    "                            analyzer = 'word', \n",
    "                            min_df=3) \n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names())\n",
    "y_train_df = pd.DataFrame(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are our most common features in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "color = cm.plasma_r(np.linspace(.9,.05, 30))\n",
    "plt.figure(figsize=(15,10))\n",
    "X_train_df.sum().sort_values(ascending=False).head(30).plot.barh(color=color)\n",
    "plt.yticks(rotation=15)\n",
    "plt.title(\"The 30 Most Common Words in the Training Set\", fontsize=22)\n",
    "plt.xlabel(\"Number of Occurrences\", fontsize=15)\n",
    "plt.ylabel(\"Most Common Words\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are our most common features in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "color2 = cm.Spectral_r(np.linspace(.9,.1, 30))\n",
    "plt.figure(figsize=(15,10))\n",
    "X_test_df.sum().sort_values(ascending=False).head(30).plot.barh(color=color2)\n",
    "plt.yticks(rotation=15)\n",
    "plt.title(\"The 30 Most Common Words in the Testing Set\", fontsize=22)\n",
    "plt.xlabel(\"Number of Occurrences\", fontsize=15)\n",
    "plt.ylabel(\"Most Common Words\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_df = y_train_df.reset_index()\n",
    "# corr = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "# corr.corr()[['is_ca']].sort_values('is_ca', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_map = corr[['is_ca', 'newsom', 'gavin', 'ca', 'housing', 'los angeles', 'prop', 'proposition', 'wildfires', 'border', 'migrant', 'beto', 'abbott']].corr()\n",
    "# # [['is_ca']].sort_values('is_ca', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (40, 35))\n",
    "# mask = np.zeros_like(h_map)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# sns.set(font_scale = 2)\n",
    "# ax = sns.heatmap(h_map, mask=mask, annot=True, cmap='Spectral', vmax=0.3, vmin=-0.3,\n",
    "#             square=False, linewidths=1.5,  cbar_kws={\"shrink\": 1.0}, xticklabels='auto')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.yticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_df, y_train)\n",
    "print(lr.score(X_train_df, y_train))\n",
    "print(lr.score(X_test_df, y_test))\n",
    "print(f'Intercept: {lr.intercept_}')\n",
    "print('')\n",
    "print(f'Coefficient: {lr.coef_}')\n",
    "print('')\n",
    "print(f'Exponentiated Coefficient: {np.exp(lr.coef_)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Logreg predicted values: {lr.predict(X_train_df.head())}')\n",
    "print(f'Logreg predicted probabilities: {lr.predict_proba(X_train_df.head())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lr.predict(X_test_df)\n",
    "confusion_matrix(y_test, # True values.\n",
    "                 preds)  # Predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = tn / (tn + fp)\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'variables':X_train_df.columns})\n",
    "coe = pd.DataFrame({'ß - Beta':np.squeeze(lr.coef_)})\n",
    "coef_df = pd.concat([coef_df, coe], axis=1)\n",
    "values = pd.DataFrame(X_train_df, index=list(range(0,2049)), columns=coef_df['variables'])\n",
    "values['CA_Post'] = y\n",
    "betas = coef_df.set_index('variables').sort_values(by='ß - Beta', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_betas = coef_df.set_index('variables').sort_values(by='ß - Beta', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color3 = cm.twilight_shifted_r(np.linspace(.9,.1, 20))\n",
    "plt.figure(figsize = (15,9))\n",
    "plt.bar(low_betas.index, # False Positive Rate on X-axis\n",
    "         low_betas['ß - Beta'], # True Positive Rate on Y-axis\n",
    "         label='Beta Values',\n",
    "         color=color3,\n",
    "         linewidth=5)\n",
    "plt.title('Negative Beta values from Logistic Regression Model' , fontsize=22)\n",
    "plt.ylabel('Beta', fontsize=18)\n",
    "plt.xlabel('Feature', fontsize=18)\n",
    "plt.xticks(rotation=55)\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "color3 = cm.twilight_r(np.linspace(.9,.1, 20))\n",
    "plt.figure(figsize = (15,9))\n",
    "plt.bar(betas.index, # False Positive Rate on X-axis\n",
    "         betas['ß - Beta'], # True Positive Rate on Y-axis\n",
    "         label='Beta Values',\n",
    "         color=color3,\n",
    "         linewidth=5)\n",
    "plt.title('Positive Beta values from Logistic Regression Model' , fontsize=22)\n",
    "plt.ylabel('Beta', fontsize=18)\n",
    "plt.xlabel('Feature', fontsize=18)\n",
    "plt.xticks(rotation=55)\n",
    "plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.sort_values('ß - Beta', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of California subreddit words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cali_mask = np.array(Image.open(\"./images/download.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "transformed_cali_mask = np.ndarray((cali_mask.shape[0],cali_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(cali_mask)):\n",
    "    transformed_cali_mask[i] = list(map(transform_format, cali_mask[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_small = zoom(transformed_cali_mask, (2.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.loc[(df_reddit['ca'] == 1)]['title']\n",
    "ca_text = \" \".join(post for post in (df_reddit.loc[(df_reddit['ca'] == 1)]['title'])) # This is getting me just the words for posts on the CA Subreddit\n",
    "print (\"There are {} words in all posts.\".format(len(ca_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_words, max_font_size=30, max_words=500, background_color=\"white\", mask=ca_small, contour_color='grey', contour_width=0.5).generate(ca_text)\n",
    "plt.figure(figsize= [8,23])\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"./images/california.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Texas subreddit words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_mask = np.array(Image.open(\"./images/TEXAS_BLACK.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of the last part of the array\n",
    "tx_mask = tx_mask[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "transformed_tx_mask = np.ndarray((tx_mask.shape[0],tx_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(tx_mask)):\n",
    "    transformed_tx_mask[i] = list(map(transform_format, tx_mask[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_small = zoom(transformed_tx_mask, (1.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.loc[(df_reddit['ca'] == 0)]['title']\n",
    "tx_text = \" \".join(post for post in (df_reddit.loc[(df_reddit['ca'] == 0)]['title'])) # This is getting just the words for posts on the CA Subreddit\n",
    "print (\"There are {} words in all posts.\".format(len(tx_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_words, max_font_size=40, max_words=500, background_color=\"white\", mask=tx_small, contour_color='grey', contour_width=1.15).generate(tx_text)\n",
    "plt.figure(figsize= [8,23])\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"./images/texas.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the posts that were misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lr.predict(X_test_df), columns=['predicted'])\n",
    "\n",
    "# Create column for observed values.\n",
    "y_test = y_test.reset_index()\n",
    "y_test.head()\n",
    "results['actual'] = y_test['is_ca']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ids = results[results['predicted'] != results['actual']].index\n",
    "row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reddit['title'][18])\n",
    "print('')\n",
    "print(df_reddit['title'][35])\n",
    "print('')\n",
    "print(df_reddit['title'][50])\n",
    "print('')\n",
    "print(df_reddit['title'][71])\n",
    "print('')\n",
    "print(df_reddit['title'][82])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure.\n",
    "plt.figure(figsize = (10,7))\n",
    "\n",
    "# Create two histograms of observations.\n",
    "plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='b',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 0 (Alive)')\n",
    "plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "         bins=25,\n",
    "         color='orange',\n",
    "         alpha = 0.6,\n",
    "         label='Outcome = 1 (Dead)')\n",
    "\n",
    "# Label axes.\n",
    "plt.title('Distribution of P(Outcome = 1)', fontsize=22)\n",
    "plt.ylabel('Frequency', fontsize=18)\n",
    "plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "plt.vlines(x=0.5,\n",
    "           ymin = 0,\n",
    "           ymax = 65,\n",
    "           color='aqua',\n",
    "           linestyle = '--')\n",
    "\n",
    "# Create legend.\n",
    "plt.legend(fontsize=20);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
